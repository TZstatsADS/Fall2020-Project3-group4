{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install ipynb' in the command line. This code lets us import functions from notebooks in the lib folder. Lib has all of the feature extraction and model training/predicting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import sys\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install imblearn' in the command line. This code lets us do SMOTE (synthetic minority oversampling technique) and random undersampling to help deal with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the remaining libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.image as img\n",
    "import scipy.io\n",
    "import pickle\n",
    "import bz2\n",
    "import _pickle as cPickle\n",
    "from sklearn.metrics import pairwise_distances, classification_report, confusion_matrix, roc_auc_score\n",
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Work Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the directories for the training set points and labels. (To do: relative paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change train_dir to your own path\n",
    "\n",
    "#root=sys.path[0]\n",
    "#train_dir = os.path.join(root,\"../data/train_set/\")\n",
    "#train_dir = os.path.join(root,\"../data/train_set/\")\n",
    "train_dir = '../data/train_set/'\n",
    "train_image_dir = train_dir+\"images/\"\n",
    "train_pt_dir = train_dir+\"points/\"\n",
    "train_label_path = train_dir+\"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set Up Controls\n",
    "\n",
    "In this cell, we have a set of controls for the feature extraction. If true, then we process the features from scratch, and if false, then we load existing features from files in the output folder. \n",
    "\n",
    "+ (T/F) initial feature extraction on training set\n",
    "+ (T/F) initial feature extraction on test set\n",
    "+ (T/F) improved feature extraction on training set\n",
    "+ (T/F) improved feature extraction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_feature_train_initial = True\n",
    "run_feature_test_initial = True\n",
    "\n",
    "run_feature_train = True \n",
    "run_feature_test = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we have a set of controls for model training/testing. If true, then we train the model and generate predictions on the test set, and if false, then we skip that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline = True\n",
    "run_baseline_improved = True\n",
    "run_baseline_pca = True\n",
    "run_knn = True\n",
    "run_knn_smote = True\n",
    "run_xgboost=True\n",
    "run_xgboost_smote=True\n",
    "feature_initial=True\n",
    "run_randonforest=True\n",
    "run_LDA=True\n",
    "run_logistic=True\n",
    "run_svm = True\n",
    "run_lasso = True\n",
    "run_bagging_smote = True\n",
    "run_naivebayes = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Data and Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data, and we can see that the dataset is imbalanced and that there are more records with basic emotions than records with complex emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   2402 \n",
      "Number of records with label 1 (complex emotion): 598 \n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "n = info.shape[0]\n",
    "\n",
    "#Data is imbalanced \n",
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(info.loc[info['label']==0].shape[0]))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(info.loc[info['label']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do an 80-20 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(round(n*(4/5),0))\n",
    "train_idx = np.random.choice(list(info.index),size=n_train,replace=False)\n",
    "test_idx = list(set(list(info.index))-set(train_idx)) #set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial points are stored in matlab format. In this step, we read them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read fiducial points\n",
    "#input: index\n",
    "#output: matrix of fiducial points corresponding to the index\n",
    "\n",
    "n_files = len(os.listdir(train_pt_dir))\n",
    "\n",
    "def readMat_matrix(index):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinatesUnwarped']\n",
    "    except KeyError:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinates2']\n",
    "    return np.matrix.round(mat_data,0)\n",
    "\n",
    "#load fiducial points\n",
    "#pickle is the closest equivalent to .RData that I could find in Python\n",
    "fiducial_pt_list = list(map(readMat_matrix,list(range(1,n_files+1))))\n",
    "pickle.dump(fiducial_pt_list, open( \"../output/fiducial_pt_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: construct features and responses\n",
    "#### Remove this text later\n",
    "`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. \n",
    "  \n",
    "  + `feature.R`\n",
    "  + Input: list of images or fiducial point\n",
    "  + Output: an RData file that contains extracted features and corresponding responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature_initial.ipynb's feature_initial function to generate pairwise distance features for the baseline model. This is the same feature extraction method as that of the starter code. Note that this method double counts distances between points. That is, there are separate entries for the distance from point A to point B and the distance from point B to point A even though the distances are the same.\n",
    "\n",
    "Feature extraction times exclude the time it takes to write to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature extraction time for train: 7.076202\n",
      "Initial feature extraction time for test:  2.152687\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature_initial import feature_initial\n",
    "\n",
    "tm_feature_train = np.nan\n",
    "if run_feature_train_initial == True:\n",
    "    start = time.time()\n",
    "    dat_train_initial = feature_initial(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "    with bz2.BZ2File('../output/train_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_initial, f)\n",
    "    print('Initial feature extraction time for train: {:4f}'.format(tm_feature_train))\n",
    "else:\n",
    "    dat_train_initial = cPickle.load(bz2.BZ2File('../output/train_data_initial.pbz2', 'rb'))\n",
    "        \n",
    "        \n",
    "tm_feature_test = np.nan\n",
    "if run_feature_test_initial == True:\n",
    "    start = time.time()\n",
    "    dat_test_initial = feature_initial(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    with bz2.BZ2File('../output/test_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test_initial, f)\n",
    "    print('Initial feature extraction time for test:  {:4f}'.format(tm_feature_test))\n",
    "else:\n",
    "    dat_test_initial = cPickle.load(bz2.BZ2File('../output/test_data_initial.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature_improved.ipynb's feature_improved function to generate pairwise distance features to be used by all of the models other than the baseline. Unlike feature_initial, feature_improved does not double count distances. Hence, feature_improved produces exactly half as many features as feature_initial while keeping the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved feature extraction time for train: 0.198280\n",
      "Improved feature extraction time for test:  0.035590\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature_improved import feature_improved\n",
    "\n",
    "tm_feature_train = np.nan\n",
    "if run_feature_train == True:\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "    with bz2.BZ2File('../output/train_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train, f)\n",
    "    print('Improved feature extraction time for train: {:4f}'.format(tm_feature_train))\n",
    "else:\n",
    "    dat_train = cPickle.load(bz2.BZ2File('../output/train_data.pbz2', 'rb'))\n",
    "\n",
    "\n",
    "tm_feature_test = np.nan\n",
    "if run_feature_test == True:\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    with bz2.BZ2File('../output/test_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test, f)\n",
    "    print('Improved feature extraction time for test:  {:4f}'.format(tm_feature_test))\n",
    "else:\n",
    "    dat_test = cPickle.load(bz2.BZ2File('../output/test.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we get the training/test features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_initial = dat_train_initial.loc[:, dat_train_initial.columns != 'labels']\n",
    "label_train_initial = dat_train_initial['labels'] \n",
    "\n",
    "feature_test_initial = dat_test_initial.loc[:, dat_test_initial.columns != 'labels']\n",
    "label_test_initial = dat_test_initial['labels'] \n",
    "\n",
    "feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "label_train = dat_train['labels'] #same values as label_train_initial\n",
    "\n",
    "feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "label_test = dat_test['labels'] #same values as label_test_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA (made some changes)\n",
    "scaler = MinMaxScaler()\n",
    "feature_train_scaled = scaler.fit_transform(feature_train)\n",
    "feature_test_scaled = scaler.fit_transform(feature_test)\n",
    "\n",
    "#pick the number of components that captures 95% of the variance\n",
    "pca = PCA(n_components = 0.95, svd_solver='full').fit(feature_train_scaled)\n",
    "feature_train_PCA = pca.transform(feature_train_scaled)\n",
    "feature_test_PCA = pca.transform(feature_test_scaled)\n",
    "\n",
    "#print how many components after pca\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Baseline Model\n",
    "Call the train model and test model from library. \n",
    "\n",
    "`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. \n",
    "\n",
    "+ `train.R`\n",
    "  + Input: a data frame containing features and labels and a parameter list.\n",
    "  + Output:a trained model\n",
    "+ `test.R`\n",
    "  + Input: the fitted classification model using training data and processed features from testing images \n",
    "  + Input: an R object that contains a trained classifier.\n",
    "  + Output: training model specification\n",
    "\n",
    "+ In this Starter Code, we use logistic regression with LASSO penalty to do classification. \n",
    "\n",
    "* Model selection with cross-validation\n",
    "* Do model selection by choosing among different values of training model parameters.\n",
    "\n",
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 183.109893 seconds\n",
      "Prediction time: 0.046860 seconds\n",
      "\n",
      "Classification Error: 0.188333\n",
      "Accuracy: 0.811667\n",
      "Balanced Accuracy: 0.595437\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89       473\n",
      "           1       0.67      0.22      0.33       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.74      0.60      0.61       600\n",
      "weighted avg       0.79      0.81      0.77       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[459  14]\n",
      " [ 99  28]]\n",
      "\n",
      "AUC: 0.785038\n"
     ]
    }
   ],
   "source": [
    "if run_baseline == True:\n",
    "    \n",
    "    #grid search for optimal parameters\n",
    "    #params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train_initial,label_train_initial)\n",
    "    #gscv.best_params_\n",
    "    # output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
    "    \n",
    "    start = time.time()\n",
    "    baseline = GradientBoostingClassifier(learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    baseline.fit(feature_train_initial,label_train_initial)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = baseline.predict(feature_test_initial)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test_initial))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test_initial,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test_initial,test_preds))\n",
    "    \n",
    "    test_probs = baseline.predict_proba(feature_test_initial)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    #save baseline model\n",
    "    pickle.dump(baseline,open(\"../output/baseline.p\", \"wb\"))\n",
    "    \n",
    "    #load baseline model\n",
    "    #baseline = pickle.load(open(\"../output/baseline.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model with Improved Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 180.717161 seconds\n",
      "Prediction time: 0.024233 seconds\n",
      "\n",
      "Classification Error: 0.185000\n",
      "Accuracy: 0.815000\n",
      "Balanced Accuracy: 0.606191\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89       473\n",
      "           1       0.67      0.24      0.36       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.75      0.61      0.63       600\n",
      "weighted avg       0.79      0.81      0.78       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[458  15]\n",
      " [ 96  31]]\n",
      "\n",
      "AUC: 0.799720\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_improved == True:\n",
    "    \n",
    "    #grid search for optimal parameters\n",
    "    #params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train,label_train)\n",
    "    #gscv.best_params_\n",
    "    # output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
    "    \n",
    "    start = time.time()\n",
    "    baseline = GradientBoostingClassifier(learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    baseline.fit(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = baseline.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = baseline.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.181412 seconds\n",
      "Prediction time: 0.001284 seconds\n",
      "\n",
      "Classification Error: 0.211667\n",
      "Accuracy: 0.788333\n",
      "\n",
      "Balanced Accuracy: 0.531679\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       473\n",
      "           1       0.50      0.09      0.15       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.65      0.53      0.51       600\n",
      "weighted avg       0.74      0.79      0.72       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[462  11]\n",
      " [116  11]]\n",
      "\n",
      "AUC: 0.693596\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_pca == True:\n",
    "    \n",
    "    #params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3).fit(feature_train_PCA,label_train)\n",
    "    #gscv.best_params_\n",
    "    #output: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}\n",
    "\n",
    "    #Baseline model with PCA\n",
    "    #need to do grid search to get optimal parameters though\n",
    "    start = time.time()\n",
    "    gbm_pca=GradientBoostingClassifier(learning_rate=0.1,max_depth=2,n_estimators=150)\n",
    "    gbm_pca.fit(feature_train_PCA,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = gbm_pca.predict(feature_test_PCA)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = gbm_pca.predict_proba(feature_test_PCA)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.379051 seconds\n",
      "Prediction time: 5.723319 seconds\n",
      "\n",
      "Classification Error: 0.208333\n",
      "Balanced Accuracy: 0.516514\n",
      "Accuracy: 0.791667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       473\n",
      "           1       0.62      0.04      0.07       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.71      0.52      0.48       600\n",
      "weighted avg       0.76      0.79      0.71       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[470   3]\n",
      " [122   5]]\n",
      "\n",
      "AUC: 0.674527\n"
     ]
    }
   ],
   "source": [
    "if run_knn == True:\n",
    "    \n",
    "    #params = {'n_neighbors':list(range(5,55,5))}\n",
    "    #gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train,label_train)\n",
    "    #gscv.best_params_\n",
    "    #output: {'n_neighbors': 25}\n",
    "    \n",
    "    start = time.time()\n",
    "    #need to cross validate to pick best value (after finalizing features)\n",
    "    knn = KNeighborsClassifier(n_neighbors = 25) \n",
    "    knn.fit(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = knn.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test)) #Classification Error\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = knn.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN With SMOTE and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.734488 seconds\n",
      "Prediction time: 7.845165 seconds\n",
      "\n",
      "Classification Error: 0.411667\n",
      "Accuracy: 0.588333\n",
      "\n",
      "Balanced Accuracy: 0.615064\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.57      0.69       473\n",
      "           1       0.29      0.66      0.40       127\n",
      "\n",
      "    accuracy                           0.59       600\n",
      "   macro avg       0.58      0.62      0.55       600\n",
      "weighted avg       0.74      0.59      0.63       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[269 204]\n",
      " [ 43  84]]\n",
      "\n",
      "AUC: 0.664339\n"
     ]
    }
   ],
   "source": [
    "if run_knn_smote == True:\n",
    "    \n",
    "    over = SMOTE(sampling_strategy='auto')\n",
    "    under = RandomUnderSampler(sampling_strategy='auto')\n",
    "    sm = Pipeline(steps = [('o', over), ('u', under)])\n",
    "    feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "    \n",
    "    #params = {'n_neighbors':list(range(5,55,5))}\n",
    "    #gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train_sm,label_train_sm)\n",
    "    #gscv.best_params_\n",
    "    #output: {'n_neighbors': 5}\n",
    "    \n",
    "    start = time.time()\n",
    "    #need to cross validate to pick best value (after finalizing features)\n",
    "    knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "    knn.fit(feature_train_sm,label_train_sm)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = knn.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test)) #Classification Error\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = knn.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 82.137282 seconds\n",
      "Prediction time: 0.074498 seconds\n",
      "\n",
      "Classification Error: 0.186667\n",
      "Accuracy: 0.813333\n",
      "\n",
      "Balanced Accuracy: 0.688652\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       473\n",
      "           1       0.57      0.47      0.52       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.72      0.69      0.70       600\n",
      "weighted avg       0.80      0.81      0.81       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[428  45]\n",
      " [ 67  60]]\n",
      "\n",
      "AUC: 0.808726\n"
     ]
    }
   ],
   "source": [
    "if run_xgboost == True:\n",
    "    \n",
    "    # train_labels_xgb = [ x  - 1 for x in label_train ] \n",
    "    # test_labels_xgb = [ x  - 1 for x in label_test ] \n",
    "    start_time=time.time()\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "     n_estimators= 200,\n",
    "     max_depth=3,\n",
    "     min_child_weight=1,\n",
    "     objective= 'binary:logistic',\n",
    "     scale_pos_weight=4\n",
    "    )\n",
    "    \n",
    "    xgb.fit(feature_train, label_train ,eval_metric='auc')\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_preds = xgb.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = xgb.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'learning_rate':[0.1,0.25,0.5],'n_estimators':[100,200,300]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(XGBClassifier(min_child_weight=1,max_depth=3,objective= 'binary:logistic',scale_pos_weight=4),params,cv=cv,scoring='roc_auc',verbose=True).fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {'learning_rate': 0.25, 'n_estimators': 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 197.387028 seconds\n",
      "Prediction time: 0.068110 seconds\n",
      "\n",
      "Classification Error: 0.180000\n",
      "Accuracy: 0.820000\n",
      "\n",
      "Balanced Accuracy: 0.710160\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.89       473\n",
      "           1       0.58      0.52      0.55       127\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.73      0.71      0.72       600\n",
      "weighted avg       0.81      0.82      0.82       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[426  47]\n",
      " [ 61  66]]\n",
      "\n",
      "AUC: 0.808626\n"
     ]
    }
   ],
   "source": [
    "# train_labels_xgb = [ x  - 1 for x in label_train ] \n",
    "# test_labels_xgb = [ x  - 1 for x in label_test ] \n",
    "\n",
    "if run_xgboost_smote == True:\n",
    "    #sm = SMOTE(sampling_strategy='auto',k_neighbors=20,random_state=42)\n",
    "    #feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "\n",
    "    over = SMOTE(sampling_strategy='auto')\n",
    "    under = RandomUnderSampler(sampling_strategy='auto')\n",
    "    sm = Pipeline(steps = [('o', over), ('u', under)])\n",
    "    feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "    \n",
    "    start_time=time.time()\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "     learning_rate =0.25,\n",
    "     n_estimators= 300,\n",
    "     max_depth=3,\n",
    "     min_child_weight=1,\n",
    "     objective= 'binary:logistic',\n",
    "     scale_pos_weight=4\n",
    "    )\n",
    "\n",
    "    xgb.fit(feature_train_sm, label_train_sm ,eval_metric='auc')\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_preds = xgb.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "\n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "\n",
    "    test_probs = xgb.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandonForest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.482402 seconds\n",
      "Prediction time: 0.031882 seconds\n",
      "\n",
      "Classification Error: 0.188333\n",
      "Accuracy: 0.811667\n",
      "\n",
      "Balanced Accuracy: 0.578158\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89       473\n",
      "           1       0.73      0.17      0.28       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.77      0.58      0.59       600\n",
      "weighted avg       0.80      0.81      0.76       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[465   8]\n",
      " [105  22]]\n",
      "\n",
      "AUC: 0.740540\n"
     ]
    }
   ],
   "source": [
    "if run_randonforest==True:\n",
    "    start_time = time.time()\n",
    "    rf = RandomForestClassifier(n_estimators = 100, criterion = 'gini', min_samples_leaf=1, max_features='sqrt')\n",
    "    rf_model = rf.fit(feature_train, label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = rf_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = rf_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.696389 seconds\n",
      "Prediction time: 0.019311 seconds\n",
      "\n",
      "Classification Error: 0.178333\n",
      "Accuracy: 0.821667\n",
      "\n",
      "Balanced Accuracy: 0.636339\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.89       473\n",
      "           1       0.67      0.31      0.43       127\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.75      0.64      0.66       600\n",
      "weighted avg       0.80      0.82      0.80       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[453  20]\n",
      " [ 87  40]]\n",
      "\n",
      "AUC: 0.786203\n"
     ]
    }
   ],
   "source": [
    "if run_LDA==True:    \n",
    "    start_time = time.time()\n",
    "    lda = LDA(solver='eigen', shrinkage=.1, n_components=1)\n",
    "    lda_model = lda.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds=lda_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lda_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 171.402036 seconds\n",
      "Prediction time: 0.008260 seconds\n",
      "\n",
      "Classification Error: 0.213333\n",
      "Accuracy: 0.786667\n",
      "\n",
      "Balanced Accuracy: 0.689018\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86       473\n",
      "           1       0.50      0.52      0.51       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.68      0.69      0.69       600\n",
      "weighted avg       0.79      0.79      0.79       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[406  67]\n",
      " [ 61  66]]\n",
      "\n",
      "AUC: 0.812255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "if run_logistic==True:    \n",
    "    start_time = time.time() \n",
    "    lr = LogisticRegression(C=1, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=1200000,\n",
    "                   multi_class='multinomial', penalty='l2',\n",
    "                   solver='lbfgs', tol=0.0001)\n",
    "    lr_model = lr.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds = lr_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lr_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 110.158746 seconds\n",
      "Prediction time: 0.005710 seconds\n",
      "\n",
      "Classification Error: 0.178333\n",
      "Accuracy: 0.821667\n",
      "\n",
      "Balanced Accuracy: 0.699697\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       473\n",
      "           1       0.60      0.49      0.54       127\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.73      0.70      0.71       600\n",
      "weighted avg       0.81      0.82      0.81       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[431  42]\n",
      " [ 65  62]]\n",
      "\n",
      "AUC: 0.822210\n"
     ]
    }
   ],
   "source": [
    "if run_logistic==True:    \n",
    "    start_time = time.time() \n",
    "    lr = LogisticRegression(C=0.01, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=1200000,\n",
    "                   multi_class='multinomial', penalty='l2',\n",
    "                   solver='lbfgs', tol=0.0001)\n",
    "    lr_model = lr.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds = lr_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lr_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#weights = {0:80.0, 1:20.0}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001,class_weight=weights),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 139.922658 seconds\n",
      "Prediction time: 0.005956 seconds\n",
      "\n",
      "Classification Error: 0.170000\n",
      "Accuracy: 0.830000\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       473\n",
      "           1       0.65      0.43      0.51       127\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.75      0.68      0.71       600\n",
      "weighted avg       0.81      0.83      0.82       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[444  29]\n",
      " [ 73  54]]\n",
      "\n",
      "AUC: 0.823159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "if run_logistic==True:     \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    start_time = time.time() \n",
    "    lr = LogisticRegression(C=0.001, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=120000000000,\n",
    "                   multi_class='multinomial', penalty='l2',\n",
    "                   solver='lbfgs', tol=0.0001,class_weight=weights)\n",
    "    lr_model = lr.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds = lr_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lr_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.00001,0.0001,0.001,0.01,1,10],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 81.798285 seconds\n",
      "Prediction time: 2.002354 seconds\n",
      "\n",
      "Classification Error: 0.145000\n",
      "Accuracy: 0.855000\n",
      "\n",
      "Balanced Accuracy: 0.683400\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91       473\n",
      "           1       0.84      0.39      0.53       127\n",
      "\n",
      "    accuracy                           0.85       600\n",
      "   macro avg       0.85      0.68      0.72       600\n",
      "weighted avg       0.85      0.85      0.83       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[464   9]\n",
      " [ 78  49]]\n",
      "\n",
      "AUC: 0.828137\n"
     ]
    }
   ],
   "source": [
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:    \n",
    "    start_time = time.time() \n",
    "    svm = SVC(C=10, kernel='poly', degree=4, random_state=2020,probability=True)\n",
    "    svm_model = svm.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = svm_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.001,0.01,1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train_PCA,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 2, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.891167 seconds\n",
      "Prediction time: 0.020211 seconds\n",
      "\n",
      "Classification Error: 0.213333\n",
      "Accuracy: 0.786667\n",
      "\n",
      "Balanced Accuracy: 0.585341\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       473\n",
      "           1       0.49      0.24      0.32       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.66      0.59      0.60       600\n",
      "weighted avg       0.75      0.79      0.76       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[442  31]\n",
      " [ 97  30]]\n",
      "\n",
      "AUC: 0.745093\n"
     ]
    }
   ],
   "source": [
    "#improved svm with PCA\n",
    "if run_svm==True:    \n",
    "    start_time = time.time() \n",
    "    svm = SVC(C=10, kernel='rbf', degree=2, random_state=2020,probability=True)\n",
    "    svm_model = svm.fit(feature_train_PCA,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = svm_model.predict_proba(feature_test_PCA)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# params= {'C': [1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(SVC(class_weight=weights,random_state = 2020,probability=True), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: output: {'C': 1, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 385.956874 seconds\n",
      "Prediction time: 1.710385 seconds\n",
      "\n",
      "Classification Error: 0.163333\n",
      "Accuracy: 0.836667\n",
      "\n",
      "Balanced Accuracy: 0.717851\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       473\n",
      "           1       0.64      0.51      0.57       127\n",
      "\n",
      "    accuracy                           0.84       600\n",
      "   macro avg       0.76      0.72      0.73       600\n",
      "weighted avg       0.83      0.84      0.83       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[437  36]\n",
      " [ 62  65]]\n",
      "\n",
      "AUC: 0.837209\n"
     ]
    }
   ],
   "source": [
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    start_time = time.time() \n",
    "    svm = SVC(C=10, kernel='poly', degree=4, random_state=2020,class_weight=weights,probability=True)\n",
    "    svm_model = svm.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = svm_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.150770 seconds\n",
      "Prediction time: 0.046284 seconds\n",
      "\n",
      "Classification Error: 0.336667\n",
      "Accuracy: 0.663333\n",
      "\n",
      "Balanced Accuracy: 0.717851\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.69      0.76       473\n",
      "           1       0.33      0.57      0.42       127\n",
      "\n",
      "    accuracy                           0.66       600\n",
      "   macro avg       0.59      0.63      0.59       600\n",
      "weighted avg       0.74      0.66      0.69       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[326 147]\n",
      " [ 55  72]]\n",
      "\n",
      "AUC: 0.660369\n"
     ]
    }
   ],
   "source": [
    "if run_naivebayes == True:\n",
    "    start = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gaussian = gnb.fit(feature_train, label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_pred = gaussian.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_pred) != np.array(label_test)) #Classification Error\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error))\n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_pred))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_pred))\n",
    "    \n",
    "    test_probs = gaussian.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 225.099178 seconds\n",
      "Prediction time: 0.017482 seconds\n",
      "\n",
      "Classification Error: 0.175000\n",
      "Accuracy: 0.825000\n",
      "\n",
      "Balanced Accuracy: 0.693171\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89       473\n",
      "           1       0.61      0.46      0.53       127\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.74      0.69      0.71       600\n",
      "weighted avg       0.81      0.82      0.82       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[436  37]\n",
      " [ 68  59]]\n",
      "\n",
      "AUC: 0.826389\n"
     ]
    }
   ],
   "source": [
    "if run_lasso==True:    \n",
    "    start_time = time.time() \n",
    "    lasso = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    lasso_model = lasso.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds = lasso_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lasso_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# param= {'solver':['liblinear', 'saga']}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(LogisticRegression(penalty='l1', class_weight=weights), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 60.624311 seconds\n",
      "Prediction time: 0.032459 seconds\n",
      "\n",
      "Classification Error: 0.168333\n",
      "Accuracy: 0.831667\n",
      "\n",
      "Balanced Accuracy: 0.622522\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90       473\n",
      "           1       0.82      0.26      0.40       127\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.83      0.62      0.65       600\n",
      "weighted avg       0.83      0.83      0.79       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[466   7]\n",
      " [ 94  33]]\n",
      "\n",
      "AUC: 0.819231\n"
     ]
    }
   ],
   "source": [
    "if run_lasso==True:    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    start_time = time.time() \n",
    "    lasso_w = LogisticRegression(penalty='l1', solver='liblinear', class_weight=weights)\n",
    "    lasso_model_w = lasso_w.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds = lasso_model_w.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lasso_model_w.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_estimators':[25,50,75,100]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(BaggingClassifier(),params,cv=cv,scoring='roc_auc').fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {{'n_estimators': 100}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 728.257860 seconds\n",
      "Prediction time: 0.505236 seconds\n",
      "\n",
      "Classification Error: 0.206667\n",
      "Accuracy: 0.793333\n",
      "\n",
      "Balanced Accuracy: 0.627008\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.87       473\n",
      "           1       0.52      0.34      0.41       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.68      0.63      0.64       600\n",
      "weighted avg       0.77      0.79      0.78       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[433  40]\n",
      " [ 84  43]]\n",
      "\n",
      "AUC: 0.788808\n"
     ]
    }
   ],
   "source": [
    "if run_bagging_smote == True:\n",
    "    \n",
    "    over = SMOTE(sampling_strategy='auto')\n",
    "    under = RandomUnderSampler(sampling_strategy='auto')\n",
    "    sm = Pipeline(steps = [('o', over), ('u', under)])\n",
    "    feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "    \n",
    "    start_time=time.time()\n",
    "\n",
    "    bagging_smote = BaggingClassifier(n_estimators = 100)\n",
    "    bagging_smote.fit(feature_train_sm, label_train_sm)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_preds = bagging_smote.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "\n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "\n",
    "    test_probs = bagging_smote.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After undersampling/oversampling, we now have equal number of members in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   1929 \n",
      "Number of records with label 1 (complex emotion): 1929 \n"
     ]
    }
   ],
   "source": [
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(len(label_train_sm)-sum(label_train_sm)))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(sum(label_train_sm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  0.8445833333333332\n"
     ]
    }
   ],
   "source": [
    "# weighted lasso\n",
    "print(\"Cross Validation Score: \", np.mean(cross_val_score(lasso_w, feature_train, label_train, cv=10)))\n",
    "# output:0.8445833333333332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  0.8195833333333333\n"
     ]
    }
   ],
   "source": [
    "# weighted svm\n",
    "print(\"Cross Validation Score: \", np.mean(cross_val_score(svm, feature_train, label_train, cv=10)))\n",
    "# output:0.8195833333333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
