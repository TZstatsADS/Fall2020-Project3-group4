{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.image as img\n",
    "import scipy.io\n",
    "import pickle\n",
    "from sklearn.metrics import pairwise_distances, classification_report, confusion_matrix, roc_auc_score\n",
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install ipynb' in the command line. This code lets us import functions from notebooks in the lib folder. Lib is supposed to have all the model training/predicting functions and the doc folder is only supposed to have report/presentation files like main.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: set work directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide directories for training images. Training images and Training fiducial points will be in different subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change train_dir to where you have the train set because we can't\n",
    "#upload the train set to github\n",
    "\n",
    "train_dir = \"/Users/rohan/Desktop/train_set/\"\n",
    "train_image_dir = train_dir+\"images/\"\n",
    "train_pt_dir = train_dir+\"points/\"\n",
    "train_label_path = train_dir+\"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: set up controls for evaluation experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chunk, we have a set of controls for the evaluation experiments. \n",
    "\n",
    "+ (T/F) cross-validation on the training set\n",
    "+ (T/F) reweighting the samples for training set \n",
    "+ (number) K, the number of CV folds\n",
    "+ (T/F) process features for training set\n",
    "+ (T/F) run evaluation on an independent test set\n",
    "+ (T/F) process features for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cv = True # run cross-validation on the training set\n",
    "sample_reweight = True # run sample reweighting in model training\n",
    "K = 5  # number of CV folds\n",
    "run_feature_train = True # process features for training set\n",
    "run_test = True # run evaluation on an independent test set\n",
    "run_feature_test = True # process features for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set these to be False if you don't want to go through training certain models when running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline = True\n",
    "run_baseline_pca = True\n",
    "run_knn = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: import data and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion): 2402 \n",
      "Number of records with label 1 (complex emotion): 598 \n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "n = info.shape[0]\n",
    "\n",
    "#Data is imbalanced \n",
    "print('Number of records with label 0 (basic emotion): {:4d} '.format(info.loc[info['label']==0].shape[0]))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(info.loc[info['label']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we could use sklearn train_test_split here instead of doing it\n",
    "#manually like in the starter code\n",
    "n_train = int(round(n*(4/5),0))\n",
    "train_idx = np.random.choice(list(info.index),size=n_train,replace=False)\n",
    "test_idx = list(set(list(info.index))-set(train_idx)) #set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading images (we never use the image list later since we're just using the fiducial points for the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(os.listdir(train_image_dir))\n",
    "\n",
    "image_list = []\n",
    "for i in range(1,101): # 1 to 100\n",
    "    image = img.imread(train_image_dir+'{:04d}'.format(i)+'.jpg')\n",
    "    image_list.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial points are stored in matlab format. In this step, we read them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read fiducial points\n",
    "#input: index\n",
    "#output: matrix of fiducial points corresponding to the index\n",
    "\n",
    "def readMat_matrix(index):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinatesUnwarped']\n",
    "    except KeyError:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinates2']\n",
    "    return np.matrix.round(mat_data,0)\n",
    "\n",
    "#load fiducial points\n",
    "#pickle is the closest equivalent to .RData that I could find in Python\n",
    "fiducial_pt_list = list(map(readMat_matrix,list(range(1,n_files+1))))\n",
    "pickle.dump(fiducial_pt_list, open( \"../output/fiducial_pt_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: construct features and responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. \n",
    "  \n",
    "  + `feature.R`\n",
    "  + Input: list of images or fiducial point\n",
    "  + Output: an RData file that contains extracted features and corresponding responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use feature.ipynb's feature function to generate features for the train and test points\n",
    "\n",
    "from ipynb.fs.full.feature import feature\n",
    "\n",
    "tm_feature_train = np.nan\n",
    "if run_feature_train == True:\n",
    "    start = time.time()\n",
    "    dat_train = feature(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "    pickle.dump(dat_train, open( \"../output/train_data.p\", \"wb\" ) )\n",
    "else:\n",
    "    dat_train = pickle.load(open(\"../output/train_data.p\", \"rb\"))\n",
    "    \n",
    "    \n",
    "tm_feature_test = np.nan\n",
    "if run_feature_test == True:\n",
    "    start = time.time()\n",
    "    dat_test = feature(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    pickle.dump(dat_test, open( \"../output/test_data.p\", \"wb\" ) )\n",
    "else:\n",
    "    dat_test = pickle.load(open(\"../output/test_data.p\", \"rb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the traning/test features and labels\n",
    "\n",
    "feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "label_train = dat_train['labels']\n",
    "\n",
    "feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "label_test = dat_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA (not sure if i did it correctly)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "feature_train_scaled = scaler.fit_transform(feature_train)\n",
    "\n",
    "#pick the number of components that captures 95% of the variance\n",
    "pca = PCA(n_components = 0.95).fit(feature_train_scaled)\n",
    "feature_train_PCA = pca.transform(feature_train_scaled)\n",
    "feature_test_PCA = pca.transform(feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a classification model with training features and responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the train model and test model from library. \n",
    "\n",
    "`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. \n",
    "\n",
    "+ `train.R`\n",
    "  + Input: a data frame containing features and labels and a parameter list.\n",
    "  + Output:a trained model\n",
    "+ `test.R`\n",
    "  + Input: the fitted classification model using training data and processed features from testing images \n",
    "  + Input: an R object that contains a trained classifier.\n",
    "  + Output: training model specification\n",
    "\n",
    "+ In this Starter Code, we use logistic regression with LASSO penalty to do classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection with cross-validation\n",
    "* Do model selection by choosing among different values of training model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 81.052073 seconds\n",
      "Prediction time: 0.049767 seconds\n",
      "\n",
      "Classification Error: 0.188333\n",
      "Accuracy: 0.811667\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       473\n",
      "           1       0.75      0.17      0.27       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.78      0.58      0.58       600\n",
      "weighted avg       0.80      0.81      0.76       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[466   7]\n",
      " [106  21]]\n",
      "\n",
      "AUC: 0.575278\n"
     ]
    }
   ],
   "source": [
    "if run_baseline == True:\n",
    "    \n",
    "    lmbd = [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
    "    model_labels = [\"Gradient Boosting with learning rate = \"+str(x) for x in lmbd]\n",
    "    \n",
    "    #need to do a grid search for optimal parameters\n",
    "    #takes a really long time and we don't know the features yet\n",
    "    #so i commented the grid search out for now\n",
    "    params = {'learning_rate':lmbd, 'max_depth': [1,2,3,4], 'n_estimators':[100,200,300,400,500]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=K).fit(feature_train,label_train)\n",
    "\n",
    "    #Baseline model\n",
    "    #need to do grid search to get optimal parameters though\n",
    "    start = time.time()\n",
    "    gbm = GradientBoostingClassifier(learning_rate=0.1,max_depth=2,n_estimators=100)\n",
    "    gbm.fit(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = gbm.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}'.format(1-classification_error)) #same as gbm.score(feature_test,label_test)\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.045840 seconds\n",
      "Prediction time: 0.000811 seconds\n",
      "\n",
      "Classification Error: 0.788333\n",
      "Accuracy: 0.211667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       473\n",
      "           1       0.21      1.00      0.35       127\n",
      "\n",
      "    accuracy                           0.21       600\n",
      "   macro avg       0.11      0.50      0.17       600\n",
      "weighted avg       0.04      0.21      0.07       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[  0 473]\n",
      " [  0 127]]\n",
      "\n",
      "AUC: 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#something wrong with PCA since we're getting AUC of 0.5\n",
    "if run_baseline_pca == True:\n",
    "    \n",
    "    lmbd = [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
    "    model_labels = [\"Gradient Boosting with learning rate = \"+str(x) for x in lmbd]\n",
    "    \n",
    "    #need to do a grid search for optimal parameters\n",
    "    #takes a really long time and we don't know the features yet\n",
    "    #so i commented the grid search out for now\n",
    "    params = {'learning_rate':lmbd, 'max_depth': [1,2,3,4], 'n_estimators':[100,200,300,400,500]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=K).fit(feature_train,label_train)\n",
    "\n",
    "    #Baseline model with PCA\n",
    "    #need to do grid search to get optimal parameters though\n",
    "    start = time.time()\n",
    "    gbm_pca=GradientBoostingClassifier(learning_rate=0.1,max_depth=2,n_estimators=100)\n",
    "    gbm_pca.fit(feature_train_PCA,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = gbm_pca.predict(feature_test_PCA)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.804046 seconds\n",
      "Prediction time: 12.661891 seconds\n",
      "\n",
      "Classification Error: 0.208333\n",
      "Accuracy: 0.791667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       473\n",
      "           1       0.55      0.09      0.16       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.67      0.54      0.52       600\n",
      "weighted avg       0.75      0.79      0.73       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[463  10]\n",
      " [115  12]]\n",
      "\n",
      "AUC: 0.536673\n"
     ]
    }
   ],
   "source": [
    "if run_knn == True:\n",
    "    \n",
    "    start = time.time()\n",
    "    #need to cross validate to pick best value (after finalizing features)\n",
    "    knn = KNeighborsClassifier(n_neighbors = 8)\n",
    "    knn.fit(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = knn.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test)) #Classification Error\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
