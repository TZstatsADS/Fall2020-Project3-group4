{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install ipynb' in the command line. This code lets us import functions from notebooks in the lib folder. Lib has all of the feature extraction and model training/predicting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import sys\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install imblearn' in the command line. This code lets us do SMOTE (synthetic minority oversampling technique) and random undersampling to help deal with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the remaining libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .boolean { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .integer { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .string  { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import scipy.io\n",
    "import pickle\n",
    "import bz2\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "from sklearn.metrics import pairwise_distances, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Work Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the directories for the training set points and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/train_set/'\n",
    "train_image_dir = train_dir+\"images/\"\n",
    "train_pt_dir = train_dir+\"points/\"\n",
    "train_label_path = train_dir+\"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set Up Controls\n",
    "\n",
    "In this cell, we have a set of controls for the feature extraction. If true, then we process the features from scratch, and if false, then we load existing features from files in the output folder. \n",
    "\n",
    "+ (T/F) initial feature extraction on training set\n",
    "+ (T/F) initial feature extraction on test set\n",
    "\n",
    "+ (T/F) improved feature extraction on training set\n",
    "+ (T/F) improved feature extraction on test set\n",
    "\n",
    "+ (T/F) SMOTE using improved features on train set\n",
    "\n",
    "+ (T/F) PCA using improved features on training set and test set (doesn't make sense to only do one from scratch and not the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_feature_train_initial = True\n",
    "run_feature_test_initial = True\n",
    "\n",
    "run_feature_train = True \n",
    "run_feature_test = True \n",
    "\n",
    "run_feature_train_SMOTE = True\n",
    "\n",
    "run_feature_PCA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we have a set of controls for model training/testing. If true, then we train the model and generate predictions on the test set, and if false, then we skip that model. By default only the baseline and advanced models are set to run, but you can set the other models to be True to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline = True\n",
    "run_advanced = True\n",
    "\n",
    "run_baseline_improved = True\n",
    "run_baseline_pca = True\n",
    "run_knn = True\n",
    "run_knn_smote = True\n",
    "run_xgboost=True\n",
    "feature_initial=True\n",
    "run_random_forest=True\n",
    "run_LDA=True\n",
    "run_logistic=True\n",
    "run_weighted_logistic=True\n",
    "run_svm = True\n",
    "run_svm_pca = True\n",
    "run_weighted_svm = True\n",
    "run_lasso = True\n",
    "run_weighted_lasso = True\n",
    "run_bagging_smote = True\n",
    "run_naivebayes = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Data and Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data, and we can see that the dataset is imbalanced and that there are more records with basic emotions than records with complex emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   2402 \n",
      "Number of records with label 1 (complex emotion): 598 \n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "n = info.shape[0]\n",
    "\n",
    "#Data is imbalanced \n",
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(info.loc[info['label']==0].shape[0]))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(info.loc[info['label']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do an 80-20 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(round(n*(4/5),0))\n",
    "train_idx = np.random.choice(list(info.index),size=n_train,replace=False)\n",
    "test_idx = list(set(list(info.index))-set(train_idx)) #set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial points are stored in matlab format. In this step, we read them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read fiducial points\n",
    "#input: index\n",
    "#output: matrix of fiducial points corresponding to the index\n",
    "\n",
    "n_files = len(os.listdir(train_pt_dir))\n",
    "\n",
    "def readMat_matrix(index):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinatesUnwarped']\n",
    "    except KeyError:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinates2']\n",
    "    return np.matrix.round(mat_data,0)\n",
    "\n",
    "#load fiducial points into list and store them in output\n",
    "fiducial_pt_list = list(map(readMat_matrix,list(range(1,n_files+1))))\n",
    "pickle.dump(fiducial_pt_list, open( \"../output/fiducial_pt_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Construct Features and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starter Code Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature.ipynb's feature_initial function to generate pairwise distance features for the baseline model. This is the same feature extraction method as that of the starter code. Note that this method double counts distances between points. That is, there are separate entries for the distance from point A to point B and the distance from point B to point A even though the distances are the same.\n",
    "\n",
    "Feature extraction times exclude the time it takes to write to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature extraction time for train: 5.011157\n",
      "Initial feature extraction time for test:  1.175852\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_initial\n",
    "\n",
    "tm_feature_train_intitial = np.nan\n",
    "if run_feature_train_initial == True:\n",
    "    start = time.time()\n",
    "    dat_train_initial = feature_initial(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_initial = end-start\n",
    "    with bz2.BZ2File('../output/train_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_initial, f)\n",
    "    print('Initial feature extraction time for train: {:4f}'.format(tm_feature_train_initial))\n",
    "else:\n",
    "    dat_train_initial = cPickle.load(bz2.BZ2File('../output/train_data_initial.pbz2', 'rb'))\n",
    "        \n",
    "        \n",
    "tm_feature_test_initial = np.nan\n",
    "if run_feature_test_initial == True:\n",
    "    start = time.time()\n",
    "    dat_test_initial = feature_initial(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test_initial = end-start\n",
    "    with bz2.BZ2File('../output/test_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test_initial, f)\n",
    "    print('Initial feature extraction time for test:  {:4f}'.format(tm_feature_test_initial))\n",
    "else:\n",
    "    dat_test_initial = cPickle.load(bz2.BZ2File('../output/test_data_initial.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_initial = dat_train_initial.loc[:, dat_train_initial.columns != 'labels']\n",
    "label_train_initial = dat_train_initial['labels']\n",
    "\n",
    "feature_test_initial = dat_test_initial.loc[:, dat_test_initial.columns != 'labels']\n",
    "label_test_initial = dat_test_initial['labels'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature's feature_improved function to generate pairwise euclidean distance features to be used by all of the models other than the baseline. Since feature_improved just uses a single euclidean distance value rather than separate x-distance and y-distance values, feature_improved produces exactly half as many features as feature_initial while keeping the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved feature extraction time for train: 0.179238\n",
      "Improved feature extraction time for test:  0.035093\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_improved\n",
    "\n",
    "tm_feature_train_improved = np.nan\n",
    "if run_feature_train == True:\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_improved = end-start\n",
    "    with bz2.BZ2File('../output/train_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train, f)\n",
    "    print('Improved feature extraction time for train: {:4f}'.format(tm_feature_train_improved))\n",
    "else:\n",
    "    dat_train = cPickle.load(bz2.BZ2File('../output/train_data.pbz2', 'rb'))\n",
    "\n",
    "\n",
    "tm_feature_test_improved = np.nan\n",
    "if run_feature_test == True:\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test_improved = end-start\n",
    "    with bz2.BZ2File('../output/test_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test, f)\n",
    "    print('Improved feature extraction time for test:  {:4f}'.format(tm_feature_test_improved))\n",
    "else:\n",
    "    dat_test = cPickle.load(bz2.BZ2File('../output/test_data.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "label_train = dat_train['labels'] \n",
    "\n",
    "feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "label_test = dat_test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the feature extraction for SMOTE which will be discussed more in the advanced model section. SMOTE is only done on the training data and not on the test data. SMOTE is a modification of the improved features. \n",
    "\n",
    "If the improved features are obtained from scratch, then we include the time it takes to get the improved features with the time it takes to use SMOTE. Otherwise, in the case where the improved features are loaded from the disk, we just use the time it takes to use SMOTE on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE feature extraction time for train: 2.538188\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_SMOTE\n",
    "\n",
    "tm_feature_train_SMOTE = np.nan\n",
    "if run_feature_train_SMOTE == True:\n",
    "    start = time.time()\n",
    "    dat_train_SMOTE = feature_SMOTE(dat_train)\n",
    "    end = time.time()\n",
    "    if pd.isnull(tm_feature_train_improved):\n",
    "        tm_feature_train_SMOTE = end-start\n",
    "    else:\n",
    "        tm_feature_train_SMOTE = (end-start)+tm_feature_train_improved\n",
    "    with bz2.BZ2File('../output/train_data_SMOTE' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train, f)\n",
    "    print('SMOTE feature extraction time for train: {:4f}'.format(tm_feature_train_SMOTE))\n",
    "else:\n",
    "    dat_train_SMOTE = cPickle.load(bz2.BZ2File('../output/train_data_SMOTE.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_sm = dat_train_SMOTE.loc[:,dat_train_SMOTE.columns!='labels']\n",
    "label_train_sm = dat_train_SMOTE['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here we do PCA which is only done for the model candidates that were not chosen for the advanced model. PCA is done as a modification of the improved features. Also, it doesn't make sense to only do the PCA transformation on one of either the training data or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA feature extraction time for train: 6.596193\n",
      "PCA feature extraction time for test:  0.113220\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_PCA\n",
    "\n",
    "tm_feature_train_PCA = np.nan\n",
    "tm_feature_test_PCA = np.nan\n",
    "\n",
    "if run_feature_PCA == True:\n",
    "    [dat_train_PCA, dat_test_PCA, tm_feature_train_PCA, tm_feature_test_PCA] = feature_PCA(dat_train,dat_test)\n",
    "    \n",
    "    if pd.isnull(tm_feature_train_improved)==False:\n",
    "        tm_feature_train_PCA = tm_feature_train_PCA+tm_feature_train_improved\n",
    "    if pd.isnull(tm_feature_test_improved)==False:\n",
    "        tm_feature_test_PCA = tm_feature_test_PCA+tm_feature_test_improved\n",
    "    \n",
    "    with bz2.BZ2File('../output/train_data_PCA' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_PCA, f)\n",
    "    with bz2.BZ2File('../output/test_data_PCA' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test_PCA, f)\n",
    "        \n",
    "    print('PCA feature extraction time for train: {:4f}'.format(tm_feature_train_PCA))\n",
    "    print('PCA feature extraction time for test:  {:4f}'.format(tm_feature_test_PCA))\n",
    "        \n",
    "else:\n",
    "    dat_train_PCA = cPickle.load(bz2.BZ2File('../output/train_data_PCA.pbz2', 'rb'))\n",
    "    dat_test_PCA = cPickle.load(bz2.BZ2File('../output/test_data_PCA.pbz2', 'rb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_PCA = dat_train_PCA.loc[:,dat_train_PCA.columns!='labels']\n",
    "label_train_PCA = dat_train_PCA['labels'] #labels are same as label_train\n",
    "\n",
    "feature_test_PCA = dat_test_PCA.loc[:,dat_test_PCA.columns!='labels']\n",
    "label_test_PCA = dat_test_PCA['labels'] #labels are same as label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discussing the models, we will go over the two main metrics we used for finding a better model than the baseline. Since the data is imbalanced, we examined AUC and balanced accuracy rather than the regular accuracy metric.\n",
    "\n",
    "AUC is the area under the ROC curve which measures the TP vs FP rate as the classification decision threshold changes over time.\n",
    "\n",
    "Balanced accuracy is given by the formula $$balanced\\_accuracy = \\frac{1}{2}(\\frac{TP}{TP+FN}+\\frac{TN}{TN+FP}).$$ \n",
    "\n",
    "\n",
    "Balanced accuracy is used for imbalanced data as an estimate for the accuracy if the data was balanced, so the true performance of our models on balanced data will be close to the balanced accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model is a GBM fitted on the initial pairwise fiducial features. The parameters were chosen from a grid search with AUC scoring. Feature extraction times for all models are already known from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame used to store all of the model results\n",
    "model_results_df = pd.DataFrame(columns=['Feature Extraction Train Time','Feature Extraction Test Time',\n",
    "                                         'Train Time','Prediction Time','Accuracy','Balanced Accuracy','AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimal parameters\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train_initial,label_train_initial)\n",
    "#gscv.best_params_\n",
    "# output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 189.633951 seconds\n",
      "Prediction time: 0.054330 seconds\n",
      "\n",
      "Accuracy: 0.808333\n",
      "Balanced Accuracy: 0.587563\n",
      "AUC: 0.785287\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_gbm import train_gbm\n",
    "from ipynb.fs.full.test_model import test_model\n",
    "from ipynb.fs.full.compute_metrics import compute_metrics\n",
    "\n",
    "if run_baseline == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_initial))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_initial))\n",
    "    \n",
    "    [train_time,baseline] = train_gbm(feature_train_initial,label_train_initial,\n",
    "                                      learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline,feature_test_initial)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_initial,label_test_initial,test_preds,baseline)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    #save baseline model\n",
    "    pickle.dump(baseline,open(\"../output/baseline.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_initial,\n",
    "                    'Feature Extraction Test Time':tm_feature_test_initial,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Baseline')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Advanced Model (SMOTEBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the advanced model, we decided to use SMOTEBoost, which is a modified version of XGBoost that uses SMOTE (Synethic Minority Oversampling Technique) (add reference). Our model also uses the improved features which do not double count distances, and the parameters were chosen from grid search with AUC scoring.\n",
    "\n",
    "The idea of SMOTE is to modify the imbalanced training data by first randomly undersampling from the majority class and then creating new synthetic minority data that is close to the existing feature space. The modified SMOTE features then have equal number of data in each class.\n",
    "\n",
    "We went with this model for a couple of reasons. First of all, it addresses the fact that the training data is imbalanced. It also has a higher AUC, accuracy, and balanced accuracy than the baseline GBM model. Finally, compared to the other candidates for the advanced model, it has the highest AUC and balanced accuracy from 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 2.554618 seconds\n",
      "Feature extraction time for test: 0.035055  seconds\n",
      "\n",
      "Training time: 204.890282 seconds\n",
      "Prediction time: 0.081375 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.670898\n",
      "AUC: 0.821778\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cd191fe09ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     'Balanced Accuracy':balanced_accuracy},name='Advanced (SMOTEBoost)')\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Advanced (SMOTEBoost)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_xgb import train_xgb\n",
    "from ipynb.fs.full.test_model import test_model\n",
    "from ipynb.fs.full.compute_metrics import compute_metrics\n",
    "\n",
    "if run_advanced == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, advanced] = train_xgb(feature_train_sm, label_train_sm, learning_rate=0.25, n_estimators=300,\n",
    "                                      max_depth=3,min_child_weight=1,objective='binary:logistic',scale_pos_weight=4)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "\n",
    "         \n",
    "    [prediction_time,test_preds] = test_model(advanced,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,advanced)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    pickle.dump(advanced,open(\"../output/advanced.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "                    'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Advanced (SMOTEBoost)')\n",
    "    model_results_df.loc['Advanced (SMOTEBoost)'] = row    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 202.863661 seconds\n",
      "Prediction time: 0.075305 seconds\n",
      "\n",
      "Classification Error: 0.175000\n",
      "Accuracy: 0.825000\n",
      "\n",
      "Balanced Accuracy: 0.719091\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89       473\n",
      "           1       0.60      0.54      0.56       127\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.74      0.72      0.73       600\n",
      "weighted avg       0.82      0.82      0.82       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[427  46]\n",
      " [ 59  68]]\n",
      "\n",
      "AUC: 0.804381\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e5661aecab05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     'Balanced Accuracy':balanced_accuracy},name='Advanced (SMOTEBoost)')\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m#model_results_df = model_results_df.append(row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Advanced (SMOTEBoost)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_labels_xgb = [ x  - 1 for x in label_train ] \n",
    "# test_labels_xgb = [ x  - 1 for x in label_test ] \n",
    "\n",
    "if run_advanced == True:\n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    over = SMOTE(sampling_strategy='auto')\n",
    "    under = RandomUnderSampler(sampling_strategy='auto')\n",
    "    sm = Pipeline(steps = [('o', over), ('u', under)])\n",
    "    feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "\n",
    "    \n",
    "    start_time=time.time()\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "     learning_rate =0.25,\n",
    "     n_estimators= 300,\n",
    "     max_depth=3,\n",
    "     min_child_weight=1,\n",
    "     objective= 'binary:logistic',\n",
    "     scale_pos_weight=4\n",
    "    )\n",
    "\n",
    "    xgb.fit(feature_train_sm, label_train_sm ,eval_metric='auc')\n",
    "    train_time = time.time()-start\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_preds = xgb.predict(feature_test)\n",
    "    end = time.time()\n",
    "    prediction_time = end-start_time\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = xgb.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "\n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "\n",
    "    test_probs = xgb.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    pickle.dump(xgb,open(\"../output/advanced.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "                    'Feature Extraction Test Time':tm_feature_test,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Advanced (SMOTEBoost)')\n",
    "    #model_results_df = model_results_df.append(row)\n",
    "    model_results_df.loc['Advanced (SMOTEBoost)'] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Step: Remaining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the other models that were candidates for the advanced model. (By default they are set to not run to save time on running this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model with Improved Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimal parameters\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "# output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 190.495405 seconds\n",
      "Prediction time: 0.027578 seconds\n",
      "\n",
      "Accuracy: 0.816667\n",
      "Balanced Accuracy: 0.610128\n",
      "AUC: 0.785287\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_xgb import train_xgb\n",
    "\n",
    "if run_baseline_improved == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, baseline_improved] = train_gbm(feature_train,label_train,\n",
    "                                                learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline_improved,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_initial,label_test_initial,test_preds,baseline)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "\n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "                'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "                 'Train Time':train_time,\n",
    "                 'Prediction Time':prediction_time,\n",
    "                'Accuracy':accuracy,\n",
    "                'AUC':auc,\n",
    "                'Balanced Accuracy':balanced_accuracy},name='Baseline with Improved Features')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3).fit(feature_train_PCA,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 6.596193 seconds\n",
      "Feature extraction time for test:  0.113220  seconds\n",
      "\n",
      "Training time: 1.203978 seconds\n",
      "Prediction time: 0.002393 seconds\n",
      "\n",
      "Accuracy: 0.790000\n",
      "Balanced Accuracy: 0.535616\n",
      "AUC: 0.689501\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c948e52571a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             'Balanced Accuracy':balanced_accuracy},name='Baseline with PCA')\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "if run_baseline_pca == True:\n",
    "\n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_PCA))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_PCA))\n",
    "    \n",
    "    [train_time, baseline_PCA] = train_gbm(feature_train_PCA,label_train_PCA,\n",
    "                                                learning_rate=0.1,max_depth=2,n_estimators=150)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline_PCA,feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_PCA,label_test_PCA,test_preds,baseline_PCA)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_PCA,\n",
    "            'Feature Extraction Test Time':tm_feature_test_PCA,\n",
    "             'Train Time':train_time,\n",
    "             'Prediction Time':prediction_time,\n",
    "            'Accuracy':accuracy,\n",
    "            'AUC':auc,\n",
    "            'Balanced Accuracy':balanced_accuracy},name='Baseline with PCA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':list(range(5,55,5))}\n",
    "#gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'n_neighbors': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.397339 seconds\n",
      "Prediction time: 5.921443 seconds\n",
      "\n",
      "Accuracy: 0.791667\n",
      "Balanced Accuracy: 0.516514\n",
      "AUC: 0.674527\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_knn import train_knn\n",
    "\n",
    "if run_knn == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, knn] = train_knn(feature_train,label_train,n_neighbors=25)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(knn,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,knn)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='KNN')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':list(range(5,55,5))}\n",
    "#gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {'n_neighbors': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.771669 seconds\n",
      "Prediction time: 8.114751 seconds\n",
      "\n",
      "Accuracy: 0.606667\n",
      "Balanced Accuracy: 0.638211\n",
      "AUC: 0.683691\n"
     ]
    }
   ],
   "source": [
    "if run_knn_smote == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, knn] = train_knn(feature_train_sm,label_train_sm,n_neighbors=5)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(knn,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,knn)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SMOTE KNN')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 89.071407 seconds\n",
      "Prediction time: 0.079029 seconds\n",
      "\n",
      "Classification Error: 0.186667\n",
      "Accuracy: 0.813333\n",
      "\n",
      "Balanced Accuracy: 0.688652\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       473\n",
      "           1       0.57      0.47      0.52       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.72      0.69      0.70       600\n",
      "weighted avg       0.80      0.81      0.81       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[428  45]\n",
      " [ 67  60]]\n",
      "\n",
      "AUC: 0.808726\n"
     ]
    }
   ],
   "source": [
    "if run_xgboost == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f} seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, xgb] = train_xgb(feature_train, label_train, learning_rate=0.1, n_estimators=200,\n",
    "                                      max_depth=3,min_child_weight=1,objective='binary:logistic',scale_pos_weight=4)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "\n",
    "         \n",
    "    [prediction_time,test_preds] = test_model(xgb,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "\n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,xgb)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='XGBoost')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 7.173245 seconds\n",
      "Prediction time: 0.031935 seconds\n",
      "\n",
      "Accuracy: 0.808333\n",
      "Balanced Accuracy: 0.567404\n",
      "AUC: 0.746608\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c0d022623e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         'Balanced Accuracy':balanced_accuracy},name='Random Forest')\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_random_forest import train_random_forest\n",
    "\n",
    "if run_random_forest==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, rf_model] = train_random_forest(feature_train,label_train,n_estimators=100,criterion='gini',min_samples_leaf=1,max_features='sqrt')\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(rf_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,rf_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Random Forest')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.055222 seconds\n",
      "Prediction time: 0.019049 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.636339\n",
      "AUC: 0.786203\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-27d27ee1793f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         'Balanced Accuracy':balanced_accuracy},name='LDA')\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_lda import train_lda\n",
    "\n",
    "if run_LDA==True: \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, lda_model] = train_lda(feature_train, label_train,solver='eigen', shrinkage=.1, n_components=1)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lda_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lda_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='LDA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 104.362289 seconds\n",
      "Prediction time: 0.026762 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.699697\n",
      "AUC: 0.822310\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8f1b342fa020>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         'Balanced Accuracy':balanced_accuracy},name='Logistic Regression')\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_logistic import train_logistic\n",
    "\n",
    "if run_logistic==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time,lr_model] = train_logistic(feature_train,label_train,\n",
    "                                           C=0.01, dual=False, fit_intercept=True,\n",
    "                                           intercept_scaling=1, max_iter=1200000,\n",
    "                                           multi_class='multinomial', penalty='l2',\n",
    "                                           solver='lbfgs', tol=0.0001,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lr_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lr_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Logistic Regression')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#weights = {0:80.0, 1:20.0}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001,class_weight=weights),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 128.495774 seconds\n",
      "Prediction time: 0.023449 seconds\n",
      "\n",
      "Classification Error: 0.170000\n",
      "Accuracy: 0.830000\n",
      "\n",
      "Balanced Accuracy: 0.679063\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       473\n",
      "           1       0.65      0.42      0.51       127\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.76      0.68      0.70       600\n",
      "weighted avg       0.81      0.83      0.82       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[445  28]\n",
      " [ 74  53]]\n",
      "\n",
      "AUC: 0.822843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_logistic import train_logistic\n",
    "if run_weighted_logistic==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time,lr_model] = train_logistic(feature_train,label_train,\n",
    "                                           C=0.001, dual=False, fit_intercept=True,\n",
    "                                           intercept_scaling=1, max_iter=120000000000,\n",
    "                                           multi_class='multinomial', penalty='l2',\n",
    "                                           solver='lbfgs', tol=0.0001,class_weight=weights)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lr_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lr_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted Logistic')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.00001,0.0001,0.001,0.01,1,10],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 84.079776 seconds\n",
      "Prediction time: 1.913208 seconds\n",
      "\n",
      "Accuracy: 0.855000\n",
      "Balanced Accuracy: 0.683400\n",
      "AUC: 0.828070\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3d07836bed86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         'Balanced Accuracy':balanced_accuracy},name='SVM')\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_svm import train_svm\n",
    "\n",
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time,svm_model] = train_svm(feature_train,label_train,\n",
    "                                      C=10, kernel='poly', degree=4,\n",
    "                                      probability=True,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,svm_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SVM')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.001,0.01,1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train_PCA,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 2, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 6.596193 seconds\n",
      "Feature extraction time for test:  0.113220  seconds\n",
      "\n",
      "Training time: 0.791071 seconds\n",
      "Prediction time: 0.017091 seconds\n",
      "\n",
      "Accuracy: 0.786667\n",
      "Balanced Accuracy: 0.585341\n",
      "AUC: 0.745118\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-70ca835275ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         'Balanced Accuracy':balanced_accuracy},name='SVM with PCA')\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "#improved svm with PCA\n",
    "from ipynb.fs.full.train_svm import train_svm\n",
    "\n",
    "if run_svm==True:    \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_PCA))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_PCA))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time, svm_PCA] = train_svm(feature_train_PCA,label_train_PCA,\n",
    "                                      C=10,degree=2,kernel='rbf',probability=True,\n",
    "                                     class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_PCA,feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_PCA,label_test_PCA,test_preds,svm_PCA)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_PCA,\n",
    "        'Feature Extraction Test Time':tm_feature_test_PCA,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SVM with PCA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# params= {'C': [1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(SVC(class_weight=weights,random_state = 2020,probability=True), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.train_svm import train_svm\n",
    "\n",
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:  \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time,svm_model] = train_svm(feature_train,label_train,\n",
    "                                      C=10, kernel='poly', degree=4,\n",
    "                                      probability=True,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,svm_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted SVM')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-eabcd5e8ed90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_naivebayes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgnb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_naive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time: {:4f} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/applied_ds/Fall2020-Project3-group4/lib/train_naive_bayes.ipynb\u001b[0m in \u001b[0;36mtrain_naive_bayes\u001b[0;34m(feature_train, label_train)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"from sklearn.naive_bayes import GaussianNB\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"import time\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m    ]\n\u001b[0m\u001b[1;32m     12\u001b[0m   },\n\u001b[1;32m     13\u001b[0m   {\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_naive_bayes import train_naive_bayes\n",
    "\n",
    "if run_naivebayes == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time,gnb] = train_naive_bayes(feature_train,label_train)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(gnb,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,gnb)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Naive Bayes')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 318.815320 seconds\n",
      "Prediction time: 0.017846 seconds\n",
      "\n",
      "Accuracy: 0.833333\n",
      "Balanced Accuracy: 0.718616\n",
      "AUC: 0.828403\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8a0411d617ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#Note: AUC is a better metric than accuracy because of imbalanced classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nAUC: {:4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_probs' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_lasso import train_lasso\n",
    "\n",
    "if run_lasso==True:  \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time, lasso_model] = train_lasso(feature_train,label_train,\n",
    "                                           penalty='l1',solver='liblinear',\n",
    "                                           class_weight=weights)\n",
    "\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lasso_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lasso_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "\n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Lasso')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# param= {'solver':['liblinear', 'saga']}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(LogisticRegression(penalty='l1', class_weight=weights), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 59.915938 seconds\n",
      "Prediction time: 0.017789 seconds\n",
      "\n",
      "Classification Error: 0.168333\n",
      "Accuracy: 0.831667\n",
      "\n",
      "Balanced Accuracy: 0.622522\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90       473\n",
      "           1       0.82      0.26      0.40       127\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.83      0.62      0.65       600\n",
      "weighted avg       0.83      0.83      0.79       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[466   7]\n",
      " [ 94  33]]\n",
      "\n",
      "AUC: 0.819414\n"
     ]
    }
   ],
   "source": [
    "if run_weighted_lasso==True: \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time, lasso_model] = train_lasso(feature_train,label_train,\n",
    "                                           penalty='l1',solver='liblinear',\n",
    "                                           class_weight=weights)\n",
    "\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lasso_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lasso_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted Lasso')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_estimators':[25,50,75,100]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(BaggingClassifier(),params,cv=cv,scoring='roc_auc').fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {{'n_estimators': 100}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 2.474462 seconds\n",
      "Feature extraction time for test: 0.039230  seconds\n",
      "\n",
      "Training time: 662.785580 seconds\n",
      "Prediction time: 0.519333 seconds\n",
      "\n",
      "Accuracy: 0.813333\n",
      "Balanced Accuracy: 0.662732\n",
      "AUC: 0.788975\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-24a0443e090f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         'Balanced Accuracy':balanced_accuracy},name='SMOTE Bagging')\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_bagging import train_bagging\n",
    "\n",
    "if run_bagging_smote == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, smote_bagging] = train_bagging(feature_train_sm,label_train_sm,n_estimators=100) \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(smote_bagging,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "\n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,smote_bagging)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SMOTE Bagging')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After undersampling/oversampling, we now have equal number of members in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   1929 \n",
      "Number of records with label 1 (complex emotion): 1929 \n"
     ]
    }
   ],
   "source": [
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(len(label_train_sm)-sum(label_train_sm)))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(sum(label_train_sm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted lasso\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(lasso_w, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8445833333333332\n",
    "# roc_auc output:0.8010359440647241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted svm\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(svm, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8195833333333333\n",
    "# roc_auc output:0.8117648633580459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted Logistic\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(lr, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8300000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with SMOTE\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(xgb_sm, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8300000000000001\n",
    "# roc_auc output:0.8387998261113715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Extraction Train Time</th>\n",
       "      <th>Feature Extraction Test Time</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>5.087896</td>\n",
       "      <td>1.252400</td>\n",
       "      <td>186.447609</td>\n",
       "      <td>0.050044</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.595437</td>\n",
       "      <td>0.785237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advanced (SMOTEBoost)</th>\n",
       "      <td>2.307424</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>207.430647</td>\n",
       "      <td>0.070880</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.712274</td>\n",
       "      <td>0.825024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with Improved Features</th>\n",
       "      <td>0.148918</td>\n",
       "      <td>0.033018</td>\n",
       "      <td>178.205290</td>\n",
       "      <td>0.029718</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.606191</td>\n",
       "      <td>0.798672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with PCA</th>\n",
       "      <td>8.117146</td>\n",
       "      <td>0.039645</td>\n",
       "      <td>1.319941</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.527742</td>\n",
       "      <td>0.691332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.151489</td>\n",
       "      <td>0.035817</td>\n",
       "      <td>0.419009</td>\n",
       "      <td>6.096245</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.516514</td>\n",
       "      <td>0.674527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE KNN</th>\n",
       "      <td>2.284506</td>\n",
       "      <td>0.029388</td>\n",
       "      <td>0.747403</td>\n",
       "      <td>8.509360</td>\n",
       "      <td>0.578333</td>\n",
       "      <td>0.623121</td>\n",
       "      <td>0.682376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.152626</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.688652</td>\n",
       "      <td>0.808726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.153098</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.574221</td>\n",
       "      <td>0.752093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.155403</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.636339</td>\n",
       "      <td>0.786203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.166774</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>100.128016</td>\n",
       "      <td>0.024673</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.699697</td>\n",
       "      <td>0.822310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Logistic</th>\n",
       "      <td>0.156370</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>128.495772</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.679063</td>\n",
       "      <td>0.822843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.149456</td>\n",
       "      <td>0.030665</td>\n",
       "      <td>77.353649</td>\n",
       "      <td>1.641004</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.828137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM with PCA</th>\n",
       "      <td>7.166126</td>\n",
       "      <td>0.030425</td>\n",
       "      <td>77.353649</td>\n",
       "      <td>1.641004</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.585341</td>\n",
       "      <td>0.745093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted SVM</th>\n",
       "      <td>0.143586</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>77.353649</td>\n",
       "      <td>1.641004</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.717851</td>\n",
       "      <td>0.837209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.148540</td>\n",
       "      <td>0.028759</td>\n",
       "      <td>0.128203</td>\n",
       "      <td>0.034613</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.628073</td>\n",
       "      <td>0.660369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.147932</td>\n",
       "      <td>0.033399</td>\n",
       "      <td>108.838572</td>\n",
       "      <td>0.018197</td>\n",
       "      <td>0.828333</td>\n",
       "      <td>0.692406</td>\n",
       "      <td>0.824241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Lasso</th>\n",
       "      <td>0.132466</td>\n",
       "      <td>0.032575</td>\n",
       "      <td>59.916026</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>0.831667</td>\n",
       "      <td>0.622522</td>\n",
       "      <td>0.819414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Bagging</th>\n",
       "      <td>2.143459</td>\n",
       "      <td>0.025363</td>\n",
       "      <td>648.656513</td>\n",
       "      <td>0.470587</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.629414</td>\n",
       "      <td>0.777122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Feature Extraction Train Time  \\\n",
       "Baseline                                              5.087896   \n",
       "Advanced (SMOTEBoost)                                 2.307424   \n",
       "Baseline with Improved Features                       0.148918   \n",
       "Baseline with PCA                                     8.117146   \n",
       "KNN                                                   0.151489   \n",
       "SMOTE KNN                                             2.284506   \n",
       "XGBoost                                               0.152626   \n",
       "Random Forest                                         0.153098   \n",
       "LDA                                                   0.155403   \n",
       "Logistic Regression                                   0.166774   \n",
       "Weighted Logistic                                     0.156370   \n",
       "SVM                                                   0.149456   \n",
       "SVM with PCA                                          7.166126   \n",
       "Weighted SVM                                          0.143586   \n",
       "Naive Bayes                                           0.148540   \n",
       "Lasso                                                 0.147932   \n",
       "Weighted Lasso                                        0.132466   \n",
       "SMOTE Bagging                                         2.143459   \n",
       "\n",
       "                                 Feature Extraction Test Time  Train Time  \\\n",
       "Baseline                                             1.252400  186.447609   \n",
       "Advanced (SMOTEBoost)                                0.035263  207.430647   \n",
       "Baseline with Improved Features                      0.033018  178.205290   \n",
       "Baseline with PCA                                    0.039645    1.319941   \n",
       "KNN                                                  0.035817    0.419009   \n",
       "SMOTE KNN                                            0.029388    0.747403   \n",
       "XGBoost                                              0.040548    0.040548   \n",
       "Random Forest                                        0.032813    0.040548   \n",
       "LDA                                                  0.032234    0.040548   \n",
       "Logistic Regression                                  0.031862  100.128016   \n",
       "Weighted Logistic                                    0.031693  128.495772   \n",
       "SVM                                                  0.030665   77.353649   \n",
       "SVM with PCA                                         0.030425   77.353649   \n",
       "Weighted SVM                                         0.029650   77.353649   \n",
       "Naive Bayes                                          0.028759    0.128203   \n",
       "Lasso                                                0.033399  108.838572   \n",
       "Weighted Lasso                                       0.032575   59.916026   \n",
       "SMOTE Bagging                                        0.025363  648.656513   \n",
       "\n",
       "                                 Prediction Time  Accuracy  Balanced Accuracy  \\\n",
       "Baseline                                0.050044  0.811667           0.595437   \n",
       "Advanced (SMOTEBoost)                   0.070880  0.823333           0.712274   \n",
       "Baseline with Improved Features         0.029718  0.815000           0.606191   \n",
       "Baseline with PCA                       0.001212  0.786667           0.527742   \n",
       "KNN                                     6.096245  0.791667           0.516514   \n",
       "SMOTE KNN                               8.509360  0.578333           0.623121   \n",
       "XGBoost                                 0.040548  0.813333           0.688652   \n",
       "Random Forest                           0.040548  0.810000           0.574221   \n",
       "LDA                                     0.040548  0.821667           0.636339   \n",
       "Logistic Regression                     0.024673  0.821667           0.699697   \n",
       "Weighted Logistic                       0.023447  0.830000           0.679063   \n",
       "SVM                                     1.641004  0.855000           0.683400   \n",
       "SVM with PCA                            1.641004  0.786667           0.585341   \n",
       "Weighted SVM                            1.641004  0.836667           0.717851   \n",
       "Naive Bayes                             0.034613  0.663333           0.628073   \n",
       "Lasso                                   0.018197  0.828333           0.692406   \n",
       "Weighted Lasso                          0.017855  0.831667           0.622522   \n",
       "SMOTE Bagging                           0.470587  0.801667           0.629414   \n",
       "\n",
       "                                      AUC  \n",
       "Baseline                         0.785237  \n",
       "Advanced (SMOTEBoost)            0.825024  \n",
       "Baseline with Improved Features  0.798672  \n",
       "Baseline with PCA                0.691332  \n",
       "KNN                              0.674527  \n",
       "SMOTE KNN                        0.682376  \n",
       "XGBoost                          0.808726  \n",
       "Random Forest                    0.752093  \n",
       "LDA                              0.786203  \n",
       "Logistic Regression              0.822310  \n",
       "Weighted Logistic                0.822843  \n",
       "SVM                              0.828137  \n",
       "SVM with PCA                     0.745093  \n",
       "Weighted SVM                     0.837209  \n",
       "Naive Bayes                      0.660369  \n",
       "Lasso                            0.824241  \n",
       "Weighted Lasso                   0.819414  \n",
       "SMOTE Bagging                    0.777122  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_df = model_results_df.applymap(lambda x: round(x,3))\n",
    "model_results_df['Train Time'] = [str(x)+' s' for x in list(model_results_df['Train Time'])]\n",
    "model_results_df['Prediction Time'] = [str(x)+' s' for x in list(model_results_df['Prediction Time'])]\n",
    "model_results_df['Feature Extraction Train Time'] = [str(x)+' s' for x in list(model_results_df['Feature Extraction Train Time'])]\n",
    "model_results_df['Feature Extraction Test Time'] = [str(x)+' s' for x in list(model_results_df['Feature Extraction Test Time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Extraction Train Time</th>\n",
       "      <th>Feature Extraction Test Time</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>5.088 s</td>\n",
       "      <td>1.252 s</td>\n",
       "      <td>186.448 s</td>\n",
       "      <td>0.05 s</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advanced (SMOTEBoost)</th>\n",
       "      <td>2.307 s</td>\n",
       "      <td>0.035 s</td>\n",
       "      <td>207.431 s</td>\n",
       "      <td>0.071 s</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with Improved Features</th>\n",
       "      <td>0.149 s</td>\n",
       "      <td>0.033 s</td>\n",
       "      <td>178.205 s</td>\n",
       "      <td>0.03 s</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with PCA</th>\n",
       "      <td>8.117 s</td>\n",
       "      <td>0.04 s</td>\n",
       "      <td>1.32 s</td>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.151 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>0.419 s</td>\n",
       "      <td>6.096 s</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE KNN</th>\n",
       "      <td>2.285 s</td>\n",
       "      <td>0.029 s</td>\n",
       "      <td>0.747 s</td>\n",
       "      <td>8.509 s</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.153 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.153 s</td>\n",
       "      <td>0.033 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.155 s</td>\n",
       "      <td>0.032 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.167 s</td>\n",
       "      <td>0.032 s</td>\n",
       "      <td>100.128 s</td>\n",
       "      <td>0.025 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Logistic</th>\n",
       "      <td>0.156 s</td>\n",
       "      <td>0.032 s</td>\n",
       "      <td>128.496 s</td>\n",
       "      <td>0.023 s</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.149 s</td>\n",
       "      <td>0.031 s</td>\n",
       "      <td>77.354 s</td>\n",
       "      <td>1.641 s</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM with PCA</th>\n",
       "      <td>7.166 s</td>\n",
       "      <td>0.03 s</td>\n",
       "      <td>77.354 s</td>\n",
       "      <td>1.641 s</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted SVM</th>\n",
       "      <td>0.144 s</td>\n",
       "      <td>0.03 s</td>\n",
       "      <td>77.354 s</td>\n",
       "      <td>1.641 s</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.149 s</td>\n",
       "      <td>0.029 s</td>\n",
       "      <td>0.128 s</td>\n",
       "      <td>0.035 s</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.148 s</td>\n",
       "      <td>0.033 s</td>\n",
       "      <td>108.839 s</td>\n",
       "      <td>0.018 s</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Lasso</th>\n",
       "      <td>0.132 s</td>\n",
       "      <td>0.033 s</td>\n",
       "      <td>59.916 s</td>\n",
       "      <td>0.018 s</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Bagging</th>\n",
       "      <td>2.143 s</td>\n",
       "      <td>0.025 s</td>\n",
       "      <td>648.657 s</td>\n",
       "      <td>0.471 s</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Feature Extraction Train Time  \\\n",
       "Baseline                                              5.088 s   \n",
       "Advanced (SMOTEBoost)                                 2.307 s   \n",
       "Baseline with Improved Features                       0.149 s   \n",
       "Baseline with PCA                                     8.117 s   \n",
       "KNN                                                   0.151 s   \n",
       "SMOTE KNN                                             2.285 s   \n",
       "XGBoost                                               0.153 s   \n",
       "Random Forest                                         0.153 s   \n",
       "LDA                                                   0.155 s   \n",
       "Logistic Regression                                   0.167 s   \n",
       "Weighted Logistic                                     0.156 s   \n",
       "SVM                                                   0.149 s   \n",
       "SVM with PCA                                          7.166 s   \n",
       "Weighted SVM                                          0.144 s   \n",
       "Naive Bayes                                           0.149 s   \n",
       "Lasso                                                 0.148 s   \n",
       "Weighted Lasso                                        0.132 s   \n",
       "SMOTE Bagging                                         2.143 s   \n",
       "\n",
       "                                Feature Extraction Test Time Train Time  \\\n",
       "Baseline                                             1.252 s  186.448 s   \n",
       "Advanced (SMOTEBoost)                                0.035 s  207.431 s   \n",
       "Baseline with Improved Features                      0.033 s  178.205 s   \n",
       "Baseline with PCA                                     0.04 s     1.32 s   \n",
       "KNN                                                  0.036 s    0.419 s   \n",
       "SMOTE KNN                                            0.029 s    0.747 s   \n",
       "XGBoost                                              0.041 s    0.041 s   \n",
       "Random Forest                                        0.033 s    0.041 s   \n",
       "LDA                                                  0.032 s    0.041 s   \n",
       "Logistic Regression                                  0.032 s  100.128 s   \n",
       "Weighted Logistic                                    0.032 s  128.496 s   \n",
       "SVM                                                  0.031 s   77.354 s   \n",
       "SVM with PCA                                          0.03 s   77.354 s   \n",
       "Weighted SVM                                          0.03 s   77.354 s   \n",
       "Naive Bayes                                          0.029 s    0.128 s   \n",
       "Lasso                                                0.033 s  108.839 s   \n",
       "Weighted Lasso                                       0.033 s   59.916 s   \n",
       "SMOTE Bagging                                        0.025 s  648.657 s   \n",
       "\n",
       "                                Prediction Time  Accuracy  Balanced Accuracy  \\\n",
       "Baseline                                 0.05 s     0.812              0.595   \n",
       "Advanced (SMOTEBoost)                   0.071 s     0.823              0.712   \n",
       "Baseline with Improved Features          0.03 s     0.815              0.606   \n",
       "Baseline with PCA                       0.001 s     0.787              0.528   \n",
       "KNN                                     6.096 s     0.792              0.517   \n",
       "SMOTE KNN                               8.509 s     0.578              0.623   \n",
       "XGBoost                                 0.041 s     0.813              0.689   \n",
       "Random Forest                           0.041 s     0.810              0.574   \n",
       "LDA                                     0.041 s     0.822              0.636   \n",
       "Logistic Regression                     0.025 s     0.822              0.700   \n",
       "Weighted Logistic                       0.023 s     0.830              0.679   \n",
       "SVM                                     1.641 s     0.855              0.683   \n",
       "SVM with PCA                            1.641 s     0.787              0.585   \n",
       "Weighted SVM                            1.641 s     0.837              0.718   \n",
       "Naive Bayes                             0.035 s     0.663              0.628   \n",
       "Lasso                                   0.018 s     0.828              0.692   \n",
       "Weighted Lasso                          0.018 s     0.832              0.623   \n",
       "SMOTE Bagging                           0.471 s     0.802              0.629   \n",
       "\n",
       "                                   AUC  \n",
       "Baseline                         0.785  \n",
       "Advanced (SMOTEBoost)            0.825  \n",
       "Baseline with Improved Features  0.799  \n",
       "Baseline with PCA                0.691  \n",
       "KNN                              0.675  \n",
       "SMOTE KNN                        0.682  \n",
       "XGBoost                          0.809  \n",
       "Random Forest                    0.752  \n",
       "LDA                              0.786  \n",
       "Logistic Regression              0.822  \n",
       "Weighted Logistic                0.823  \n",
       "SVM                              0.828  \n",
       "SVM with PCA                     0.745  \n",
       "Weighted SVM                     0.837  \n",
       "Naive Bayes                      0.660  \n",
       "Lasso                            0.824  \n",
       "Weighted Lasso                   0.819  \n",
       "SMOTE Bagging                    0.777  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
