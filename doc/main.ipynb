{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install ipynb' in the command line. This code lets us import functions from notebooks in the lib folder. Lib has all of the feature extraction and model training/predicting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import sys\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install imblearn' in the command line. This code lets us do SMOTE (synthetic minority oversampling technique) and random undersampling to help deal with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the remaining libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import scipy.io\n",
    "import pickle\n",
    "import bz2\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "from sklearn.metrics import pairwise_distances, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Work Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the directories for the training set points and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/train_set/'\n",
    "train_image_dir = train_dir+\"images/\"\n",
    "train_pt_dir = train_dir+\"points/\"\n",
    "train_label_path = train_dir+\"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set Up Controls\n",
    "\n",
    "In this cell, we have a set of controls for the feature extraction. If true, then we process the features from scratch, and if false, then we load existing features from files in the output folder. \n",
    "\n",
    "+ (T/F) initial feature extraction on training set\n",
    "+ (T/F) initial feature extraction on test set\n",
    "\n",
    "+ (T/F) improved feature extraction on training set\n",
    "+ (T/F) improved feature extraction on test set\n",
    "\n",
    "+ (T/F) SMOTE using improved features on train set\n",
    "\n",
    "+ (T/F) PCA using improved features on training set and test set (doesn't make sense to only do one from scratch and not the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_feature_train_initial = True\n",
    "run_feature_test_initial = True\n",
    "\n",
    "run_feature_train = True \n",
    "run_feature_test = True \n",
    "\n",
    "run_feature_train_SMOTE = True\n",
    "\n",
    "run_feature_PCA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we have a set of controls for model training/testing. If true, then we train the model and generate predictions on the test set, and if false, then we skip that model. By default only the baseline and advanced models are set to run, but you can set the other models to be True to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline = True\n",
    "run_advanced = True\n",
    "\n",
    "run_baseline_improved = True\n",
    "run_baseline_pca = True\n",
    "run_knn = True\n",
    "run_knn_smote = True\n",
    "run_xgboost=True\n",
    "feature_initial=True\n",
    "run_random_forest=True\n",
    "run_LDA=True\n",
    "run_logistic=True\n",
    "run_weighted_logistic=True\n",
    "run_svm = True\n",
    "run_svm_pca = True\n",
    "run_weighted_svm = True\n",
    "run_lasso = True\n",
    "run_weighted_lasso = True\n",
    "run_bagging_smote = True\n",
    "run_naivebayes = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Data and Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data, and we can see that the dataset is imbalanced and that there are more records with basic emotions than records with complex emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   2402 \n",
      "Number of records with label 1 (complex emotion): 598 \n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "n = info.shape[0]\n",
    "\n",
    "#Data is imbalanced \n",
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(info.loc[info['label']==0].shape[0]))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(info.loc[info['label']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do an 80-20 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(round(n*(4/5),0))\n",
    "train_idx = np.random.choice(list(info.index),size=n_train,replace=False)\n",
    "test_idx = list(set(list(info.index))-set(train_idx)) #set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial points are stored in matlab format. In this step, we read them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read fiducial points\n",
    "#input: index\n",
    "#output: matrix of fiducial points corresponding to the index\n",
    "\n",
    "n_files = len(os.listdir(train_pt_dir))\n",
    "\n",
    "def readMat_matrix(index):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinatesUnwarped']\n",
    "    except KeyError:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinates2']\n",
    "    return np.matrix.round(mat_data,0)\n",
    "\n",
    "#load fiducial points into list and store them in output\n",
    "fiducial_pt_list = list(map(readMat_matrix,list(range(1,n_files+1))))\n",
    "pickle.dump(fiducial_pt_list, open( \"../output/fiducial_pt_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Construct Features and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starter Code Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature.ipynb's feature_initial function to generate pairwise distance features for the baseline model. This is the same feature extraction method as that of the starter code. Note that this method counts distances from x-axis and from y-axis separately between points.\n",
    "\n",
    "Feature extraction times exclude the time it takes to write to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature extraction time for train: 5.857164\n",
      "Initial feature extraction time for test:  1.419747\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_initial\n",
    "\n",
    "tm_feature_train_intitial = np.nan\n",
    "if run_feature_train_initial == True:\n",
    "    start = time.time()\n",
    "    dat_train_initial = feature_initial(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_initial = end-start\n",
    "    with bz2.BZ2File('../output/train_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_initial, f)\n",
    "    print('Initial feature extraction time for train: {:4f}'.format(tm_feature_train_initial))\n",
    "else:\n",
    "    dat_train_initial = cPickle.load(bz2.BZ2File('../output/train_data_initial.pbz2', 'rb'))\n",
    "        \n",
    "        \n",
    "tm_feature_test_initial = np.nan\n",
    "if run_feature_test_initial == True:\n",
    "    start = time.time()\n",
    "    dat_test_initial = feature_initial(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test_initial = end-start\n",
    "    with bz2.BZ2File('../output/test_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test_initial, f)\n",
    "    print('Initial feature extraction time for test:  {:4f}'.format(tm_feature_test_initial))\n",
    "else:\n",
    "    dat_test_initial = cPickle.load(bz2.BZ2File('../output/test_data_initial.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_initial = dat_train_initial.loc[:, dat_train_initial.columns != 'labels']\n",
    "label_train_initial = dat_train_initial['labels']\n",
    "\n",
    "feature_test_initial = dat_test_initial.loc[:, dat_test_initial.columns != 'labels']\n",
    "label_test_initial = dat_test_initial['labels'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature's feature_improved function to generate pairwise euclidean distance features to be used by all of the models other than the baseline. Since feature_improved just uses a single euclidean distance value rather than separate x-distance and y-distance values, feature_improved produces exactly half as many features as feature_initial while keeping the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved feature extraction time for train: 0.187968\n",
      "Improved feature extraction time for test:  0.040353\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_improved\n",
    "\n",
    "tm_feature_train_improved = np.nan\n",
    "if run_feature_train == True:\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_improved = end-start\n",
    "    with bz2.BZ2File('../output/train_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train, f)\n",
    "    print('Improved feature extraction time for train: {:4f}'.format(tm_feature_train_improved))\n",
    "else:\n",
    "    dat_train = cPickle.load(bz2.BZ2File('../output/train_data.pbz2', 'rb'))\n",
    "\n",
    "\n",
    "tm_feature_test_improved = np.nan\n",
    "if run_feature_test == True:\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test_improved = end-start\n",
    "    with bz2.BZ2File('../output/test_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test, f)\n",
    "    print('Improved feature extraction time for test:  {:4f}'.format(tm_feature_test_improved))\n",
    "else:\n",
    "    dat_test = cPickle.load(bz2.BZ2File('../output/test_data.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "label_train = dat_train['labels'] \n",
    "\n",
    "feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "label_test = dat_test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the feature extraction for SMOTE which will be discussed more in the advanced model section. SMOTE is only done on the training data and not on the test data. SMOTE is a modification of the improved features. \n",
    "\n",
    "If the improved features are obtained from scratch, then we include the time it takes to get the improved features with the time it takes to use SMOTE. Otherwise, in the case where the improved features are loaded from the disk, we just use the time it takes to use SMOTE on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE feature extraction time for train: 2.498043\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_SMOTE\n",
    "\n",
    "tm_feature_train_SMOTE = np.nan\n",
    "if run_feature_train_SMOTE == True:\n",
    "    start = time.time()\n",
    "    dat_train_SMOTE = feature_SMOTE(dat_train)\n",
    "    end = time.time()\n",
    "    if pd.isnull(tm_feature_train_improved):\n",
    "        tm_feature_train_SMOTE = end-start\n",
    "    else:\n",
    "        tm_feature_train_SMOTE = (end-start)+tm_feature_train_improved\n",
    "    with bz2.BZ2File('../output/train_data_SMOTE' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_SMOTE, f)\n",
    "    print('SMOTE feature extraction time for train: {:4f}'.format(tm_feature_train_SMOTE))\n",
    "else:\n",
    "    dat_train_SMOTE = cPickle.load(bz2.BZ2File('../output/train_data_SMOTE.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_sm = dat_train_SMOTE.loc[:,dat_train_SMOTE.columns!='labels']\n",
    "label_train_sm = dat_train_SMOTE['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After undersampling/oversampling, we now have equal number of members in each class (move this to feature section later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   1929 \n",
      "Number of records with label 1 (complex emotion): 1929 \n"
     ]
    }
   ],
   "source": [
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(len(label_train_sm)-sum(label_train_sm)))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(sum(label_train_sm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here we do PCA which is only done for the model candidates that were not chosen for the advanced model. PCA is done as a modification of the improved features. Also, it doesn't make sense to only do the PCA transformation on one of either the training data or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA feature extraction time for train: 7.268351\n",
      "PCA feature extraction time for test:  0.118223\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_PCA\n",
    "\n",
    "tm_feature_train_PCA = np.nan\n",
    "tm_feature_test_PCA = np.nan\n",
    "\n",
    "if run_feature_PCA == True:\n",
    "    [dat_train_PCA, dat_test_PCA, tm_feature_train_PCA, tm_feature_test_PCA] = feature_PCA(dat_train,dat_test)\n",
    "    \n",
    "    if pd.isnull(tm_feature_train_improved)==False:\n",
    "        tm_feature_train_PCA = tm_feature_train_PCA+tm_feature_train_improved\n",
    "    if pd.isnull(tm_feature_test_improved)==False:\n",
    "        tm_feature_test_PCA = tm_feature_test_PCA+tm_feature_test_improved\n",
    "    \n",
    "    with bz2.BZ2File('../output/train_data_PCA' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_PCA, f)\n",
    "    with bz2.BZ2File('../output/test_data_PCA' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test_PCA, f)\n",
    "        \n",
    "    print('PCA feature extraction time for train: {:4f}'.format(tm_feature_train_PCA))\n",
    "    print('PCA feature extraction time for test:  {:4f}'.format(tm_feature_test_PCA))\n",
    "        \n",
    "else:\n",
    "    dat_train_PCA = cPickle.load(bz2.BZ2File('../output/train_data_PCA.pbz2', 'rb'))\n",
    "    dat_test_PCA = cPickle.load(bz2.BZ2File('../output/test_data_PCA.pbz2', 'rb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_PCA = dat_train_PCA.loc[:,dat_train_PCA.columns!='labels']\n",
    "label_train_PCA = dat_train_PCA['labels'] #labels are same as label_train\n",
    "\n",
    "feature_test_PCA = dat_test_PCA.loc[:,dat_test_PCA.columns!='labels']\n",
    "label_test_PCA = dat_test_PCA['labels'] #labels are same as label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Baseline Model ~58% Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discussing the models, we will go over the two main metrics we used for finding a better model than the baseline. Since the data is imbalanced, we examined AUC and balanced accuracy rather than the regular accuracy metric.\n",
    "\n",
    "AUC is the area under the ROC curve which measures the TP vs FP rate as the classification decision threshold changes over time.\n",
    "\n",
    "Balanced accuracy is given by the formula $$balanced\\_accuracy = \\frac{1}{2}(\\frac{TP}{TP+FN}+\\frac{TN}{TN+FP}).$$ \n",
    "\n",
    "\n",
    "Balanced accuracy is used for imbalanced data as an estimate for the accuracy if the data was balanced, so the true performance of our models on balanced data will be close to the balanced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame used to store all of the model results\n",
    "model_results_df = pd.DataFrame(columns=['Feature Extraction Train Time','Feature Extraction Test Time',\n",
    "                                         'Train Time','Prediction Time','Accuracy','Balanced Accuracy','AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model is a GBM fitted on the initial pairwise fiducial features. The parameters were chosen from a grid search with AUC scoring. Feature extraction times for all models are already known from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimal parameters\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train_initial,label_train_initial)\n",
    "#gscv.best_params_\n",
    "# output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 5.857164 seconds\n",
      "Feature extraction time for test: 1.419747  seconds\n",
      "\n",
      "Training time: 187.676153 seconds\n",
      "Prediction time: 0.059905 seconds\n",
      "\n",
      "Accuracy: 0.808333\n",
      "Balanced Accuracy: 0.587563\n",
      "AUC: 0.785287\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_gbm import train_gbm\n",
    "from ipynb.fs.full.test_model import test_model\n",
    "from ipynb.fs.full.compute_metrics import compute_metrics\n",
    "\n",
    "if run_baseline == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_initial))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_initial))\n",
    "    \n",
    "    [train_time,baseline] = train_gbm(feature_train_initial,label_train_initial,\n",
    "                                      learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline,feature_test_initial)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_initial,label_test_initial,test_preds,baseline)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    #save baseline model\n",
    "    pickle.dump(baseline,open(\"../output/baseline.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_initial,\n",
    "                    'Feature Extraction Test Time':tm_feature_test_initial,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Baseline')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Advanced Model (SMOTEBoost) ~70% Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the advanced model, we decided to use SMOTEBoost, which is a modified version of XGBoost that uses SMOTE (Synethic Minority Oversampling Technique) (add reference). Our model also uses the improved features which do not double count distances, and the parameters were chosen from grid search with AUC scoring.\n",
    "\n",
    "The idea of SMOTE is to modify the imbalanced training data by first randomly undersampling from the majority class and then creating new synthetic minority data that is close to the existing feature space. The modified SMOTE features then have equal number of data in each class.\n",
    "\n",
    "We went with this model for a couple of reasons. First of all, it addresses the fact that the training data is imbalanced. It also has a higher AUC, accuracy, and balanced accuracy than the baseline GBM model. Finally, compared to the other candidates for the advanced model, it has the highest AUC from 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 2.498043 seconds\n",
      "Feature extraction time for test:  0.040353  seconds\n",
      "\n",
      "Training time: 28.193406 seconds\n",
      "Prediction time: 0.105752 seconds\n",
      "\n",
      "Accuracy: 0.810000\n",
      "Balanced Accuracy: 0.706697\n",
      "AUC: 0.827371\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_xgb import train_xgb\n",
    "from ipynb.fs.full.test_model import test_model\n",
    "from ipynb.fs.full.compute_metrics import compute_metrics\n",
    "\n",
    "if run_advanced == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, advanced] = train_xgb(feature_train_sm, label_train_sm, learning_rate=0.25, n_estimators=300,\n",
    "                                      max_depth=3,min_child_weight=1,objective='binary:logistic',scale_pos_weight=4)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "\n",
    "         \n",
    "    [prediction_time,test_preds] = test_model(advanced,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,advanced)\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    pickle.dump(advanced,open(\"../output/advanced.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "                    'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Advanced (SMOTEBoost)')\n",
    "    model_results_df.loc['Advanced (SMOTEBoost)'] = row    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Step: Remaining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the other models that were candidates for the advanced model. \n",
    "(To Do: Make them not run by default and have a table stored as a pickle file to load up here)\n",
    "\n",
    "It takes about 30 minutes for the remaining models to finish if they are all set to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model with Improved Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimal parameters\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "# output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test:  0.038007  seconds\n",
      "\n",
      "Training time: 192.445398 seconds\n",
      "Prediction time: 0.032289 seconds\n",
      "\n",
      "Accuracy: 0.816667\n",
      "Balanced Accuracy: 0.610128\n",
      "AUC: 0.785287\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_xgb import train_xgb\n",
    "\n",
    "if run_baseline_improved == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, baseline_improved] = train_gbm(feature_train,label_train,\n",
    "                                                learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline_improved,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_initial,label_test_initial,test_preds,baseline)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "\n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "                'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "                 'Train Time':train_time,\n",
    "                 'Prediction Time':prediction_time,\n",
    "                'Accuracy':accuracy,\n",
    "                'AUC':auc,\n",
    "                'Balanced Accuracy':balanced_accuracy},name='Baseline with Improved Features')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3).fit(feature_train_PCA,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 6.889987 seconds\n",
      "Feature extraction time for test:  0.119862  seconds\n",
      "\n",
      "Training time: 1.294838 seconds\n",
      "Prediction time: 0.002415 seconds\n",
      "\n",
      "Accuracy: 0.790000\n",
      "Balanced Accuracy: 0.535616\n",
      "AUC: 0.692814\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_pca == True:\n",
    "\n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_PCA))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_PCA))\n",
    "    \n",
    "    [train_time, baseline_PCA] = train_gbm(feature_train_PCA,label_train_PCA,\n",
    "                                                learning_rate=0.1,max_depth=2,n_estimators=150)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline_PCA,feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_PCA,label_test_PCA,test_preds,baseline_PCA)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_PCA,\n",
    "            'Feature Extraction Test Time':tm_feature_test_PCA,\n",
    "             'Train Time':train_time,\n",
    "             'Prediction Time':prediction_time,\n",
    "            'Accuracy':accuracy,\n",
    "            'AUC':auc,\n",
    "            'Balanced Accuracy':balanced_accuracy},name='Baseline with PCA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':list(range(5,55,5))}\n",
    "#gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'n_neighbors': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test:  0.038007  seconds\n",
      "\n",
      "Training time: 0.381865 seconds\n",
      "Prediction time: 6.070336 seconds\n",
      "\n",
      "Accuracy: 0.791667\n",
      "Balanced Accuracy: 0.516514\n",
      "AUC: 0.674527\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_knn import train_knn\n",
    "\n",
    "if run_knn == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, knn] = train_knn(feature_train,label_train,n_neighbors=25)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(knn,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,knn)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='KNN')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':list(range(5,55,5))}\n",
    "#gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {'n_neighbors': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 2.512055 seconds\n",
      "Feature extraction time for test:  0.038007  seconds\n",
      "\n",
      "Training time: 0.762404 seconds\n",
      "Prediction time: 8.466854 seconds\n",
      "\n",
      "Accuracy: 0.606667\n",
      "Balanced Accuracy: 0.638211\n",
      "AUC: 0.683691\n"
     ]
    }
   ],
   "source": [
    "if run_knn_smote == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, knn] = train_knn(feature_train_sm,label_train_sm,n_neighbors=5)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(knn,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,knn)\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SMOTE KNN')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test:  0.038007 seconds\n",
      "\n",
      "Training time: 89.336492 seconds\n",
      "Prediction time: 0.087405 seconds\n",
      "\n",
      "Accuracy: 0.820000\n",
      "Balanced Accuracy: 0.620882\n",
      "AUC: 0.809126\n"
     ]
    }
   ],
   "source": [
    "if run_xgboost == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f} seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, xgb] = train_xgb(feature_train, label_train, learning_rate=0.1, n_estimators=200,\n",
    "                                      max_depth=3,min_child_weight=1,objective='binary:logistic',scale_pos_weight=4)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "\n",
    "         \n",
    "    [prediction_time,test_preds] = test_model(xgb,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "\n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,xgb)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='XGBoost')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 7.629287 seconds\n",
      "Prediction time: 0.039910 seconds\n",
      "\n",
      "Accuracy: 0.810000\n",
      "Balanced Accuracy: 0.562701\n",
      "AUC: 0.766485\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_random_forest import train_random_forest\n",
    "\n",
    "if run_random_forest==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, rf_model] = train_random_forest(feature_train,label_train,n_estimators=100,criterion='gini',min_samples_leaf=1,max_features='sqrt')\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(rf_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,rf_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Random Forest')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 7.088265 seconds\n",
      "Prediction time: 0.018853 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.636339\n",
      "AUC: 0.786203\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_lda import train_lda\n",
    "\n",
    "if run_LDA==True: \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, lda_model] = train_lda(feature_train, label_train,solver='eigen', shrinkage=.1, n_components=1)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lda_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lda_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='LDA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 119.274133 seconds\n",
      "Prediction time: 0.024061 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.699697\n",
      "AUC: 0.822310\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_logistic import train_logistic\n",
    "\n",
    "if run_logistic==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time,lr_model] = train_logistic(feature_train,label_train,\n",
    "                                           C=0.01, dual=False, fit_intercept=True,\n",
    "                                           intercept_scaling=1, max_iter=1200000,\n",
    "                                           multi_class='multinomial', penalty='l2',\n",
    "                                           solver='lbfgs', tol=0.0001,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lr_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lr_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Logistic Regression')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#weights = {0:80.0, 1:20.0}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001,class_weight=weights),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 145.245618 seconds\n",
      "Prediction time: 0.023004 seconds\n",
      "\n",
      "Accuracy: 0.830000\n",
      "Balanced Accuracy: 0.679063\n",
      "AUC: 0.822843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_logistic import train_logistic\n",
    "if run_weighted_logistic==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time,lr_model] = train_logistic(feature_train,label_train,\n",
    "                                           C=0.001, dual=False, fit_intercept=True,\n",
    "                                           intercept_scaling=1, max_iter=120000000000,\n",
    "                                           multi_class='multinomial', penalty='l2',\n",
    "                                           solver='lbfgs', tol=0.0001,class_weight=weights)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lr_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lr_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted Logistic')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.00001,0.0001,0.001,0.01,1,10],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 85.878458 seconds\n",
      "Prediction time: 1.919353 seconds\n",
      "\n",
      "Accuracy: 0.855000\n",
      "Balanced Accuracy: 0.683400\n",
      "AUC: 0.828070\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_svm import train_svm\n",
    "\n",
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time,svm_model] = train_svm(feature_train,label_train,\n",
    "                                      C=10, kernel='poly', degree=4,\n",
    "                                      probability=True,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,svm_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SVM')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.001,0.01,1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train_PCA,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 2, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 6.889987 seconds\n",
      "Feature extraction time for test:  0.119862  seconds\n",
      "\n",
      "Training time: 0.835471 seconds\n",
      "Prediction time: 0.020878 seconds\n",
      "\n",
      "Accuracy: 0.786667\n",
      "Balanced Accuracy: 0.585341\n",
      "AUC: 0.745118\n"
     ]
    }
   ],
   "source": [
    "#improved svm with PCA\n",
    "from ipynb.fs.full.train_svm import train_svm\n",
    "\n",
    "if run_svm==True:    \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_PCA))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_PCA))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time, svm_PCA] = train_svm(feature_train_PCA,label_train_PCA,\n",
    "                                      C=10,degree=2,kernel='rbf',probability=True,\n",
    "                                     class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_PCA,feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_PCA,label_test_PCA,test_preds,svm_PCA)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_PCA,\n",
    "        'Feature Extraction Test Time':tm_feature_test_PCA,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SVM with PCA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# params= {'C': [1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(SVC(class_weight=weights,random_state = 2020,probability=True), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 403.409784 seconds\n",
      "Prediction time: 1.722822 seconds\n",
      "\n",
      "Accuracy: 0.836667\n",
      "Balanced Accuracy: 0.717851\n",
      "AUC: 0.837193\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_svm import train_svm\n",
    "\n",
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:  \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time,svm_model] = train_svm(feature_train,label_train,\n",
    "                                      C=10, kernel='poly', degree=4,\n",
    "                                      probability=True,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,svm_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted SVM')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 0.127418 seconds\n",
      "Prediction time: 0.039222 seconds\n",
      "\n",
      "Accuracy: 0.663333\n",
      "Balanced Accuracy: 0.628073\n",
      "AUC: 0.660369\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_naive_bayes import train_naive_bayes\n",
    "\n",
    "if run_naivebayes == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time,gnb] = train_naive_bayes(feature_train,label_train)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(gnb,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,gnb)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Naive Bayes')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 212.432784 seconds\n",
      "Prediction time: 0.017244 seconds\n",
      "\n",
      "Accuracy: 0.825000\n",
      "Balanced Accuracy: 0.693171\n",
      "AUC: 0.826389\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_lasso import train_lasso\n",
    "\n",
    "if run_lasso==True:  \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time, lasso_model] = train_lasso(feature_train,label_train,\n",
    "                                           penalty='l1',solver='liblinear',\n",
    "                                           class_weight=weights)\n",
    "\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lasso_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lasso_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "\n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Lasso')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# param= {'solver':['liblinear', 'saga']}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(LogisticRegression(penalty='l1', class_weight=weights), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.193292 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 61.974368 seconds\n",
      "Prediction time: 0.017205 seconds\n",
      "\n",
      "Accuracy: 0.831667\n",
      "Balanced Accuracy: 0.622522\n",
      "AUC: 0.819164\n"
     ]
    }
   ],
   "source": [
    "if run_weighted_lasso==True: \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time, lasso_model] = train_lasso(feature_train,label_train,\n",
    "                                           penalty='l1',solver='liblinear',\n",
    "                                           class_weight=weights)\n",
    "\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lasso_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lasso_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted Lasso')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_estimators':[25,50,75,100]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(BaggingClassifier(),params,cv=cv,scoring='roc_auc').fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {{'n_estimators': 100}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 2.512055 seconds\n",
      "Feature extraction time for test: 0.038007  seconds\n",
      "\n",
      "Training time: 668.403124 seconds\n",
      "Prediction time: 0.653561 seconds\n",
      "\n",
      "Accuracy: 0.806667\n",
      "Balanced Accuracy: 0.632585\n",
      "AUC: 0.794535\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_bagging import train_bagging\n",
    "\n",
    "if run_bagging_smote == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, smote_bagging] = train_bagging(feature_train_sm,label_train_sm,n_estimators=100) \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(smote_bagging,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "\n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,smote_bagging)\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SMOTE Bagging')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted lasso\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(lasso_w, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8445833333333332\n",
    "# roc_auc output:0.8010359440647241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted svm\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(svm, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8195833333333333\n",
    "# roc_auc output:0.8117648633580459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted Logistic\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(lr, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8300000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with SMOTE\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(xgb_sm, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8300000000000001\n",
    "# roc_auc output:0.8387998261113715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Extraction Train Time</th>\n",
       "      <th>Feature Extraction Test Time</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>5.376008</td>\n",
       "      <td>1.175714</td>\n",
       "      <td>186.140792</td>\n",
       "      <td>0.055967</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.587563</td>\n",
       "      <td>0.785287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advanced (SMOTEBoost)</th>\n",
       "      <td>2.512055</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>206.047342</td>\n",
       "      <td>0.077489</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.670898</td>\n",
       "      <td>0.821778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with Improved Features</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>192.445398</td>\n",
       "      <td>0.032289</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.610128</td>\n",
       "      <td>0.785287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with PCA</th>\n",
       "      <td>6.889987</td>\n",
       "      <td>0.119862</td>\n",
       "      <td>1.294838</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.535616</td>\n",
       "      <td>0.692814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>0.381865</td>\n",
       "      <td>6.070336</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.516514</td>\n",
       "      <td>0.674527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE KNN</th>\n",
       "      <td>2.512055</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>0.762404</td>\n",
       "      <td>8.466854</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.638211</td>\n",
       "      <td>0.683691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>89.336492</td>\n",
       "      <td>0.087405</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.620882</td>\n",
       "      <td>0.809126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>7.629287</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.562701</td>\n",
       "      <td>0.766485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>7.088265</td>\n",
       "      <td>0.018853</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.636339</td>\n",
       "      <td>0.786203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>119.274133</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.699697</td>\n",
       "      <td>0.822310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Logistic</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>145.245618</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.679063</td>\n",
       "      <td>0.822843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>85.878458</td>\n",
       "      <td>1.919353</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.828070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM with PCA</th>\n",
       "      <td>6.889987</td>\n",
       "      <td>0.119862</td>\n",
       "      <td>0.835471</td>\n",
       "      <td>0.020878</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.585341</td>\n",
       "      <td>0.745118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted SVM</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>403.409784</td>\n",
       "      <td>1.722822</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.717851</td>\n",
       "      <td>0.837193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>0.127418</td>\n",
       "      <td>0.039222</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.628073</td>\n",
       "      <td>0.660369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>212.432784</td>\n",
       "      <td>0.017244</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.693171</td>\n",
       "      <td>0.826389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Lasso</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>61.974368</td>\n",
       "      <td>0.017205</td>\n",
       "      <td>0.831667</td>\n",
       "      <td>0.622522</td>\n",
       "      <td>0.819164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Bagging</th>\n",
       "      <td>2.512055</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>668.403124</td>\n",
       "      <td>0.653561</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.632585</td>\n",
       "      <td>0.794535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Feature Extraction Train Time  \\\n",
       "Baseline                                              5.376008   \n",
       "Advanced (SMOTEBoost)                                 2.512055   \n",
       "Baseline with Improved Features                       0.193292   \n",
       "Baseline with PCA                                     6.889987   \n",
       "KNN                                                   0.193292   \n",
       "SMOTE KNN                                             2.512055   \n",
       "XGBoost                                               0.193292   \n",
       "Random Forest                                         0.193292   \n",
       "LDA                                                   0.193292   \n",
       "Logistic Regression                                   0.193292   \n",
       "Weighted Logistic                                     0.193292   \n",
       "SVM                                                   0.193292   \n",
       "SVM with PCA                                          6.889987   \n",
       "Weighted SVM                                          0.193292   \n",
       "Naive Bayes                                           0.193292   \n",
       "Lasso                                                 0.193292   \n",
       "Weighted Lasso                                        0.193292   \n",
       "SMOTE Bagging                                         2.512055   \n",
       "\n",
       "                                 Feature Extraction Test Time  Train Time  \\\n",
       "Baseline                                             1.175714  186.140792   \n",
       "Advanced (SMOTEBoost)                                0.038007  206.047342   \n",
       "Baseline with Improved Features                      0.038007  192.445398   \n",
       "Baseline with PCA                                    0.119862    1.294838   \n",
       "KNN                                                  0.038007    0.381865   \n",
       "SMOTE KNN                                            0.038007    0.762404   \n",
       "XGBoost                                              0.038007   89.336492   \n",
       "Random Forest                                        0.038007    7.629287   \n",
       "LDA                                                  0.038007    7.088265   \n",
       "Logistic Regression                                  0.038007  119.274133   \n",
       "Weighted Logistic                                    0.038007  145.245618   \n",
       "SVM                                                  0.038007   85.878458   \n",
       "SVM with PCA                                         0.119862    0.835471   \n",
       "Weighted SVM                                         0.038007  403.409784   \n",
       "Naive Bayes                                          0.038007    0.127418   \n",
       "Lasso                                                0.038007  212.432784   \n",
       "Weighted Lasso                                       0.038007   61.974368   \n",
       "SMOTE Bagging                                        0.038007  668.403124   \n",
       "\n",
       "                                 Prediction Time  Accuracy  Balanced Accuracy  \\\n",
       "Baseline                                0.055967  0.808333           0.587563   \n",
       "Advanced (SMOTEBoost)                   0.077489  0.821667           0.670898   \n",
       "Baseline with Improved Features         0.032289  0.816667           0.610128   \n",
       "Baseline with PCA                       0.002415  0.790000           0.535616   \n",
       "KNN                                     6.070336  0.791667           0.516514   \n",
       "SMOTE KNN                               8.466854  0.606667           0.638211   \n",
       "XGBoost                                 0.087405  0.820000           0.620882   \n",
       "Random Forest                           0.039910  0.810000           0.562701   \n",
       "LDA                                     0.018853  0.821667           0.636339   \n",
       "Logistic Regression                     0.024061  0.821667           0.699697   \n",
       "Weighted Logistic                       0.023004  0.830000           0.679063   \n",
       "SVM                                     1.919353  0.855000           0.683400   \n",
       "SVM with PCA                            0.020878  0.786667           0.585341   \n",
       "Weighted SVM                            1.722822  0.836667           0.717851   \n",
       "Naive Bayes                             0.039222  0.663333           0.628073   \n",
       "Lasso                                   0.017244  0.825000           0.693171   \n",
       "Weighted Lasso                          0.017205  0.831667           0.622522   \n",
       "SMOTE Bagging                           0.653561  0.806667           0.632585   \n",
       "\n",
       "                                      AUC  \n",
       "Baseline                         0.785287  \n",
       "Advanced (SMOTEBoost)            0.821778  \n",
       "Baseline with Improved Features  0.785287  \n",
       "Baseline with PCA                0.692814  \n",
       "KNN                              0.674527  \n",
       "SMOTE KNN                        0.683691  \n",
       "XGBoost                          0.809126  \n",
       "Random Forest                    0.766485  \n",
       "LDA                              0.786203  \n",
       "Logistic Regression              0.822310  \n",
       "Weighted Logistic                0.822843  \n",
       "SVM                              0.828070  \n",
       "SVM with PCA                     0.745118  \n",
       "Weighted SVM                     0.837193  \n",
       "Naive Bayes                      0.660369  \n",
       "Lasso                            0.826389  \n",
       "Weighted Lasso                   0.819164  \n",
       "SMOTE Bagging                    0.794535  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_df = model_results_df.applymap(lambda x: round(x,3))\n",
    "model_results_df['Train Time'] = [str(x)+' s' for x in list(model_results_df['Train Time'])]\n",
    "model_results_df['Prediction Time'] = [str(x)+' s' for x in list(model_results_df['Prediction Time'])]\n",
    "model_results_df['Feature Extraction Train Time'] = [str(x)+' s' for x in list(model_results_df['Feature Extraction Train Time'])]\n",
    "model_results_df['Feature Extraction Test Time'] = [str(x)+' s' for x in list(model_results_df['Feature Extraction Test Time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Extraction Train Time</th>\n",
       "      <th>Feature Extraction Test Time</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>5.376 s</td>\n",
       "      <td>1.176 s</td>\n",
       "      <td>186.141 s</td>\n",
       "      <td>0.056 s</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advanced (SMOTEBoost)</th>\n",
       "      <td>2.512 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>206.047 s</td>\n",
       "      <td>0.077 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with Improved Features</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>192.445 s</td>\n",
       "      <td>0.032 s</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with PCA</th>\n",
       "      <td>6.89 s</td>\n",
       "      <td>0.12 s</td>\n",
       "      <td>1.295 s</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>0.382 s</td>\n",
       "      <td>6.07 s</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE KNN</th>\n",
       "      <td>2.512 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>0.762 s</td>\n",
       "      <td>8.467 s</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>89.336 s</td>\n",
       "      <td>0.087 s</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>7.629 s</td>\n",
       "      <td>0.04 s</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>7.088 s</td>\n",
       "      <td>0.019 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>119.274 s</td>\n",
       "      <td>0.024 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Logistic</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>145.246 s</td>\n",
       "      <td>0.023 s</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>85.878 s</td>\n",
       "      <td>1.919 s</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM with PCA</th>\n",
       "      <td>6.89 s</td>\n",
       "      <td>0.12 s</td>\n",
       "      <td>0.835 s</td>\n",
       "      <td>0.021 s</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted SVM</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>403.41 s</td>\n",
       "      <td>1.723 s</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>0.127 s</td>\n",
       "      <td>0.039 s</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>212.433 s</td>\n",
       "      <td>0.017 s</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Lasso</th>\n",
       "      <td>0.193 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>61.974 s</td>\n",
       "      <td>0.017 s</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Bagging</th>\n",
       "      <td>2.512 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>668.403 s</td>\n",
       "      <td>0.654 s</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Feature Extraction Train Time  \\\n",
       "Baseline                                              5.376 s   \n",
       "Advanced (SMOTEBoost)                                 2.512 s   \n",
       "Baseline with Improved Features                       0.193 s   \n",
       "Baseline with PCA                                      6.89 s   \n",
       "KNN                                                   0.193 s   \n",
       "SMOTE KNN                                             2.512 s   \n",
       "XGBoost                                               0.193 s   \n",
       "Random Forest                                         0.193 s   \n",
       "LDA                                                   0.193 s   \n",
       "Logistic Regression                                   0.193 s   \n",
       "Weighted Logistic                                     0.193 s   \n",
       "SVM                                                   0.193 s   \n",
       "SVM with PCA                                           6.89 s   \n",
       "Weighted SVM                                          0.193 s   \n",
       "Naive Bayes                                           0.193 s   \n",
       "Lasso                                                 0.193 s   \n",
       "Weighted Lasso                                        0.193 s   \n",
       "SMOTE Bagging                                         2.512 s   \n",
       "\n",
       "                                Feature Extraction Test Time Train Time  \\\n",
       "Baseline                                             1.176 s  186.141 s   \n",
       "Advanced (SMOTEBoost)                                0.038 s  206.047 s   \n",
       "Baseline with Improved Features                      0.038 s  192.445 s   \n",
       "Baseline with PCA                                     0.12 s    1.295 s   \n",
       "KNN                                                  0.038 s    0.382 s   \n",
       "SMOTE KNN                                            0.038 s    0.762 s   \n",
       "XGBoost                                              0.038 s   89.336 s   \n",
       "Random Forest                                        0.038 s    7.629 s   \n",
       "LDA                                                  0.038 s    7.088 s   \n",
       "Logistic Regression                                  0.038 s  119.274 s   \n",
       "Weighted Logistic                                    0.038 s  145.246 s   \n",
       "SVM                                                  0.038 s   85.878 s   \n",
       "SVM with PCA                                          0.12 s    0.835 s   \n",
       "Weighted SVM                                         0.038 s   403.41 s   \n",
       "Naive Bayes                                          0.038 s    0.127 s   \n",
       "Lasso                                                0.038 s  212.433 s   \n",
       "Weighted Lasso                                       0.038 s   61.974 s   \n",
       "SMOTE Bagging                                        0.038 s  668.403 s   \n",
       "\n",
       "                                Prediction Time  Accuracy  Balanced Accuracy  \\\n",
       "Baseline                                0.056 s     0.808              0.588   \n",
       "Advanced (SMOTEBoost)                   0.077 s     0.822              0.671   \n",
       "Baseline with Improved Features         0.032 s     0.817              0.610   \n",
       "Baseline with PCA                       0.002 s     0.790              0.536   \n",
       "KNN                                      6.07 s     0.792              0.517   \n",
       "SMOTE KNN                               8.467 s     0.607              0.638   \n",
       "XGBoost                                 0.087 s     0.820              0.621   \n",
       "Random Forest                            0.04 s     0.810              0.563   \n",
       "LDA                                     0.019 s     0.822              0.636   \n",
       "Logistic Regression                     0.024 s     0.822              0.700   \n",
       "Weighted Logistic                       0.023 s     0.830              0.679   \n",
       "SVM                                     1.919 s     0.855              0.683   \n",
       "SVM with PCA                            0.021 s     0.787              0.585   \n",
       "Weighted SVM                            1.723 s     0.837              0.718   \n",
       "Naive Bayes                             0.039 s     0.663              0.628   \n",
       "Lasso                                   0.017 s     0.825              0.693   \n",
       "Weighted Lasso                          0.017 s     0.832              0.623   \n",
       "SMOTE Bagging                           0.654 s     0.807              0.633   \n",
       "\n",
       "                                   AUC  \n",
       "Baseline                         0.785  \n",
       "Advanced (SMOTEBoost)            0.822  \n",
       "Baseline with Improved Features  0.785  \n",
       "Baseline with PCA                0.693  \n",
       "KNN                              0.675  \n",
       "SMOTE KNN                        0.684  \n",
       "XGBoost                          0.809  \n",
       "Random Forest                    0.766  \n",
       "LDA                              0.786  \n",
       "Logistic Regression              0.822  \n",
       "Weighted Logistic                0.823  \n",
       "SVM                              0.828  \n",
       "SVM with PCA                     0.745  \n",
       "Weighted SVM                     0.837  \n",
       "Naive Bayes                      0.660  \n",
       "Lasso                            0.826  \n",
       "Weighted Lasso                   0.819  \n",
       "SMOTE Bagging                    0.795  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
