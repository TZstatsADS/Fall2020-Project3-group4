{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install ipynb' in the command line. This code lets us import functions from notebooks in the lib folder. Lib has all of the feature extraction and model training/predicting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import sys\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install imblearn' in the command line. This code lets us do SMOTE (synthetic minority oversampling technique) and random undersampling to help deal with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the remaining libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .boolean { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .integer { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .string  { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import scipy.io\n",
    "import pickle\n",
    "import bz2\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "from sklearn.metrics import pairwise_distances, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll import the training and test functions from the lib folder in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.train_gbm import train_gbm\n",
    "from ipynb.fs.full.train_xgb import train_xgb\n",
    "from ipynb.fs.full.train_knn import train_knn\n",
    "from ipynb.fs.full.train_lda import train_lda\n",
    "from ipynb.fs.full.train_random_forest import train_random_forest\n",
    "from ipynb.fs.full.train_logistic import train_logistic\n",
    "from ipynb.fs.full.train_svm import train_svm\n",
    "from ipynb.fs.full.train_naive_bayes import train_naive_bayes\n",
    "from ipynb.fs.full.train_lasso import train_lasso\n",
    "from ipynb.fs.full.train_bagging import train_bagging\n",
    "\n",
    "from ipynb.fs.full.test_model import test_model\n",
    "from ipynb.fs.full.compute_metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Work Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the directories for the training set points and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/train_set/'\n",
    "train_image_dir = train_dir+\"images/\"\n",
    "train_pt_dir = train_dir+\"points/\"\n",
    "train_label_path = train_dir+\"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set Up Controls\n",
    "\n",
    "In this cell, we have a set of controls for the feature extraction. If true, then we process the features from scratch, and if false, then we load existing features from files in the output folder. \n",
    "\n",
    "+ (T/F) initial feature extraction on training set\n",
    "+ (T/F) initial feature extraction on test set\n",
    "\n",
    "+ (T/F) improved feature extraction on training set\n",
    "+ (T/F) improved feature extraction on test set\n",
    "\n",
    "+ (T/F) SMOTE using improved features on train set\n",
    "\n",
    "+ (T/F) PCA using improved features on training set and test set (doesn't make sense to only do PCA from scratch on train but not test and vice versa so only the option to do it from scratch on both or neither is given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_feature_train_initial = True\n",
    "run_feature_test_initial = True\n",
    "\n",
    "run_feature_train = True \n",
    "run_feature_test = True \n",
    "\n",
    "run_feature_train_SMOTE = True\n",
    "\n",
    "run_feature_PCA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we have a set of controls for model training/testing. If true, then we train the model and generate predictions on the test set, and if false, then we skip that model. By default all the models are set to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline = True\n",
    "run_advanced = True\n",
    "\n",
    "run_baseline_improved = True\n",
    "run_baseline_pca = True\n",
    "run_knn = True\n",
    "run_knn_smote = True\n",
    "run_xgboost=True\n",
    "feature_initial=True\n",
    "run_random_forest=True\n",
    "run_LDA=True\n",
    "run_logistic=True\n",
    "run_weighted_logistic=True\n",
    "run_svm = True\n",
    "run_svm_pca = True\n",
    "run_weighted_svm = True\n",
    "run_lasso = True\n",
    "run_weighted_lasso = True\n",
    "run_bagging_smote = True\n",
    "run_naivebayes = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overwrite_saved_model_results option lets you decide if you want to save the model statistics from your run to a saved model results file. It is recommended that you set it to False if you plan to not run all of the models.\n",
    "\n",
    "The run_10_cv option runs through the 10-fold cross validation with AUC scoring that we used on weighted logistic, weighted SVM, weighted lasso, and XGBoost with SMOTE to determine which to pick as the advanced model. By default it is set to False as it takes about 3 hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_saved_model_results = True\n",
    "run_10_cv = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Data and Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data, and we can see that the dataset is imbalanced and that there are more records with basic emotions than records with complex emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   2402 \n",
      "Number of records with label 1 (complex emotion): 598 \n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "n = info.shape[0]\n",
    "\n",
    "#Data is imbalanced \n",
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(info.loc[info['label']==0].shape[0]))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(info.loc[info['label']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do an 80-20 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(round(n*(4/5),0))\n",
    "train_idx = np.random.choice(list(info.index),size=n_train,replace=False)\n",
    "test_idx = list(set(list(info.index))-set(train_idx)) #set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial points are stored in matlab format. In this step, we read them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read fiducial points\n",
    "#input: index\n",
    "#output: matrix of fiducial points corresponding to the index\n",
    "\n",
    "n_files = len(os.listdir(train_pt_dir))\n",
    "\n",
    "def readMat_matrix(index):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinatesUnwarped']\n",
    "    except KeyError:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinates2']\n",
    "    return np.matrix.round(mat_data,0)\n",
    "\n",
    "#load fiducial points into list and store them in output\n",
    "fiducial_pt_list = list(map(readMat_matrix,list(range(1,n_files+1))))\n",
    "pickle.dump(fiducial_pt_list, open( \"../output/fiducial_pt_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Construct Features and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starter Code Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature.ipynb's feature_initial function to generate pairwise distance features for the baseline model. This is the same feature extraction method as that of the starter code. Note that this method counts distances from x-axis and from y-axis separately between points.\n",
    "\n",
    "Feature extraction times exclude the time it takes to write to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature extraction time for train: 5.326151\n",
      "Initial feature extraction time for test:  1.202793\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_initial\n",
    "\n",
    "tm_feature_train_intitial = np.nan\n",
    "if run_feature_train_initial == True:\n",
    "    start = time.time()\n",
    "    dat_train_initial = feature_initial(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_initial = end-start\n",
    "    with bz2.BZ2File('../output/train_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_initial, f)\n",
    "    print('Initial feature extraction time for train: {:4f}'.format(tm_feature_train_initial))\n",
    "else:\n",
    "    dat_train_initial = cPickle.load(bz2.BZ2File('../output/train_data_initial.pbz2', 'rb'))\n",
    "        \n",
    "        \n",
    "tm_feature_test_initial = np.nan\n",
    "if run_feature_test_initial == True:\n",
    "    start = time.time()\n",
    "    dat_test_initial = feature_initial(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test_initial = end-start\n",
    "    with bz2.BZ2File('../output/test_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test_initial, f)\n",
    "    print('Initial feature extraction time for test:  {:4f}'.format(tm_feature_test_initial))\n",
    "else:\n",
    "    dat_test_initial = cPickle.load(bz2.BZ2File('../output/test_data_initial.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_initial = dat_train_initial.loc[:, dat_train_initial.columns != 'labels']\n",
    "label_train_initial = dat_train_initial['labels']\n",
    "\n",
    "feature_test_initial = dat_test_initial.loc[:, dat_test_initial.columns != 'labels']\n",
    "label_test_initial = dat_test_initial['labels'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature.ipynb's feature_improved function to generate pairwise euclidean distance features to be used by all of the models other than the baseline. Since feature_improved just uses a single euclidean distance value rather than separate x-distance and y-distance values, feature_improved produces exactly half as many features as feature_initial while keeping the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved feature extraction time for train: 0.181775\n",
      "Improved feature extraction time for test:  0.036049\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_improved\n",
    "\n",
    "tm_feature_train_improved = np.nan\n",
    "if run_feature_train == True:\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_improved = end-start\n",
    "    with bz2.BZ2File('../output/train_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train, f)\n",
    "    print('Improved feature extraction time for train: {:4f}'.format(tm_feature_train_improved))\n",
    "else:\n",
    "    dat_train = cPickle.load(bz2.BZ2File('../output/train_data.pbz2', 'rb'))\n",
    "\n",
    "\n",
    "tm_feature_test_improved = np.nan\n",
    "if run_feature_test == True:\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test_improved = end-start\n",
    "    with bz2.BZ2File('../output/test_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test, f)\n",
    "    print('Improved feature extraction time for test:  {:4f}'.format(tm_feature_test_improved))\n",
    "else:\n",
    "    dat_test = cPickle.load(bz2.BZ2File('../output/test_data.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "label_train = dat_train['labels'] \n",
    "\n",
    "feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "label_test = dat_test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the feature extraction for SMOTE which will be discussed more in the advanced model section. SMOTE is only done on the training data and not on the test data. SMOTE is a modification of the improved features. \n",
    "\n",
    "If the improved features are obtained from scratch, then we include the time it takes to get the improved features with the time it takes to use SMOTE. Otherwise, in the case where the improved features are loaded from the disk, we just use the time it takes to use SMOTE on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE feature extraction time for train: 2.594856\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_SMOTE\n",
    "\n",
    "tm_feature_train_SMOTE = np.nan\n",
    "if run_feature_train_SMOTE == True:\n",
    "    start = time.time()\n",
    "    dat_train_SMOTE = feature_SMOTE(dat_train)\n",
    "    end = time.time()\n",
    "    if pd.isnull(tm_feature_train_improved):\n",
    "        tm_feature_train_SMOTE = end-start\n",
    "    else:\n",
    "        tm_feature_train_SMOTE = (end-start)+tm_feature_train_improved\n",
    "    with bz2.BZ2File('../output/train_data_SMOTE' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_SMOTE, f)\n",
    "    print('SMOTE feature extraction time for train: {:4f}'.format(tm_feature_train_SMOTE))\n",
    "else:\n",
    "    dat_train_SMOTE = cPickle.load(bz2.BZ2File('../output/train_data_SMOTE.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_sm = dat_train_SMOTE.loc[:,dat_train_SMOTE.columns!='labels']\n",
    "label_train_sm = dat_train_SMOTE['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here we do PCA which is only done for a couple of the model candidates that were not chosen for the advanced model. PCA is done as a modification of the improved features. Also, it doesn't make sense to only do the PCA transformation on one of either the training data or test data, so both are inputs here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA feature extraction time for train: 6.715786\n",
      "PCA feature extraction time for test:  0.114706\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_PCA\n",
    "\n",
    "tm_feature_train_PCA = np.nan\n",
    "tm_feature_test_PCA = np.nan\n",
    "\n",
    "if run_feature_PCA == True:\n",
    "    [dat_train_PCA, dat_test_PCA, tm_feature_train_PCA, tm_feature_test_PCA] = feature_PCA(dat_train,dat_test)\n",
    "    \n",
    "    if pd.isnull(tm_feature_train_improved)==False:\n",
    "        tm_feature_train_PCA = tm_feature_train_PCA+tm_feature_train_improved\n",
    "    if pd.isnull(tm_feature_test_improved)==False:\n",
    "        tm_feature_test_PCA = tm_feature_test_PCA+tm_feature_test_improved\n",
    "    \n",
    "    with bz2.BZ2File('../output/train_data_PCA' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_PCA, f)\n",
    "    with bz2.BZ2File('../output/test_data_PCA' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test_PCA, f)\n",
    "        \n",
    "    print('PCA feature extraction time for train: {:4f}'.format(tm_feature_train_PCA))\n",
    "    print('PCA feature extraction time for test:  {:4f}'.format(tm_feature_test_PCA))\n",
    "        \n",
    "else:\n",
    "    dat_train_PCA = cPickle.load(bz2.BZ2File('../output/train_data_PCA.pbz2', 'rb'))\n",
    "    dat_test_PCA = cPickle.load(bz2.BZ2File('../output/test_data_PCA.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_PCA = dat_train_PCA.loc[:,dat_train_PCA.columns!='labels']\n",
    "label_train_PCA = dat_train_PCA['labels'] #labels are same as label_train\n",
    "\n",
    "feature_test_PCA = dat_test_PCA.loc[:,dat_test_PCA.columns!='labels']\n",
    "label_test_PCA = dat_test_PCA['labels'] #labels are same as label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Baseline Model ~58% Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discussing the models, we will go over the two main metrics we used for finding a better model than the baseline. Since the data is imbalanced, we examined AUC and balanced accuracy rather than the regular accuracy metric.\n",
    "\n",
    "AUC is the area under the ROC curve which measures the TP vs FP rate as the classification decision threshold changes over time. For imbalanced data, it is a better performance metric than accuracy.\n",
    "\n",
    "Balanced accuracy is given by the formula $$balanced\\_accuracy = \\frac{1}{2}\\left(\\frac{TP}{TP+FN}+\\frac{TN}{TN+FP}\\right).$$ \n",
    "\n",
    "\n",
    "Balanced accuracy is used for imbalanced data as an estimate for the accuracy if the data was balanced, so the true performance of our models on balanced data will be close to the balanced accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model is a GBM fitted on the initial pairwise fiducial features. The parameters were chosen from a grid search with AUC scoring. Note that feature extraction times for all models are already known from the previous step, so we do not need to re-calculate them.\n",
    "\n",
    "We claim that the accuracy of the baseline model on balanced test data is about 58% based on the fact that we get 58% balanced accuracy as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimal parameters\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train_initial,label_train_initial)\n",
    "#gscv.best_params_\n",
    "# output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame used to store all of the model results\n",
    "model_results_df = pd.DataFrame(columns=['Feature Extraction Train Time','Feature Extraction Test Time',\n",
    "                                         'Train Time','Prediction Time','Accuracy','Balanced Accuracy','AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 5.326151 seconds\n",
      "Feature extraction time for test: 1.202793  seconds\n",
      "\n",
      "Training time: 186.644306 seconds\n",
      "Prediction time: 0.076027 seconds\n",
      "\n",
      "Accuracy: 0.808333\n",
      "Balanced Accuracy: 0.587563\n",
      "AUC: 0.785287\n"
     ]
    }
   ],
   "source": [
    "if run_baseline == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_initial))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_initial))\n",
    "    \n",
    "    [train_time,baseline] = train_gbm(feature_train_initial,label_train_initial,\n",
    "                                      learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline,feature_test_initial)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_initial,label_test_initial,test_preds,baseline)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    #save baseline model\n",
    "    pickle.dump(baseline,open(\"../output/baseline.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_initial,\n",
    "                    'Feature Extraction Test Time':tm_feature_test_initial,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Baseline')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Advanced Model (SMOTEBoost) ~70% Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the advanced model, we decided to use SMOTEBoost, which is a modified version of XGBoost that uses SMOTE (Synethic Minority Oversampling Technique). Our model also uses the improved features which do not double count distances, and the parameters were chosen from grid search with AUC scoring.\n",
    "\n",
    "The idea of SMOTE is to modify the imbalanced training data by randomly undersampling from the majority class and then creating new synthetic minority data that is close to the existing feature space. The modified SMOTE features then have an equal number of data in each class. To see the details of how we implemented SMOTE, check the feature_SMOTE function in feature.ipynb.\n",
    "\n",
    "We went with this model for a couple of reasons. First of all, it addresses the fact that the training data is imbalanced. It also has a higher AUC, accuracy, and balanced accuracy than the baseline GBM model. Finally, compared to the other candidates for the advanced model, it has the highest AUC from 10 fold cross validation.\n",
    "\n",
    "Hence, our claimed accuracy with the advanced model on balanced test data is about 70% based on the fact that we get 70% balanced accuracy as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 after SMOTE (basic emotion):   1929 \n",
      "Number of records with label 1 after SMOTE (complex emotion): 1929 \n"
     ]
    }
   ],
   "source": [
    "print('Number of records with label 0 after SMOTE (basic emotion):   {:4d} '.format(len(label_train_sm)-sum(label_train_sm)))\n",
    "print('Number of records with label 1 after SMOTE (complex emotion): {:2d} '.format(sum(label_train_sm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 2.594856 seconds\n",
      "Feature extraction time for test:  0.036049  seconds\n",
      "\n",
      "Training time: 206.306090 seconds\n",
      "Prediction time: 0.068411 seconds\n",
      "\n",
      "Accuracy: 0.810000\n",
      "Balanced Accuracy: 0.706697\n",
      "AUC: 0.827371\n"
     ]
    }
   ],
   "source": [
    "if run_advanced == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, advanced] = train_xgb(feature_train_sm, label_train_sm, learning_rate=0.25, n_estimators=300,\n",
    "                                      max_depth=3,min_child_weight=1,objective='binary:logistic',scale_pos_weight=4)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "\n",
    "         \n",
    "    [prediction_time,test_preds] = test_model(advanced,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,advanced)\n",
    "\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    pickle.dump(advanced,open(\"../output/advanced.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "                    'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Advanced (SMOTEBoost)')\n",
    "    model_results_df.loc['Advanced (SMOTEBoost)'] = row    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Step: Remaining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pre-made data frame of all of the model run times and metrics in case you want a comparison without running through all of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Extraction Train Time</th>\n",
       "      <th>Feature Extraction Test Time</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>5.043 s</td>\n",
       "      <td>1.285 s</td>\n",
       "      <td>184.954 s</td>\n",
       "      <td>0.046 s</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advanced (SMOTEBoost)</th>\n",
       "      <td>2.505 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>197.78 s</td>\n",
       "      <td>0.07 s</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with Improved Features</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>176.08 s</td>\n",
       "      <td>0.024 s</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with PCA</th>\n",
       "      <td>6.614 s</td>\n",
       "      <td>0.115 s</td>\n",
       "      <td>1.179 s</td>\n",
       "      <td>0.003 s</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>0.371 s</td>\n",
       "      <td>5.421 s</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE KNN</th>\n",
       "      <td>2.505 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>0.723 s</td>\n",
       "      <td>7.67 s</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>85.464 s</td>\n",
       "      <td>0.075 s</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>7.452 s</td>\n",
       "      <td>0.032 s</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>6.5 s</td>\n",
       "      <td>0.024 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>109.518 s</td>\n",
       "      <td>0.023 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Logistic</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>135.264 s</td>\n",
       "      <td>0.025 s</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>86.185 s</td>\n",
       "      <td>1.746 s</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM with PCA</th>\n",
       "      <td>6.614 s</td>\n",
       "      <td>0.115 s</td>\n",
       "      <td>0.812 s</td>\n",
       "      <td>0.018 s</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted SVM</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>396.483 s</td>\n",
       "      <td>1.844 s</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>0.141 s</td>\n",
       "      <td>0.039 s</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>208.918 s</td>\n",
       "      <td>0.019 s</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Lasso</th>\n",
       "      <td>0.186 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>61.742 s</td>\n",
       "      <td>0.018 s</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Bagging</th>\n",
       "      <td>2.505 s</td>\n",
       "      <td>0.037 s</td>\n",
       "      <td>656.995 s</td>\n",
       "      <td>0.677 s</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Feature Extraction Train Time  \\\n",
       "Baseline                                              5.043 s   \n",
       "Advanced (SMOTEBoost)                                 2.505 s   \n",
       "Baseline with Improved Features                       0.186 s   \n",
       "Baseline with PCA                                     6.614 s   \n",
       "KNN                                                   0.186 s   \n",
       "SMOTE KNN                                             2.505 s   \n",
       "XGBoost                                               0.186 s   \n",
       "Random Forest                                         0.186 s   \n",
       "LDA                                                   0.186 s   \n",
       "Logistic Regression                                   0.186 s   \n",
       "Weighted Logistic                                     0.186 s   \n",
       "SVM                                                   0.186 s   \n",
       "SVM with PCA                                          6.614 s   \n",
       "Weighted SVM                                          0.186 s   \n",
       "Naive Bayes                                           0.186 s   \n",
       "Lasso                                                 0.186 s   \n",
       "Weighted Lasso                                        0.186 s   \n",
       "SMOTE Bagging                                         2.505 s   \n",
       "\n",
       "                                Feature Extraction Test Time Train Time  \\\n",
       "Baseline                                             1.285 s  184.954 s   \n",
       "Advanced (SMOTEBoost)                                0.037 s   197.78 s   \n",
       "Baseline with Improved Features                      0.037 s   176.08 s   \n",
       "Baseline with PCA                                    0.115 s    1.179 s   \n",
       "KNN                                                  0.037 s    0.371 s   \n",
       "SMOTE KNN                                            0.037 s    0.723 s   \n",
       "XGBoost                                              0.037 s   85.464 s   \n",
       "Random Forest                                        0.037 s    7.452 s   \n",
       "LDA                                                  0.037 s      6.5 s   \n",
       "Logistic Regression                                  0.037 s  109.518 s   \n",
       "Weighted Logistic                                    0.037 s  135.264 s   \n",
       "SVM                                                  0.037 s   86.185 s   \n",
       "SVM with PCA                                         0.115 s    0.812 s   \n",
       "Weighted SVM                                         0.037 s  396.483 s   \n",
       "Naive Bayes                                          0.037 s    0.141 s   \n",
       "Lasso                                                0.037 s  208.918 s   \n",
       "Weighted Lasso                                       0.037 s   61.742 s   \n",
       "SMOTE Bagging                                        0.037 s  656.995 s   \n",
       "\n",
       "                                Prediction Time  Accuracy  Balanced Accuracy  \\\n",
       "Baseline                                0.046 s     0.808              0.588   \n",
       "Advanced (SMOTEBoost)                    0.07 s     0.810              0.707   \n",
       "Baseline with Improved Features         0.024 s     0.817              0.610   \n",
       "Baseline with PCA                       0.003 s     0.790              0.536   \n",
       "KNN                                     5.421 s     0.792              0.517   \n",
       "SMOTE KNN                                7.67 s     0.607              0.638   \n",
       "XGBoost                                 0.075 s     0.813              0.689   \n",
       "Random Forest                           0.032 s     0.810              0.563   \n",
       "LDA                                     0.024 s     0.822              0.636   \n",
       "Logistic Regression                     0.023 s     0.822              0.700   \n",
       "Weighted Logistic                       0.025 s     0.830              0.679   \n",
       "SVM                                     1.746 s     0.855              0.683   \n",
       "SVM with PCA                            0.018 s     0.787              0.585   \n",
       "Weighted SVM                            1.844 s     0.837              0.718   \n",
       "Naive Bayes                             0.039 s     0.663              0.628   \n",
       "Lasso                                   0.019 s     0.825              0.693   \n",
       "Weighted Lasso                          0.018 s     0.832              0.623   \n",
       "SMOTE Bagging                           0.677 s     0.807              0.633   \n",
       "\n",
       "                                   AUC  \n",
       "Baseline                         0.785  \n",
       "Advanced (SMOTEBoost)            0.827  \n",
       "Baseline with Improved Features  0.798  \n",
       "Baseline with PCA                0.693  \n",
       "KNN                              0.675  \n",
       "SMOTE KNN                        0.684  \n",
       "XGBoost                          0.809  \n",
       "Random Forest                    0.766  \n",
       "LDA                              0.786  \n",
       "Logistic Regression              0.822  \n",
       "Weighted Logistic                0.823  \n",
       "SVM                              0.828  \n",
       "SVM with PCA                     0.745  \n",
       "Weighted SVM                     0.837  \n",
       "Naive Bayes                      0.660  \n",
       "Lasso                            0.826  \n",
       "Weighted Lasso                   0.819  \n",
       "SMOTE Bagging                    0.795  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('../output/model_results.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In thie rest of this step, we run through the other models that were candidates for the advanced model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model with Improved Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimal parameters\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "# output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test:  0.036049  seconds\n",
      "\n",
      "Training time: 183.706270 seconds\n",
      "Prediction time: 0.023944 seconds\n",
      "\n",
      "Accuracy: 0.816667\n",
      "Balanced Accuracy: 0.610128\n",
      "AUC: 0.797856\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_improved == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, baseline_improved] = train_gbm(feature_train,label_train,\n",
    "                                                learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline_improved,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,baseline_improved)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "\n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "                'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "                 'Train Time':train_time,\n",
    "                 'Prediction Time':prediction_time,\n",
    "                'Accuracy':accuracy,\n",
    "                'AUC':auc,\n",
    "                'Balanced Accuracy':balanced_accuracy},name='Baseline with Improved Features')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3).fit(feature_train_PCA,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 6.715786 seconds\n",
      "Feature extraction time for test:  0.114706  seconds\n",
      "\n",
      "Training time: 1.191914 seconds\n",
      "Prediction time: 0.002096 seconds\n",
      "\n",
      "Accuracy: 0.790000\n",
      "Balanced Accuracy: 0.535616\n",
      "AUC: 0.692814\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_pca == True:\n",
    "\n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_PCA))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_PCA))\n",
    "    \n",
    "    [train_time, baseline_PCA] = train_gbm(feature_train_PCA,label_train,\n",
    "                                                learning_rate=0.1,max_depth=2,n_estimators=150)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline_PCA,feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_PCA,label_test,test_preds,baseline_PCA)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_PCA,\n",
    "            'Feature Extraction Test Time':tm_feature_test_PCA,\n",
    "             'Train Time':train_time,\n",
    "             'Prediction Time':prediction_time,\n",
    "            'Accuracy':accuracy,\n",
    "            'AUC':auc,\n",
    "            'Balanced Accuracy':balanced_accuracy},name='Baseline with PCA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':list(range(5,55,5))}\n",
    "#gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'n_neighbors': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test:  0.036049  seconds\n",
      "\n",
      "Training time: 0.385187 seconds\n",
      "Prediction time: 5.693887 seconds\n",
      "\n",
      "Accuracy: 0.791667\n",
      "Balanced Accuracy: 0.516514\n",
      "AUC: 0.674527\n"
     ]
    }
   ],
   "source": [
    "if run_knn == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, knn] = train_knn(feature_train,label_train,n_neighbors=25)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(knn,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,knn)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='KNN')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':list(range(5,55,5))}\n",
    "#gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {'n_neighbors': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 2.594856 seconds\n",
      "Feature extraction time for test:  0.036049  seconds\n",
      "\n",
      "Training time: 0.752246 seconds\n",
      "Prediction time: 7.720591 seconds\n",
      "\n",
      "Accuracy: 0.606667\n",
      "Balanced Accuracy: 0.638211\n",
      "AUC: 0.683691\n"
     ]
    }
   ],
   "source": [
    "if run_knn_smote == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, knn] = train_knn(feature_train_sm,label_train_sm,n_neighbors=5)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(knn,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,knn)\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SMOTE KNN')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test:  0.036049 seconds\n",
      "\n",
      "Training time: 85.553323 seconds\n",
      "Prediction time: 0.068330 seconds\n",
      "\n",
      "Accuracy: 0.813333\n",
      "Balanced Accuracy: 0.688652\n",
      "AUC: 0.808726\n"
     ]
    }
   ],
   "source": [
    "if run_xgboost == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test:  {:4f} seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, xgb] = train_xgb(feature_train, label_train, learning_rate=0.1, n_estimators=200,\n",
    "                                      max_depth=3,min_child_weight=1,objective='binary:logistic',scale_pos_weight=4)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "\n",
    "         \n",
    "    [prediction_time,test_preds] = test_model(xgb,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "\n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,xgb)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='XGBoost')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 6.693948 seconds\n",
      "Prediction time: 0.035192 seconds\n",
      "\n",
      "Accuracy: 0.810000\n",
      "Balanced Accuracy: 0.562701\n",
      "AUC: 0.766485\n"
     ]
    }
   ],
   "source": [
    "if run_random_forest==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, rf_model] = train_random_forest(feature_train,label_train,n_estimators=100,criterion='gini',min_samples_leaf=1,max_features='sqrt')\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(rf_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,rf_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Random Forest')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 5.942372 seconds\n",
      "Prediction time: 0.019037 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.636339\n",
      "AUC: 0.786203\n"
     ]
    }
   ],
   "source": [
    "if run_LDA==True: \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, lda_model] = train_lda(feature_train, label_train,solver='eigen', shrinkage=.1, n_components=1)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lda_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lda_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='LDA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 97.020723 seconds\n",
      "Prediction time: 0.022679 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.699697\n",
      "AUC: 0.822310\n"
     ]
    }
   ],
   "source": [
    "if run_logistic==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time,lr_model] = train_logistic(feature_train,label_train,\n",
    "                                           C=0.01, dual=False, fit_intercept=True,\n",
    "                                           intercept_scaling=1, max_iter=1200000,\n",
    "                                           multi_class='multinomial', penalty='l2',\n",
    "                                           solver='lbfgs', tol=0.0001,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lr_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lr_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Logistic Regression')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#weights = {0:80.0, 1:20.0}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001,class_weight=weights),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 122.259835 seconds\n",
      "Prediction time: 0.022598 seconds\n",
      "\n",
      "Accuracy: 0.830000\n",
      "Balanced Accuracy: 0.679063\n",
      "AUC: 0.822843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "if run_weighted_logistic==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time,lr_w] = train_logistic(feature_train,label_train,\n",
    "                                           C=0.001, dual=False, fit_intercept=True,\n",
    "                                           intercept_scaling=1, max_iter=120000000000,\n",
    "                                           multi_class='multinomial', penalty='l2',\n",
    "                                           solver='lbfgs', tol=0.0001,class_weight=weights)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lr_w,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lr_w)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted Logistic')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.00001,0.0001,0.001,0.01,1,10],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 79.830844 seconds\n",
      "Prediction time: 1.624750 seconds\n",
      "\n",
      "Accuracy: 0.855000\n",
      "Balanced Accuracy: 0.683400\n",
      "AUC: 0.828070\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_svm import train_svm\n",
    "\n",
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time,svm_model] = train_svm(feature_train,label_train,\n",
    "                                      C=10, kernel='poly', degree=4,\n",
    "                                      probability=True,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,svm_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SVM')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.001,0.01,1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train_PCA,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 2, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 6.715786 seconds\n",
      "Feature extraction time for test:  0.114706  seconds\n",
      "\n",
      "Training time: 0.746498 seconds\n",
      "Prediction time: 0.018930 seconds\n",
      "\n",
      "Accuracy: 0.786667\n",
      "Balanced Accuracy: 0.585341\n",
      "AUC: 0.745118\n"
     ]
    }
   ],
   "source": [
    "#improved svm with PCA\n",
    "\n",
    "if run_svm_pca==True:    \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_PCA))\n",
    "    print('Feature extraction time for test:  {:4f}  seconds'.format(tm_feature_test_PCA))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time, svm_PCA] = train_svm(feature_train_PCA,label_train_PCA,\n",
    "                                      C=10,degree=2,kernel='rbf',probability=True,\n",
    "                                     class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_PCA,feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_PCA,label_test_PCA,test_preds,svm_PCA)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_PCA,\n",
    "        'Feature Extraction Test Time':tm_feature_test_PCA,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SVM with PCA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# params= {'C': [1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(SVC(class_weight=weights,random_state = 2020,probability=True), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 384.685321 seconds\n",
      "Prediction time: 1.667472 seconds\n",
      "\n",
      "Accuracy: 0.836667\n",
      "Balanced Accuracy: 0.717851\n",
      "AUC: 0.837193\n"
     ]
    }
   ],
   "source": [
    "if run_weighted_svm==True:  \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time,svm_w] = train_svm(feature_train,label_train,\n",
    "                                      C=10, kernel='poly', degree=4,\n",
    "                                      probability=True,class_weight=weights)\n",
    "    \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(svm_w,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,svm_w)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted SVM')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 0.138798 seconds\n",
      "Prediction time: 0.037708 seconds\n",
      "\n",
      "Accuracy: 0.663333\n",
      "Balanced Accuracy: 0.628073\n",
      "AUC: 0.660369\n"
     ]
    }
   ],
   "source": [
    "if run_naivebayes == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time,gnb] = train_naive_bayes(feature_train,label_train)\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(gnb,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,gnb)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Naive Bayes')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 197.398233 seconds\n",
      "Prediction time: 0.017074 seconds\n",
      "\n",
      "Accuracy: 0.825000\n",
      "Balanced Accuracy: 0.693171\n",
      "AUC: 0.826389\n"
     ]
    }
   ],
   "source": [
    "if run_lasso==True:  \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:1.0, 1:1.0}\n",
    "    [train_time, lasso_model] = train_lasso(feature_train,label_train,\n",
    "                                           penalty='l1',solver='liblinear',\n",
    "                                           class_weight=weights)\n",
    "\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lasso_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lasso_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "\n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Lasso')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# param= {'solver':['liblinear', 'saga']}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(LogisticRegression(penalty='l1', class_weight=weights), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 0.181775 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 56.160090 seconds\n",
      "Prediction time: 0.017086 seconds\n",
      "\n",
      "Accuracy: 0.831667\n",
      "Balanced Accuracy: 0.622522\n",
      "AUC: 0.819164\n"
     ]
    }
   ],
   "source": [
    "if run_weighted_lasso==True: \n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_improved))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    [train_time, lasso_w] = train_lasso(feature_train,label_train,\n",
    "                                           penalty='l1',solver='liblinear',\n",
    "                                           class_weight=weights)\n",
    "\n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lasso_w,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lasso_w)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted Lasso')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_estimators':[25,50,75,100]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(BaggingClassifier(),params,cv=cv,scoring='roc_auc').fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {{'n_estimators': 100}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time for train: 2.594856 seconds\n",
      "Feature extraction time for test: 0.036049  seconds\n",
      "\n",
      "Training time: 639.069607 seconds\n",
      "Prediction time: 0.614470 seconds\n",
      "\n",
      "Accuracy: 0.806667\n",
      "Balanced Accuracy: 0.632585\n",
      "AUC: 0.794535\n"
     ]
    }
   ],
   "source": [
    "if run_bagging_smote == True:\n",
    "    \n",
    "    print('Feature extraction time for train: {:4f} seconds'.format(tm_feature_train_SMOTE))\n",
    "    print('Feature extraction time for test: {:4f}  seconds'.format(tm_feature_test_improved))\n",
    "    \n",
    "    [train_time, smote_bagging] = train_bagging(feature_train_sm,label_train_sm,n_estimators=100) \n",
    "    print('\\nTraining time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(smote_bagging,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "\n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,smote_bagging)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SMOTE Bagging')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Results Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we display the model results table for all of the models that were set to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_df = model_results_df.applymap(lambda x: round(x,3))\n",
    "model_results_df['Train Time'] = [str(x)+' s' for x in list(model_results_df['Train Time'])]\n",
    "model_results_df['Prediction Time'] = [str(x)+' s' for x in list(model_results_df['Prediction Time'])]\n",
    "model_results_df['Feature Extraction Train Time'] = [str(x)+' s' for x in list(model_results_df['Feature Extraction Train Time'])]\n",
    "model_results_df['Feature Extraction Test Time'] = [str(x)+' s' for x in list(model_results_df['Feature Extraction Test Time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Extraction Train Time</th>\n",
       "      <th>Feature Extraction Test Time</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>5.326 s</td>\n",
       "      <td>1.203 s</td>\n",
       "      <td>186.644 s</td>\n",
       "      <td>0.076 s</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advanced (SMOTEBoost)</th>\n",
       "      <td>2.595 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>206.306 s</td>\n",
       "      <td>0.068 s</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with Improved Features</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>183.706 s</td>\n",
       "      <td>0.024 s</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with PCA</th>\n",
       "      <td>6.716 s</td>\n",
       "      <td>0.115 s</td>\n",
       "      <td>1.192 s</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>0.385 s</td>\n",
       "      <td>5.694 s</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE KNN</th>\n",
       "      <td>2.595 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>0.752 s</td>\n",
       "      <td>7.721 s</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>85.553 s</td>\n",
       "      <td>0.068 s</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>6.694 s</td>\n",
       "      <td>0.035 s</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>5.942 s</td>\n",
       "      <td>0.019 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>97.021 s</td>\n",
       "      <td>0.023 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Logistic</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>122.26 s</td>\n",
       "      <td>0.023 s</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>79.831 s</td>\n",
       "      <td>1.625 s</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM with PCA</th>\n",
       "      <td>6.716 s</td>\n",
       "      <td>0.115 s</td>\n",
       "      <td>0.746 s</td>\n",
       "      <td>0.019 s</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted SVM</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>384.685 s</td>\n",
       "      <td>1.667 s</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>0.139 s</td>\n",
       "      <td>0.038 s</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>197.398 s</td>\n",
       "      <td>0.017 s</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Lasso</th>\n",
       "      <td>0.182 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>56.16 s</td>\n",
       "      <td>0.017 s</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Bagging</th>\n",
       "      <td>2.595 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>639.07 s</td>\n",
       "      <td>0.614 s</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Feature Extraction Train Time  \\\n",
       "Baseline                                              5.326 s   \n",
       "Advanced (SMOTEBoost)                                 2.595 s   \n",
       "Baseline with Improved Features                       0.182 s   \n",
       "Baseline with PCA                                     6.716 s   \n",
       "KNN                                                   0.182 s   \n",
       "SMOTE KNN                                             2.595 s   \n",
       "XGBoost                                               0.182 s   \n",
       "Random Forest                                         0.182 s   \n",
       "LDA                                                   0.182 s   \n",
       "Logistic Regression                                   0.182 s   \n",
       "Weighted Logistic                                     0.182 s   \n",
       "SVM                                                   0.182 s   \n",
       "SVM with PCA                                          6.716 s   \n",
       "Weighted SVM                                          0.182 s   \n",
       "Naive Bayes                                           0.182 s   \n",
       "Lasso                                                 0.182 s   \n",
       "Weighted Lasso                                        0.182 s   \n",
       "SMOTE Bagging                                         2.595 s   \n",
       "\n",
       "                                Feature Extraction Test Time Train Time  \\\n",
       "Baseline                                             1.203 s  186.644 s   \n",
       "Advanced (SMOTEBoost)                                0.036 s  206.306 s   \n",
       "Baseline with Improved Features                      0.036 s  183.706 s   \n",
       "Baseline with PCA                                    0.115 s    1.192 s   \n",
       "KNN                                                  0.036 s    0.385 s   \n",
       "SMOTE KNN                                            0.036 s    0.752 s   \n",
       "XGBoost                                              0.036 s   85.553 s   \n",
       "Random Forest                                        0.036 s    6.694 s   \n",
       "LDA                                                  0.036 s    5.942 s   \n",
       "Logistic Regression                                  0.036 s   97.021 s   \n",
       "Weighted Logistic                                    0.036 s   122.26 s   \n",
       "SVM                                                  0.036 s   79.831 s   \n",
       "SVM with PCA                                         0.115 s    0.746 s   \n",
       "Weighted SVM                                         0.036 s  384.685 s   \n",
       "Naive Bayes                                          0.036 s    0.139 s   \n",
       "Lasso                                                0.036 s  197.398 s   \n",
       "Weighted Lasso                                       0.036 s    56.16 s   \n",
       "SMOTE Bagging                                        0.036 s   639.07 s   \n",
       "\n",
       "                                Prediction Time  Accuracy  Balanced Accuracy  \\\n",
       "Baseline                                0.076 s     0.808              0.588   \n",
       "Advanced (SMOTEBoost)                   0.068 s     0.810              0.707   \n",
       "Baseline with Improved Features         0.024 s     0.817              0.610   \n",
       "Baseline with PCA                       0.002 s     0.790              0.536   \n",
       "KNN                                     5.694 s     0.792              0.517   \n",
       "SMOTE KNN                               7.721 s     0.607              0.638   \n",
       "XGBoost                                 0.068 s     0.813              0.689   \n",
       "Random Forest                           0.035 s     0.810              0.563   \n",
       "LDA                                     0.019 s     0.822              0.636   \n",
       "Logistic Regression                     0.023 s     0.822              0.700   \n",
       "Weighted Logistic                       0.023 s     0.830              0.679   \n",
       "SVM                                     1.625 s     0.855              0.683   \n",
       "SVM with PCA                            0.019 s     0.787              0.585   \n",
       "Weighted SVM                            1.667 s     0.837              0.718   \n",
       "Naive Bayes                             0.038 s     0.663              0.628   \n",
       "Lasso                                   0.017 s     0.825              0.693   \n",
       "Weighted Lasso                          0.017 s     0.832              0.623   \n",
       "SMOTE Bagging                           0.614 s     0.807              0.633   \n",
       "\n",
       "                                   AUC  \n",
       "Baseline                         0.785  \n",
       "Advanced (SMOTEBoost)            0.827  \n",
       "Baseline with Improved Features  0.798  \n",
       "Baseline with PCA                0.693  \n",
       "KNN                              0.675  \n",
       "SMOTE KNN                        0.684  \n",
       "XGBoost                          0.809  \n",
       "Random Forest                    0.766  \n",
       "LDA                              0.786  \n",
       "Logistic Regression              0.822  \n",
       "Weighted Logistic                0.823  \n",
       "SVM                              0.828  \n",
       "SVM with PCA                     0.745  \n",
       "Weighted SVM                     0.837  \n",
       "Naive Bayes                      0.660  \n",
       "Lasso                            0.826  \n",
       "Weighted Lasso                   0.819  \n",
       "SMOTE Bagging                    0.795  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store model_results\n",
    "if overwrite_saved_model_results == True:\n",
    "    pickle.dump(model_results_df, open( \"../output/model_results.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Step: 10-fold Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we did the 10-fold cross validation with AUC scoring to choose from the best candidate models. By default the option to run this cell is set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_10_cv == True:\n",
    "    # weighted lasso\n",
    "    print(\"Cross Validation Score: \", np.mean(cross_val_score(lasso_w, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "    # output:0.8010359440647241\n",
    "    \n",
    "    # weighted svm\n",
    "    print(\"Cross Validation Score: \", np.mean(cross_val_score(svm_w, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "    # output:0.8117648633580459\n",
    "    \n",
    "    # weighted Logistic\n",
    "    print(\"Cross Validation Score: \", np.mean(cross_val_score(lr_w, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "    # output:0.8300000000000001\n",
    "    \n",
    "    # XGBoost with SMOTE\n",
    "    print(\"Cross Validation Score: \", np.mean(cross_val_score(advanced, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "    # output:0.8387998261113715 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "\n",
    "2. https://arxiv.org/pdf/1106.1813.pdf\n",
    "\n",
    "3. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3648438/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
