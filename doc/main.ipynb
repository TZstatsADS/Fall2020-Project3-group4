{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install ipynb' in the command line. This code lets us import functions from notebooks in the lib folder. Lib has all of the feature extraction and model training/predicting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import sys\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install imblearn' in the command line. This code lets us do SMOTE (synthetic minority oversampling technique) and random undersampling to help deal with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the remaining libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .boolean { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .integer { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .string  { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import scipy.io\n",
    "import pickle\n",
    "import bz2\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "from sklearn.metrics import pairwise_distances, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Work Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the directories for the training set points and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/train_set/'\n",
    "train_image_dir = train_dir+\"images/\"\n",
    "train_pt_dir = train_dir+\"points/\"\n",
    "train_label_path = train_dir+\"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set Up Controls\n",
    "\n",
    "In this cell, we have a set of controls for the feature extraction. If true, then we process the features from scratch, and if false, then we load existing features from files in the output folder. \n",
    "\n",
    "+ (T/F) initial feature extraction on training set\n",
    "+ (T/F) initial feature extraction on test set\n",
    "\n",
    "+ (T/F) improved feature extraction on training set\n",
    "+ (T/F) improved feature extraction on test set\n",
    "\n",
    "+ (T/F) SMOTE using improved features on train set\n",
    "\n",
    "+ (T/F) PCA using improved features on training set\n",
    "+ (T/F) PCA using improved features on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_feature_train_initial = True\n",
    "run_feature_test_initial = True\n",
    "\n",
    "run_feature_train = True \n",
    "run_feature_test = True \n",
    "\n",
    "run_feature_train_SMOTE = True\n",
    "\n",
    "run_feature_train_PCA = True\n",
    "run_feature_test_PCA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we have a set of controls for model training/testing. If true, then we train the model and generate predictions on the test set, and if false, then we skip that model. By default only the baseline and advanced models are set to run, but you can set the other models to be True to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline = True\n",
    "run_advanced = True\n",
    "\n",
    "run_baseline_improved = True\n",
    "run_baseline_pca = True\n",
    "run_knn = True\n",
    "run_knn_smote = True\n",
    "run_xgboost=True\n",
    "feature_initial=True\n",
    "run_random_forest=True\n",
    "run_LDA=True\n",
    "run_logistic=True\n",
    "run_svm = True\n",
    "run_lasso = True\n",
    "run_bagging_smote = True\n",
    "run_naivebayes = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Data and Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data, and we can see that the dataset is imbalanced and that there are more records with basic emotions than records with complex emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   2402 \n",
      "Number of records with label 1 (complex emotion): 598 \n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "n = info.shape[0]\n",
    "\n",
    "#Data is imbalanced \n",
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(info.loc[info['label']==0].shape[0]))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(info.loc[info['label']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do an 80-20 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(round(n*(4/5),0))\n",
    "train_idx = np.random.choice(list(info.index),size=n_train,replace=False)\n",
    "test_idx = list(set(list(info.index))-set(train_idx)) #set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial points are stored in matlab format. In this step, we read them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read fiducial points\n",
    "#input: index\n",
    "#output: matrix of fiducial points corresponding to the index\n",
    "\n",
    "n_files = len(os.listdir(train_pt_dir))\n",
    "\n",
    "def readMat_matrix(index):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinatesUnwarped']\n",
    "    except KeyError:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinates2']\n",
    "    return np.matrix.round(mat_data,0)\n",
    "\n",
    "#load fiducial points into list and store them in output\n",
    "fiducial_pt_list = list(map(readMat_matrix,list(range(1,n_files+1))))\n",
    "pickle.dump(fiducial_pt_list, open( \"../output/fiducial_pt_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Construct Features and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starter Code Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature.ipynb's feature_initial function to generate pairwise distance features for the baseline model. This is the same feature extraction method as that of the starter code. Note that this method double counts distances between points. That is, there are separate entries for the distance from point A to point B and the distance from point B to point A even though the distances are the same.\n",
    "\n",
    "Feature extraction times exclude the time it takes to write to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature extraction time for train: 5.113635\n",
      "Initial feature extraction time for test:  1.275087\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_initial\n",
    "\n",
    "tm_feature_train_intitial = np.nan\n",
    "if run_feature_train_initial == True:\n",
    "    start = time.time()\n",
    "    dat_train_initial = feature_initial(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_initial = end-start\n",
    "    with bz2.BZ2File('../output/train_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train_initial, f)\n",
    "    print('Initial feature extraction time for train: {:4f}'.format(tm_feature_train_initial))\n",
    "else:\n",
    "    dat_train_initial = cPickle.load(bz2.BZ2File('../output/train_data_initial.pbz2', 'rb'))\n",
    "        \n",
    "        \n",
    "tm_feature_test_initial = np.nan\n",
    "if run_feature_test_initial == True:\n",
    "    start = time.time()\n",
    "    dat_test_initial = feature_initial(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test_initial = end-start\n",
    "    with bz2.BZ2File('../output/test_data_initial' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test_initial, f)\n",
    "    print('Initial feature extraction time for test:  {:4f}'.format(tm_feature_test_initial))\n",
    "else:\n",
    "    dat_test_initial = cPickle.load(bz2.BZ2File('../output/test_data_initial.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_initial = dat_train_initial.loc[:, dat_train_initial.columns != 'labels']\n",
    "label_train_initial = dat_train_initial['labels']\n",
    "\n",
    "feature_test_initial = dat_test_initial.loc[:, dat_test_initial.columns != 'labels']\n",
    "label_test_initial = dat_test_initial['labels'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature's feature_improved function to generate pairwise distance features to be used by all of the models other than the baseline. Unlike feature_initial, feature_improved does not double count distances. Hence, feature_improved produces exactly half as many features as feature_initial while keeping the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved feature extraction time for train: 0.183609\n",
      "Improved feature extraction time for test:  0.039230\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_improved\n",
    "\n",
    "tm_feature_train_improved = np.nan\n",
    "if run_feature_train == True:\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_improved = end-start\n",
    "    with bz2.BZ2File('../output/train_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train, f)\n",
    "    print('Improved feature extraction time for train: {:4f}'.format(tm_feature_train_improved))\n",
    "else:\n",
    "    dat_train = cPickle.load(bz2.BZ2File('../output/train_data.pbz2', 'rb'))\n",
    "\n",
    "\n",
    "tm_feature_test_improved = np.nan\n",
    "if run_feature_test == True:\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test_improved = end-start\n",
    "    with bz2.BZ2File('../output/test_data' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_test, f)\n",
    "    print('Improved feature extraction time for test:  {:4f}'.format(tm_feature_test_improved))\n",
    "else:\n",
    "    dat_test = cPickle.load(bz2.BZ2File('../output/test_data.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "label_train = dat_train['labels'] \n",
    "\n",
    "feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "label_test = dat_test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the feature extraction for SMOTE which will be discussed more in the advanced model section. SMOTE is only done on the training data and not on the test data. SMOTE is a modification of the improved features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE feature extraction time for train: 2.474462\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature import feature_SMOTE\n",
    "\n",
    "tm_feature_train_SMOTE = np.nan\n",
    "if run_feature_train_SMOTE == True:\n",
    "    start = time.time()\n",
    "    dat_train_SMOTE = feature_SMOTE(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train_SMOTE = end-start\n",
    "    with bz2.BZ2File('../output/train_data_SMOTE' + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(dat_train, f)\n",
    "    print('SMOTE feature extraction time for train: {:4f}'.format(tm_feature_train_SMOTE))\n",
    "else:\n",
    "    dat_train_SMOTE = cPickle.load(bz2.BZ2File('../output/train_data_SMOTE.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_sm = dat_train_SMOTE.loc[:,dat_train_SMOTE.columns!='labels']\n",
    "label_train_sm = dat_train_SMOTE['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here we do PCA which is only done for the model candidates that were not chosen for the advanced model. PCA is done as a modification of the improved features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.feature import feature_PCA\n",
    "\n",
    "tm_feature_train_PCA = np.nan\n",
    "if run_feature_train_PCA == True:\n",
    "    start = time.time()\n",
    "    feature_train_scaled = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_train_scaled = scaler.fit_transform(feature_train)\n",
    "\n",
    "#pick the number of components that captures 95% of the variance\n",
    "pca = PCA(n_components = 0.95, svd_solver='full').fit(feature_train_scaled)\n",
    "feature_train_PCA = pca.transform(feature_train_scaled)\n",
    "\n",
    "tm_feature_test_PCA = np.nan\n",
    "if run_feature_test_PCA == True:\n",
    "    feature_test_scaled = scaler.fit_transform(feature_test)\n",
    "    feature_test_PCA = pca.transform(feature_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discussing the models, we will go over the two main metrics we used for finding a better model than the baseline. Since the data is imbalanced, we examined AUC and balanced accuracy rather than the regular accuracy metric.\n",
    "\n",
    "AUC is the area under the ROC curve which measures the TP vs FP rate as the classification decision threshold changes over time.\n",
    "\n",
    "Balanced accuracy is given by the formula $$balanced\\_accuracy = \\frac{1}{2}(\\frac{TP}{TP+FN}+\\frac{TN}{TN+FP}).$$ \n",
    "\n",
    "\n",
    "Balanced accuracy is used for imbalanced data as an estimate for the accuracy if the data was balanced, so the true performance of our models on balanced data will be close to the balanced accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model is a GBM fitted on the initial pairwise fiducial features. The parameters were chosen from a grid search with AUC scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame used to store all of the model results\n",
    "model_results_df = pd.DataFrame(columns=['Feature Extraction Train Time','Feature Extraction Test Time',\n",
    "                                         'Train Time','Prediction Time','Accuracy','Balanced Accuracy','AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimal parameters\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train_initial,label_train_initial)\n",
    "#gscv.best_params_\n",
    "# output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 189.633951 seconds\n",
      "Prediction time: 0.054330 seconds\n",
      "\n",
      "Accuracy: 0.808333\n",
      "Balanced Accuracy: 0.587563\n",
      "AUC: 0.785287\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_gbm import train_gbm\n",
    "from ipynb.fs.full.test_model import test_model\n",
    "from ipynb.fs.full.compute_metrics import compute_metrics\n",
    "\n",
    "if run_baseline == True:\n",
    "    \n",
    "    [train_time,baseline] = train_gbm(feature_train_initial,label_train_initial,\n",
    "                                      learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    print('Training time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline,feature_test_initial)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_initial,label_test_initial,test_preds,baseline)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    #save baseline model\n",
    "    pickle.dump(baseline,open(\"../output/baseline.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_initial,\n",
    "                    'Feature Extraction Test Time':tm_feature_test_initial,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Baseline')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Advanced Model (SMOTEBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the advanced model, we decided to use SMOTEBoost, which is a modified version of XGBoost that uses SMOTE (Synethic Minority Oversampling Technique) (add reference). Our model also uses the improved features which do not double count distances, and the parameters were chosen from grid search with AUC scoring.\n",
    "\n",
    "The idea of SMOTE is to modify the imbalanced training data by first randomly undersampling from the majority class and then creating new synthetic minority data that is close to the existing feature space. The modified SMOTE features then have equal number of data in each class.\n",
    "\n",
    "We went with this model for a couple of reasons. First of all, it addresses the fact that the training data is imbalanced. It also has a higher AUC, accuracy, and balanced accuracy than the baseline GBM model. Finally, compared to the other candidates for the advanced model, it has the highest AUC and balanced accuracy from 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 211.646848 seconds\n",
      "Prediction time: 0.077861 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.670898\n",
      "AUC: 0.821778\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b77383d9ce92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     'Balanced Accuracy':balanced_accuracy},name='Advanced (SMOTEBoost)')\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Advanced (SMOTEBoost)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_xgb import train_xgb\n",
    "from ipynb.fs.full.test_model import test_model\n",
    "from ipynb.fs.full.compute_metrics import compute_metrics\n",
    "\n",
    "if run_advanced == True:\n",
    "    \n",
    "    [train_time, advanced] = train_xgb(feature_train_sm, label_train_sm, learning_rate=0.25, n_estimators=300,\n",
    "                                      max_depth=3,min_child_weight=1,objective='binary:logistic',scale_pos_weight=4)\n",
    "    print('Training time: {:4f} seconds'.format(train_time))\n",
    "\n",
    "         \n",
    "    [prediction_time,test_preds] = test_model(advanced,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,advanced)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    pickle.dump(advanced,open(\"../output/advanced.p\", \"wb\"))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "                    'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "                     'Train Time':train_time,\n",
    "                     'Prediction Time':prediction_time,\n",
    "                    'Accuracy':accuracy,\n",
    "                    'AUC':auc,\n",
    "                    'Balanced Accuracy':balanced_accuracy},name='Advanced (SMOTEBoost)')\n",
    "    model_results_df.loc['Advanced (SMOTEBoost)'] = row    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Step: Remaining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the other models that were candidates for the advanced model. (By default they are set to not run to save time on running this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model with Improved Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimal parameters\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "# output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 190.495405 seconds\n",
      "Prediction time: 0.027578 seconds\n",
      "\n",
      "Accuracy: 0.816667\n",
      "Balanced Accuracy: 0.610128\n",
      "AUC: 0.785287\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_improved == True:\n",
    "    \n",
    "    [train_time, baseline_improved] = train_gbm(feature_train,label_train,\n",
    "                                                learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    \n",
    "    print('Training time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(baseline_improved,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test_initial,label_test_initial,test_preds,baseline)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "\n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "                'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "                 'Train Time':train_time,\n",
    "                 'Prediction Time':prediction_time,\n",
    "                'Accuracy':accuracy,\n",
    "                'AUC':auc,\n",
    "                'Balanced Accuracy':balanced_accuracy},name='Baseline with Improved Features')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.319941 seconds\n",
      "Prediction time: 0.001212 seconds\n",
      "\n",
      "Classification Error: 0.213333\n",
      "Accuracy: 0.786667\n",
      "\n",
      "Balanced Accuracy: 0.527742\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       473\n",
      "           1       0.48      0.08      0.14       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.64      0.53      0.51       600\n",
      "weighted avg       0.73      0.79      0.72       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[462  11]\n",
      " [117  10]]\n",
      "\n",
      "AUC: 0.691332\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_pca == True:\n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_train_scaled = scaler.fit_transform(feature_train)\n",
    "    #pick the number of components that captures 95% of the variance\n",
    "    pca = PCA(n_components = 0.95, svd_solver='full').fit(feature_train_scaled)\n",
    "    feature_train_PCA = pca.transform(feature_train_scaled)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    feature_test_PCA = pca.transform(feature_test_scaled)\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    #params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3).fit(feature_train_PCA,label_train)\n",
    "    #gscv.best_params_\n",
    "    #output: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}\n",
    "\n",
    "    start = time.time()\n",
    "    gbm_pca=GradientBoostingClassifier(learning_rate=0.1,max_depth=2,n_estimators=150)\n",
    "    gbm_pca.fit(feature_train_PCA,label_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = gbm_pca.predict(feature_test_PCA)\n",
    "    end = time.time()\n",
    "    prediction_time = end-start\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = gbm_pca.predict_proba(feature_test_PCA)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "            'Feature Extraction Test Time':tm_feature_test,\n",
    "             'Train Time':train_time,\n",
    "             'Prediction Time':prediction_time,\n",
    "            'Accuracy':accuracy,\n",
    "            'AUC':auc,\n",
    "            'Balanced Accuracy':balanced_accuracy},name='Baseline with PCA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':list(range(5,55,5))}\n",
    "#gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'n_neighbors': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.397339 seconds\n",
      "Prediction time: 5.921443 seconds\n",
      "\n",
      "Accuracy: 0.791667\n",
      "Balanced Accuracy: 0.516514\n",
      "AUC: 0.674527\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_knn import train_knn\n",
    "\n",
    "if run_knn == True:\n",
    "    \n",
    "    [train_time, knn] = train_knn(feature_train,label_train,n_neighbors=25)\n",
    "    print('Training time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(knn,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,knn)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='KNN')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':list(range(5,55,5))}\n",
    "#gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {'n_neighbors': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.771669 seconds\n",
      "Prediction time: 8.114751 seconds\n",
      "\n",
      "Accuracy: 0.606667\n",
      "Balanced Accuracy: 0.638211\n",
      "AUC: 0.683691\n"
     ]
    }
   ],
   "source": [
    "if run_knn_smote == True:\n",
    "    \n",
    "    [train_time, knn] = train_knn(feature_train_sm,label_train_sm,n_neighbors=5)\n",
    "    print('Training time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(knn,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,knn)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_SMOTE,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SMOTE KNN')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 89.071407 seconds\n",
      "Prediction time: 0.079029 seconds\n",
      "\n",
      "Classification Error: 0.186667\n",
      "Accuracy: 0.813333\n",
      "\n",
      "Balanced Accuracy: 0.688652\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       473\n",
      "           1       0.57      0.47      0.52       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.72      0.69      0.70       600\n",
      "weighted avg       0.80      0.81      0.81       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[428  45]\n",
      " [ 67  60]]\n",
      "\n",
      "AUC: 0.808726\n"
     ]
    }
   ],
   "source": [
    "if run_xgboost == True:\n",
    "    \n",
    "    [train_time, xgb] = train_xgb(feature_train, label_train, learning_rate=0.1, n_estimators=200,\n",
    "                                      max_depth=3,min_child_weight=1,objective='binary:logistic',scale_pos_weight=4)\n",
    "    print('Training time: {:4f} seconds'.format(train_time))\n",
    "\n",
    "         \n",
    "    [prediction_time,test_preds] = test_model(xgb,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "\n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,xgb)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='XGBoost')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 7.173245 seconds\n",
      "Prediction time: 0.031935 seconds\n",
      "\n",
      "Accuracy: 0.808333\n",
      "Balanced Accuracy: 0.567404\n",
      "AUC: 0.746608\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c0d022623e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         'Balanced Accuracy':balanced_accuracy},name='Random Forest')\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_random_forest import train_random_forest\n",
    "\n",
    "if run_random_forest==True:\n",
    "    \n",
    "    [train_time, rf_model] = train_random_forest(feature_train,label_train,n_estimators=100,criterion='gini',min_samples_leaf=1,max_features='sqrt')\n",
    "    print('Training time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(rf_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,rf_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Random Forest')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.055222 seconds\n",
      "Prediction time: 0.019049 seconds\n",
      "\n",
      "Accuracy: 0.821667\n",
      "Balanced Accuracy: 0.636339\n",
      "AUC: 0.786203\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-27d27ee1793f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         'Balanced Accuracy':balanced_accuracy},name='LDA')\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_lda import train_lda\n",
    "\n",
    "if run_LDA==True: \n",
    "    \n",
    "    [train_time, lda_model] = train_lda(feature_train, label_train,solver='eigen', shrinkage=.1, n_components=1)\n",
    "    print('Training time: {:4f} seconds'.format(train_time))\n",
    "    \n",
    "    [prediction_time,test_preds] = test_model(lda_model,feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(prediction_time))\n",
    "    \n",
    "    [accuracy, balanced_accuracy, auc] = compute_metrics(feature_test,label_test,test_preds,lda_model)\n",
    "    print('\\nAccuracy: {:4f}'.format(accuracy))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy))\n",
    "    print('AUC: {:4f}'.format(auc))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train_improved,\n",
    "        'Feature Extraction Test Time':tm_feature_test_improved,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='LDA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 100.128018 seconds\n",
      "Prediction time: 0.024578 seconds\n",
      "\n",
      "Classification Error: 0.178333\n",
      "Accuracy: 0.821667\n",
      "\n",
      "Balanced Accuracy: 0.699697\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       473\n",
      "           1       0.60      0.49      0.54       127\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.73      0.70      0.71       600\n",
      "weighted avg       0.81      0.82      0.81       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[431  42]\n",
      " [ 65  62]]\n",
      "\n",
      "AUC: 0.822310\n"
     ]
    }
   ],
   "source": [
    "if run_logistic==True:\n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    start_time = time.time() \n",
    "    lr = LogisticRegression(C=0.01, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=1200000,\n",
    "                   multi_class='multinomial', penalty='l2',\n",
    "                   solver='lbfgs', tol=0.0001)\n",
    "    lr_model = lr.fit(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start_time\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = lr_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    end = time.time()\n",
    "    prediction_time = end-start_time\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = lr_model.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Logistic Regression')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid={\"C\":[0.001,0.01,0.1,0.25,0.5,1,10]}\n",
    "#weights = {0:80.0, 1:20.0}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.0001,class_weight=weights),grid,cv=cv,return_train_score=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_\n",
    "#output: {'C': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 128.495774 seconds\n",
      "Prediction time: 0.023449 seconds\n",
      "\n",
      "Classification Error: 0.170000\n",
      "Accuracy: 0.830000\n",
      "\n",
      "Balanced Accuracy: 0.679063\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       473\n",
      "           1       0.65      0.42      0.51       127\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.76      0.68      0.70       600\n",
      "weighted avg       0.81      0.83      0.82       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[445  28]\n",
      " [ 74  53]]\n",
      "\n",
      "AUC: 0.822843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "if run_logistic==True:   \n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    start_time = time.time() \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    lr = LogisticRegression(C=0.001, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=120000000000,\n",
    "                   multi_class='multinomial', penalty='l2',\n",
    "                   solver='lbfgs', tol=0.0001,class_weight=weights)\n",
    "    lr_model = lr.fit(feature_train,label_train)\n",
    "    train_time = time.time()-start_time\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = lr_model.predict(feature_test)\n",
    "    prediction_time = time.time()-start_time\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    " \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = lr_model.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted Logistic')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.00001,0.0001,0.001,0.01,1,10],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 77.353651 seconds\n",
      "Prediction time: 1.640872 seconds\n",
      "\n",
      "Classification Error: 0.145000\n",
      "Accuracy: 0.855000\n",
      "\n",
      "Balanced Accuracy: 0.683400\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91       473\n",
      "           1       0.84      0.39      0.53       127\n",
      "\n",
      "    accuracy                           0.85       600\n",
      "   macro avg       0.85      0.68      0.72       600\n",
      "weighted avg       0.85      0.85      0.83       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[464   9]\n",
      " [ 78  49]]\n",
      "\n",
      "AUC: 0.828137\n"
     ]
    }
   ],
   "source": [
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:\n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    start_time = time.time() \n",
    "    svm = SVC(C=10, kernel='poly', degree=4, random_state=2020,probability=True)\n",
    "    svm_model = svm.fit(feature_train,label_train)\n",
    "    train_time = time.time()-start_time\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    prediction_time = time.time()-start_time\n",
    "\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = svm_model.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "         'Train Time':train_time,\n",
    "         'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SVM')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.001,0.01,1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train_PCA,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 2, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.726500 seconds\n",
      "Prediction time: 0.017300 seconds\n",
      "\n",
      "Classification Error: 0.213333\n",
      "Accuracy: 0.786667\n",
      "\n",
      "Balanced Accuracy: 0.585341\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       473\n",
      "           1       0.49      0.24      0.32       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.66      0.59      0.60       600\n",
      "weighted avg       0.75      0.79      0.76       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[442  31]\n",
      " [ 97  30]]\n",
      "\n",
      "AUC: 0.745093\n"
     ]
    }
   ],
   "source": [
    "#improved svm with PCA\n",
    "if run_svm==True:    \n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_train_scaled = scaler.fit_transform(feature_train)\n",
    "    #pick the number of components that captures 95% of the variance\n",
    "    pca = PCA(n_components = 0.95, svd_solver='full').fit(feature_train_scaled)\n",
    "    feature_train_PCA = pca.transform(feature_train_scaled)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    feature_test_PCA = pca.transform(feature_test_scaled)\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    start_time = time.time() \n",
    "    svm = SVC(C=10, kernel='rbf', degree=2, random_state=2020,probability=True)\n",
    "    svm_model = svm.fit(feature_train_PCA,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = svm_model.predict_proba(feature_test_PCA)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SVM with PCA')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# params= {'C': [1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(SVC(class_weight=weights,random_state = 2020,probability=True), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: output: {'C': 1, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 396.494081 seconds\n",
      "Prediction time: 1.578482 seconds\n",
      "\n",
      "Classification Error: 0.163333\n",
      "Accuracy: 0.836667\n",
      "\n",
      "Balanced Accuracy: 0.717851\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       473\n",
      "           1       0.64      0.51      0.57       127\n",
      "\n",
      "    accuracy                           0.84       600\n",
      "   macro avg       0.76      0.72      0.73       600\n",
      "weighted avg       0.83      0.84      0.83       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[437  36]\n",
      " [ 62  65]]\n",
      "\n",
      "AUC: 0.837209\n"
     ]
    }
   ],
   "source": [
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:    \n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    start_time = time.time() \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    svm = SVC(C=10, kernel='poly', degree=4, random_state=2020,class_weight=weights,probability=True)\n",
    "    svm_model = svm.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = svm_model.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted SVM')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.128203 seconds\n",
      "Prediction time: 0.034613 seconds\n",
      "\n",
      "Classification Error: 0.336667\n",
      "Accuracy: 0.663333\n",
      "\n",
      "Balanced Accuracy: 0.628073\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.69      0.76       473\n",
      "           1       0.33      0.57      0.42       127\n",
      "\n",
      "    accuracy                           0.66       600\n",
      "   macro avg       0.59      0.63      0.59       600\n",
      "weighted avg       0.74      0.66      0.69       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[326 147]\n",
      " [ 55  72]]\n",
      "\n",
      "AUC: 0.660369\n"
     ]
    }
   ],
   "source": [
    "if run_naivebayes == True:\n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    start = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gaussian = gnb.fit(feature_train, label_train)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_pred = gaussian.predict(feature_test)\n",
    "    end = time.time()\n",
    "    prediction_time = end-start\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_pred) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_pred)\n",
    "    test_probs = gaussian.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error))\n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_pred)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_pred))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_pred))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "       \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Naive Bayes')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 108.838574 seconds\n",
      "Prediction time: 0.018132 seconds\n",
      "\n",
      "Classification Error: 0.171667\n",
      "Accuracy: 0.828333\n",
      "\n",
      "Balanced Accuracy: 0.692406\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.90       473\n",
      "           1       0.63      0.46      0.53       127\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.75      0.69      0.71       600\n",
      "weighted avg       0.81      0.83      0.82       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[439  34]\n",
      " [ 69  58]]\n",
      "\n",
      "AUC: 0.824241\n"
     ]
    }
   ],
   "source": [
    "if run_lasso==True:   \n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    start_time = time.time() \n",
    "    lasso = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    lasso_model = lasso.fit(feature_train,label_train)\n",
    "    train_time = time.time()-start_time\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = lasso_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    prediction_time = time.time()-start_time\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = lasso_model.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "\n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Lasso')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {0:80.0, 1:20.0}\n",
    "# param= {'solver':['liblinear', 'saga']}\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# gscv = GridSearchCV(LogisticRegression(penalty='l1', class_weight=weights), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 59.915938 seconds\n",
      "Prediction time: 0.017789 seconds\n",
      "\n",
      "Classification Error: 0.168333\n",
      "Accuracy: 0.831667\n",
      "\n",
      "Balanced Accuracy: 0.622522\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90       473\n",
      "           1       0.82      0.26      0.40       127\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.83      0.62      0.65       600\n",
      "weighted avg       0.83      0.83      0.79       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[466   7]\n",
      " [ 94  33]]\n",
      "\n",
      "AUC: 0.819414\n"
     ]
    }
   ],
   "source": [
    "if run_lasso==True: \n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    start_time = time.time() \n",
    "    lasso_w = LogisticRegression(penalty='l1', solver='liblinear', class_weight=weights)\n",
    "    lasso_model_w = lasso_w.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    train_time = time.time()-start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = lasso_model_w.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    prediction_time = time.time()-start_time\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = lasso_model_w.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='Weighted Lasso')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_estimators':[25,50,75,100]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(BaggingClassifier(),params,cv=cv,scoring='roc_auc').fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {{'n_estimators': 100}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 648.656514 seconds\n",
      "Prediction time: 0.470589 seconds\n",
      "\n",
      "Classification Error: 0.198333\n",
      "Accuracy: 0.801667\n",
      "\n",
      "Balanced Accuracy: 0.629414\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       473\n",
      "           1       0.55      0.33      0.41       127\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.70      0.63      0.65       600\n",
      "weighted avg       0.78      0.80      0.78       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[439  34]\n",
      " [ 85  42]]\n",
      "\n",
      "AUC: 0.777122\n"
     ]
    }
   ],
   "source": [
    "if run_bagging_smote == True:\n",
    "    \n",
    "    tm_feature_train = np.nan\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "    label_train = dat_train['labels']\n",
    "    over = SMOTE(sampling_strategy='auto')\n",
    "    under = RandomUnderSampler(sampling_strategy='auto')\n",
    "    sm = Pipeline(steps = [('o', over), ('u', under)])\n",
    "    feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "         \n",
    "    tm_feature_test = np.nan\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "    label_test = dat_test['labels']\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    \n",
    "    \n",
    "    start_time=time.time()\n",
    "    bagging_smote = BaggingClassifier(n_estimators = 100)\n",
    "    bagging_smote.fit(feature_train_sm, label_train_sm)\n",
    "    train_time = time.time()-start_time\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_preds = bagging_smote.predict(feature_test)\n",
    "    prediction_time = time.time()-start_time\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    accuracy = 1-classification_error\n",
    "    balanced_accuracy = balanced_accuracy_score(label_test,test_preds)\n",
    "    test_probs = bagging_smote.predict_proba(feature_test)[:,1]\n",
    "    auc = roc_auc_score(label_test,test_probs)\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Balanced Accuracy: {:4f}'.format(balanced_accuracy_score(label_test,test_preds)))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    row = pd.Series({'Feature Extraction Train Time':tm_feature_train,\n",
    "        'Feature Extraction Test Time':tm_feature_test,\n",
    "        'Train Time':train_time,\n",
    "        'Prediction Time':prediction_time,\n",
    "        'Accuracy':accuracy,\n",
    "        'AUC':auc,\n",
    "        'Balanced Accuracy':balanced_accuracy},name='SMOTE Bagging')\n",
    "    model_results_df = model_results_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After undersampling/oversampling, we now have equal number of members in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   1929 \n",
      "Number of records with label 1 (complex emotion): 1929 \n"
     ]
    }
   ],
   "source": [
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(len(label_train_sm)-sum(label_train_sm)))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(sum(label_train_sm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted lasso\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(lasso_w, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8445833333333332\n",
    "# roc_auc output:0.8010359440647241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted svm\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(svm, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8195833333333333\n",
    "# roc_auc output:0.8117648633580459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted Logistic\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(lr, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8300000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with SMOTE\n",
    "#print(\"Cross Validation Score: \", np.mean(cross_val_score(xgb_sm, feature_train, label_train, cv=10, scoring='roc_auc')))\n",
    "# output:0.8300000000000001\n",
    "# roc_auc output:0.8387998261113715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Extraction Train Time</th>\n",
       "      <th>Feature Extraction Test Time</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>5.087896</td>\n",
       "      <td>1.252400</td>\n",
       "      <td>186.447609</td>\n",
       "      <td>0.050044</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.595437</td>\n",
       "      <td>0.785237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advanced (SMOTEBoost)</th>\n",
       "      <td>2.307424</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>207.430647</td>\n",
       "      <td>0.070880</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.712274</td>\n",
       "      <td>0.825024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with Improved Features</th>\n",
       "      <td>0.148918</td>\n",
       "      <td>0.033018</td>\n",
       "      <td>178.205290</td>\n",
       "      <td>0.029718</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.606191</td>\n",
       "      <td>0.798672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with PCA</th>\n",
       "      <td>8.117146</td>\n",
       "      <td>0.039645</td>\n",
       "      <td>1.319941</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.527742</td>\n",
       "      <td>0.691332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.151489</td>\n",
       "      <td>0.035817</td>\n",
       "      <td>0.419009</td>\n",
       "      <td>6.096245</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.516514</td>\n",
       "      <td>0.674527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE KNN</th>\n",
       "      <td>2.284506</td>\n",
       "      <td>0.029388</td>\n",
       "      <td>0.747403</td>\n",
       "      <td>8.509360</td>\n",
       "      <td>0.578333</td>\n",
       "      <td>0.623121</td>\n",
       "      <td>0.682376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.152626</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.688652</td>\n",
       "      <td>0.808726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.153098</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.574221</td>\n",
       "      <td>0.752093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.155403</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.636339</td>\n",
       "      <td>0.786203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.166774</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>100.128016</td>\n",
       "      <td>0.024673</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.699697</td>\n",
       "      <td>0.822310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Logistic</th>\n",
       "      <td>0.156370</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>128.495772</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.679063</td>\n",
       "      <td>0.822843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.149456</td>\n",
       "      <td>0.030665</td>\n",
       "      <td>77.353649</td>\n",
       "      <td>1.641004</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.828137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM with PCA</th>\n",
       "      <td>7.166126</td>\n",
       "      <td>0.030425</td>\n",
       "      <td>77.353649</td>\n",
       "      <td>1.641004</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.585341</td>\n",
       "      <td>0.745093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted SVM</th>\n",
       "      <td>0.143586</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>77.353649</td>\n",
       "      <td>1.641004</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.717851</td>\n",
       "      <td>0.837209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.148540</td>\n",
       "      <td>0.028759</td>\n",
       "      <td>0.128203</td>\n",
       "      <td>0.034613</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.628073</td>\n",
       "      <td>0.660369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.147932</td>\n",
       "      <td>0.033399</td>\n",
       "      <td>108.838572</td>\n",
       "      <td>0.018197</td>\n",
       "      <td>0.828333</td>\n",
       "      <td>0.692406</td>\n",
       "      <td>0.824241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Lasso</th>\n",
       "      <td>0.132466</td>\n",
       "      <td>0.032575</td>\n",
       "      <td>59.916026</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>0.831667</td>\n",
       "      <td>0.622522</td>\n",
       "      <td>0.819414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Bagging</th>\n",
       "      <td>2.143459</td>\n",
       "      <td>0.025363</td>\n",
       "      <td>648.656513</td>\n",
       "      <td>0.470587</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.629414</td>\n",
       "      <td>0.777122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Feature Extraction Train Time  \\\n",
       "Baseline                                              5.087896   \n",
       "Advanced (SMOTEBoost)                                 2.307424   \n",
       "Baseline with Improved Features                       0.148918   \n",
       "Baseline with PCA                                     8.117146   \n",
       "KNN                                                   0.151489   \n",
       "SMOTE KNN                                             2.284506   \n",
       "XGBoost                                               0.152626   \n",
       "Random Forest                                         0.153098   \n",
       "LDA                                                   0.155403   \n",
       "Logistic Regression                                   0.166774   \n",
       "Weighted Logistic                                     0.156370   \n",
       "SVM                                                   0.149456   \n",
       "SVM with PCA                                          7.166126   \n",
       "Weighted SVM                                          0.143586   \n",
       "Naive Bayes                                           0.148540   \n",
       "Lasso                                                 0.147932   \n",
       "Weighted Lasso                                        0.132466   \n",
       "SMOTE Bagging                                         2.143459   \n",
       "\n",
       "                                 Feature Extraction Test Time  Train Time  \\\n",
       "Baseline                                             1.252400  186.447609   \n",
       "Advanced (SMOTEBoost)                                0.035263  207.430647   \n",
       "Baseline with Improved Features                      0.033018  178.205290   \n",
       "Baseline with PCA                                    0.039645    1.319941   \n",
       "KNN                                                  0.035817    0.419009   \n",
       "SMOTE KNN                                            0.029388    0.747403   \n",
       "XGBoost                                              0.040548    0.040548   \n",
       "Random Forest                                        0.032813    0.040548   \n",
       "LDA                                                  0.032234    0.040548   \n",
       "Logistic Regression                                  0.031862  100.128016   \n",
       "Weighted Logistic                                    0.031693  128.495772   \n",
       "SVM                                                  0.030665   77.353649   \n",
       "SVM with PCA                                         0.030425   77.353649   \n",
       "Weighted SVM                                         0.029650   77.353649   \n",
       "Naive Bayes                                          0.028759    0.128203   \n",
       "Lasso                                                0.033399  108.838572   \n",
       "Weighted Lasso                                       0.032575   59.916026   \n",
       "SMOTE Bagging                                        0.025363  648.656513   \n",
       "\n",
       "                                 Prediction Time  Accuracy  Balanced Accuracy  \\\n",
       "Baseline                                0.050044  0.811667           0.595437   \n",
       "Advanced (SMOTEBoost)                   0.070880  0.823333           0.712274   \n",
       "Baseline with Improved Features         0.029718  0.815000           0.606191   \n",
       "Baseline with PCA                       0.001212  0.786667           0.527742   \n",
       "KNN                                     6.096245  0.791667           0.516514   \n",
       "SMOTE KNN                               8.509360  0.578333           0.623121   \n",
       "XGBoost                                 0.040548  0.813333           0.688652   \n",
       "Random Forest                           0.040548  0.810000           0.574221   \n",
       "LDA                                     0.040548  0.821667           0.636339   \n",
       "Logistic Regression                     0.024673  0.821667           0.699697   \n",
       "Weighted Logistic                       0.023447  0.830000           0.679063   \n",
       "SVM                                     1.641004  0.855000           0.683400   \n",
       "SVM with PCA                            1.641004  0.786667           0.585341   \n",
       "Weighted SVM                            1.641004  0.836667           0.717851   \n",
       "Naive Bayes                             0.034613  0.663333           0.628073   \n",
       "Lasso                                   0.018197  0.828333           0.692406   \n",
       "Weighted Lasso                          0.017855  0.831667           0.622522   \n",
       "SMOTE Bagging                           0.470587  0.801667           0.629414   \n",
       "\n",
       "                                      AUC  \n",
       "Baseline                         0.785237  \n",
       "Advanced (SMOTEBoost)            0.825024  \n",
       "Baseline with Improved Features  0.798672  \n",
       "Baseline with PCA                0.691332  \n",
       "KNN                              0.674527  \n",
       "SMOTE KNN                        0.682376  \n",
       "XGBoost                          0.808726  \n",
       "Random Forest                    0.752093  \n",
       "LDA                              0.786203  \n",
       "Logistic Regression              0.822310  \n",
       "Weighted Logistic                0.822843  \n",
       "SVM                              0.828137  \n",
       "SVM with PCA                     0.745093  \n",
       "Weighted SVM                     0.837209  \n",
       "Naive Bayes                      0.660369  \n",
       "Lasso                            0.824241  \n",
       "Weighted Lasso                   0.819414  \n",
       "SMOTE Bagging                    0.777122  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_df = model_results_df.applymap(lambda x: round(x,3))\n",
    "model_results_df['Train Time'] = [str(x)+' s' for x in list(model_results_df['Train Time'])]\n",
    "model_results_df['Prediction Time'] = [str(x)+' s' for x in list(model_results_df['Prediction Time'])]\n",
    "model_results_df['Feature Extraction Train Time'] = [str(x)+' s' for x in list(model_results_df['Feature Extraction Train Time'])]\n",
    "model_results_df['Feature Extraction Test Time'] = [str(x)+' s' for x in list(model_results_df['Feature Extraction Test Time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Extraction Train Time</th>\n",
       "      <th>Feature Extraction Test Time</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>5.088 s</td>\n",
       "      <td>1.252 s</td>\n",
       "      <td>186.448 s</td>\n",
       "      <td>0.05 s</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advanced (SMOTEBoost)</th>\n",
       "      <td>2.307 s</td>\n",
       "      <td>0.035 s</td>\n",
       "      <td>207.431 s</td>\n",
       "      <td>0.071 s</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with Improved Features</th>\n",
       "      <td>0.149 s</td>\n",
       "      <td>0.033 s</td>\n",
       "      <td>178.205 s</td>\n",
       "      <td>0.03 s</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline with PCA</th>\n",
       "      <td>8.117 s</td>\n",
       "      <td>0.04 s</td>\n",
       "      <td>1.32 s</td>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.151 s</td>\n",
       "      <td>0.036 s</td>\n",
       "      <td>0.419 s</td>\n",
       "      <td>6.096 s</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE KNN</th>\n",
       "      <td>2.285 s</td>\n",
       "      <td>0.029 s</td>\n",
       "      <td>0.747 s</td>\n",
       "      <td>8.509 s</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.153 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.153 s</td>\n",
       "      <td>0.033 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.155 s</td>\n",
       "      <td>0.032 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.041 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.167 s</td>\n",
       "      <td>0.032 s</td>\n",
       "      <td>100.128 s</td>\n",
       "      <td>0.025 s</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Logistic</th>\n",
       "      <td>0.156 s</td>\n",
       "      <td>0.032 s</td>\n",
       "      <td>128.496 s</td>\n",
       "      <td>0.023 s</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.149 s</td>\n",
       "      <td>0.031 s</td>\n",
       "      <td>77.354 s</td>\n",
       "      <td>1.641 s</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM with PCA</th>\n",
       "      <td>7.166 s</td>\n",
       "      <td>0.03 s</td>\n",
       "      <td>77.354 s</td>\n",
       "      <td>1.641 s</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted SVM</th>\n",
       "      <td>0.144 s</td>\n",
       "      <td>0.03 s</td>\n",
       "      <td>77.354 s</td>\n",
       "      <td>1.641 s</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.149 s</td>\n",
       "      <td>0.029 s</td>\n",
       "      <td>0.128 s</td>\n",
       "      <td>0.035 s</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.148 s</td>\n",
       "      <td>0.033 s</td>\n",
       "      <td>108.839 s</td>\n",
       "      <td>0.018 s</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Lasso</th>\n",
       "      <td>0.132 s</td>\n",
       "      <td>0.033 s</td>\n",
       "      <td>59.916 s</td>\n",
       "      <td>0.018 s</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Bagging</th>\n",
       "      <td>2.143 s</td>\n",
       "      <td>0.025 s</td>\n",
       "      <td>648.657 s</td>\n",
       "      <td>0.471 s</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Feature Extraction Train Time  \\\n",
       "Baseline                                              5.088 s   \n",
       "Advanced (SMOTEBoost)                                 2.307 s   \n",
       "Baseline with Improved Features                       0.149 s   \n",
       "Baseline with PCA                                     8.117 s   \n",
       "KNN                                                   0.151 s   \n",
       "SMOTE KNN                                             2.285 s   \n",
       "XGBoost                                               0.153 s   \n",
       "Random Forest                                         0.153 s   \n",
       "LDA                                                   0.155 s   \n",
       "Logistic Regression                                   0.167 s   \n",
       "Weighted Logistic                                     0.156 s   \n",
       "SVM                                                   0.149 s   \n",
       "SVM with PCA                                          7.166 s   \n",
       "Weighted SVM                                          0.144 s   \n",
       "Naive Bayes                                           0.149 s   \n",
       "Lasso                                                 0.148 s   \n",
       "Weighted Lasso                                        0.132 s   \n",
       "SMOTE Bagging                                         2.143 s   \n",
       "\n",
       "                                Feature Extraction Test Time Train Time  \\\n",
       "Baseline                                             1.252 s  186.448 s   \n",
       "Advanced (SMOTEBoost)                                0.035 s  207.431 s   \n",
       "Baseline with Improved Features                      0.033 s  178.205 s   \n",
       "Baseline with PCA                                     0.04 s     1.32 s   \n",
       "KNN                                                  0.036 s    0.419 s   \n",
       "SMOTE KNN                                            0.029 s    0.747 s   \n",
       "XGBoost                                              0.041 s    0.041 s   \n",
       "Random Forest                                        0.033 s    0.041 s   \n",
       "LDA                                                  0.032 s    0.041 s   \n",
       "Logistic Regression                                  0.032 s  100.128 s   \n",
       "Weighted Logistic                                    0.032 s  128.496 s   \n",
       "SVM                                                  0.031 s   77.354 s   \n",
       "SVM with PCA                                          0.03 s   77.354 s   \n",
       "Weighted SVM                                          0.03 s   77.354 s   \n",
       "Naive Bayes                                          0.029 s    0.128 s   \n",
       "Lasso                                                0.033 s  108.839 s   \n",
       "Weighted Lasso                                       0.033 s   59.916 s   \n",
       "SMOTE Bagging                                        0.025 s  648.657 s   \n",
       "\n",
       "                                Prediction Time  Accuracy  Balanced Accuracy  \\\n",
       "Baseline                                 0.05 s     0.812              0.595   \n",
       "Advanced (SMOTEBoost)                   0.071 s     0.823              0.712   \n",
       "Baseline with Improved Features          0.03 s     0.815              0.606   \n",
       "Baseline with PCA                       0.001 s     0.787              0.528   \n",
       "KNN                                     6.096 s     0.792              0.517   \n",
       "SMOTE KNN                               8.509 s     0.578              0.623   \n",
       "XGBoost                                 0.041 s     0.813              0.689   \n",
       "Random Forest                           0.041 s     0.810              0.574   \n",
       "LDA                                     0.041 s     0.822              0.636   \n",
       "Logistic Regression                     0.025 s     0.822              0.700   \n",
       "Weighted Logistic                       0.023 s     0.830              0.679   \n",
       "SVM                                     1.641 s     0.855              0.683   \n",
       "SVM with PCA                            1.641 s     0.787              0.585   \n",
       "Weighted SVM                            1.641 s     0.837              0.718   \n",
       "Naive Bayes                             0.035 s     0.663              0.628   \n",
       "Lasso                                   0.018 s     0.828              0.692   \n",
       "Weighted Lasso                          0.018 s     0.832              0.623   \n",
       "SMOTE Bagging                           0.471 s     0.802              0.629   \n",
       "\n",
       "                                   AUC  \n",
       "Baseline                         0.785  \n",
       "Advanced (SMOTEBoost)            0.825  \n",
       "Baseline with Improved Features  0.799  \n",
       "Baseline with PCA                0.691  \n",
       "KNN                              0.675  \n",
       "SMOTE KNN                        0.682  \n",
       "XGBoost                          0.809  \n",
       "Random Forest                    0.752  \n",
       "LDA                              0.786  \n",
       "Logistic Regression              0.822  \n",
       "Weighted Logistic                0.823  \n",
       "SVM                              0.828  \n",
       "SVM with PCA                     0.745  \n",
       "Weighted SVM                     0.837  \n",
       "Naive Bayes                      0.660  \n",
       "Lasso                            0.824  \n",
       "Weighted Lasso                   0.819  \n",
       "SMOTE Bagging                    0.777  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
