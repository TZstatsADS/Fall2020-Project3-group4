{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .boolean { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .integer { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .string  { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.image as img\n",
    "import scipy.io\n",
    "import pickle\n",
    "from sklearn.metrics import pairwise_distances, classification_report, confusion_matrix, roc_auc_score\n",
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install ipynb' in the command line. This code lets us import functions from notebooks in the lib folder. Lib is supposed to have all the model training/predicting functions and the doc folder is only supposed to have report/presentation files like main.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install imblearn' in the command line. This code lets us do SMOTE (synthetic minority oversampling technique) and random undersampling to help deal with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: set work directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide directories for training images. Training images and Training fiducial points will be in different subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change train_dir to your own path\n",
    "\n",
    "#root=sys.path[0]\n",
    "#train_dir = os.path.join(root,\"../data/train_set/\")\n",
    "#train_dir = os.path.join(root,\"../data/train_set/\")\n",
    "train_dir = '/Users/rohan/Desktop/train_set/'\n",
    "train_image_dir = train_dir+\"images/\"\n",
    "train_pt_dir = train_dir+\"points/\"\n",
    "train_label_path = train_dir+\"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: set up controls for evaluation experiments.\n",
    "\n",
    "In this chunk, we have a set of controls for the evaluation experiments. \n",
    "\n",
    "+ (T/F) cross-validation on the training set\n",
    "+ (T/F) reweighting the samples for training set \n",
    "+ (number) K, the number of CV folds\n",
    "+ (T/F) process features for training set\n",
    "+ (T/F) run evaluation on an independent test set\n",
    "+ (T/F) process features for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_feature_train = True # process features for training set\n",
    "run_test = True # run evaluation on an independent test set\n",
    "run_feature_test = True # process features for test set\n",
    "run_feature_train_initial = True\n",
    "run_feature_test_initial = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set these to be False if you don't want to go through training certain models when running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline = True\n",
    "run_baseline_improved = True\n",
    "run_baseline_pca = True\n",
    "run_knn = True\n",
    "run_knn_smote = True\n",
    "run_xgboost=True\n",
    "run_xgboost_smote=True\n",
    "feature_initial=True\n",
    "run_randonforest=True\n",
    "run_LDA=True\n",
    "run_logistic=True\n",
    "run_svm = True\n",
    "run_bagging_smote = True\n",
    "run_naivebayes = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: import data and train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   2402 \n",
      "Number of records with label 1 (complex emotion): 598 \n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "n = info.shape[0]\n",
    "\n",
    "#Data is imbalanced \n",
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(info.loc[info['label']==0].shape[0]))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(info.loc[info['label']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we could use sklearn train_test_split here instead of doing it\n",
    "#manually like in the starter code\n",
    "n_train = int(round(n*(4/5),0))\n",
    "train_idx = np.random.choice(list(info.index),size=n_train,replace=False)\n",
    "test_idx = list(set(list(info.index))-set(train_idx)) #set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading images (we never use the image list later since we're just using the fiducial points for the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(os.listdir(train_image_dir))\n",
    "\n",
    "# image_list = []\n",
    "# for i in range(1,101): # 1 to 100\n",
    "#     image = img.imread(train_image_dir+'{:04d}'.format(i)+'.jpg')\n",
    "#     image_list.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial points are stored in matlab format. In this step, we read them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read fiducial points\n",
    "#input: index\n",
    "#output: matrix of fiducial points corresponding to the index\n",
    "\n",
    "def readMat_matrix(index):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinatesUnwarped']\n",
    "    except KeyError:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinates2']\n",
    "    return np.matrix.round(mat_data,0)\n",
    "\n",
    "#load fiducial points\n",
    "#pickle is the closest equivalent to .RData that I could find in Python\n",
    "fiducial_pt_list = list(map(readMat_matrix,list(range(1,n_files+1))))\n",
    "pickle.dump(fiducial_pt_list, open( \"../output/fiducial_pt_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: construct features and responses\n",
    "`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. \n",
    "  \n",
    "  + `feature.R`\n",
    "  + Input: list of images or fiducial point\n",
    "  + Output: an RData file that contains extracted features and corresponding responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature_initial.ipynb's feature_initial function to generate pairwise distance features for the baseline model. This is the same feature extraction method as that of the starter code. Note that this method double counts distances between points i.e. there are separate entries for the distance from point A to point B and the distance from point B to point A even though the distances are the same.\n",
    "\n",
    "Feature extraction times exclude the time it takes to write to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature extraction time for train: 5.407898\n",
      "Initial feature extraction time for test:  1.262230\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature_initial import feature_initial\n",
    "\n",
    "tm_feature_train = np.nan\n",
    "if run_feature_train_initial == True:\n",
    "    start = time.time()\n",
    "    dat_train_initial = feature_initial(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "    dat_train_initial.to_csv(\"../output/train_data_initial.csv\",index=False)\n",
    "    print('Initial feature extraction time for train: {:4f}'.format(tm_feature_train))\n",
    "else:\n",
    "    dat_train_initial = pd.read_csv(\"../output/train_data_initial.csv\")\n",
    "        \n",
    "        \n",
    "tm_feature_test = np.nan\n",
    "if run_feature_test_initial == True:\n",
    "    start = time.time()\n",
    "    dat_test_initial = feature_initial(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    dat_test_initial.to_csv(\"../output/test_data_initial.csv\",index=False)\n",
    "    print('Initial feature extraction time for test:  {:4f}'.format(tm_feature_test))\n",
    "else:\n",
    "    dat_test_initial = pd.read_csv(\"../output/test_data_initial.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature_improved.ipynb's feature_improved function to generate pairwise distance features to be used by all of the models other than the baseline. Unlike feature_initial, feature_improved does not double count distances. Hence, feature_improved produces exactly half as many features as feature_initial while keeping the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved feature extraction time for train: 0.182291\n",
      "Improved feature extraction time for test:  0.026276\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.feature_improved import feature_improved\n",
    "\n",
    "tm_feature_train = np.nan\n",
    "if run_feature_train == True:\n",
    "    start = time.time()\n",
    "    dat_train = feature_improved(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "    dat_train.to_csv(\"../output/train_data.csv\",index=False)\n",
    "    print('Improved feature extraction time for train: {:4f}'.format(tm_feature_train))\n",
    "else:\n",
    "    dat_train = pd.read_csv(\"../output/train_data.csv\")\n",
    "\n",
    "\n",
    "tm_feature_test = np.nan\n",
    "if run_feature_test == True:\n",
    "    start = time.time()\n",
    "    dat_test = feature_improved(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    dat_test.to_csv(\"../output/test_data.csv\",index=False)\n",
    "    print('Improved feature extraction time for test:  {:4f}'.format(tm_feature_test))\n",
    "else:\n",
    "    dat_test = pd.read_csv(\"../output/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the traning/test features and labels\n",
    "\n",
    "feature_train_initial = dat_train_initial.loc[:, dat_train_initial.columns != 'labels']\n",
    "label_train_initial = dat_train_initial['labels'] \n",
    "\n",
    "feature_test_initial = dat_test_initial.loc[:, dat_test_initial.columns != 'labels']\n",
    "label_test_initial = dat_test_initial['labels'] \n",
    "\n",
    "feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "label_train = dat_train['labels'] #same values as label_train_initial\n",
    "\n",
    "feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "label_test = dat_test['labels'] #same values as label_test_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA (made some changes)\n",
    "scaler = MinMaxScaler()\n",
    "feature_train_scaled = scaler.fit_transform(feature_train)\n",
    "feature_test_scaled = scaler.fit_transform(feature_test)\n",
    "\n",
    "#pick the number of components that captures 95% of the variance\n",
    "pca = PCA(n_components = 0.95, svd_solver='full').fit(feature_train_scaled)\n",
    "feature_train_PCA = pca.transform(feature_train_scaled)\n",
    "feature_test_PCA = pca.transform(feature_test_scaled)\n",
    "\n",
    "#print how many components after pca\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a classification model with training features and responses\n",
    "Call the train model and test model from library. \n",
    "\n",
    "`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. \n",
    "\n",
    "+ `train.R`\n",
    "  + Input: a data frame containing features and labels and a parameter list.\n",
    "  + Output:a trained model\n",
    "+ `test.R`\n",
    "  + Input: the fitted classification model using training data and processed features from testing images \n",
    "  + Input: an R object that contains a trained classifier.\n",
    "  + Output: training model specification\n",
    "\n",
    "+ In this Starter Code, we use logistic regression with LASSO penalty to do classification. \n",
    "\n",
    "* Model selection with cross-validation\n",
    "* Do model selection by choosing among different values of training model parameters.\n",
    "\n",
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 178.450470 seconds\n",
      "Prediction time: 0.044135 seconds\n",
      "\n",
      "Classification Error: 0.190000\n",
      "Accuracy: 0.810000\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89       473\n",
      "           1       0.66      0.21      0.32       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.74      0.59      0.61       600\n",
      "weighted avg       0.79      0.81      0.77       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[459  14]\n",
      " [100  27]]\n",
      "\n",
      "AUC: 0.786336\n"
     ]
    }
   ],
   "source": [
    "if run_baseline == True:\n",
    "    \n",
    "    #grid search for optimal parameters\n",
    "    #params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train_initial,label_train_initial)\n",
    "    #gscv.best_params_\n",
    "    # output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
    "    \n",
    "    start = time.time()\n",
    "    baseline = GradientBoostingClassifier(learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    baseline.fit(feature_train_initial,label_train_initial)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = baseline.predict(feature_test_initial)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test_initial))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test_initial,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test_initial,test_preds))\n",
    "    \n",
    "    test_probs = baseline.predict_proba(feature_test_initial)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))\n",
    "    \n",
    "    #save baseline model\n",
    "    pickle.dump(baseline,open(\"../output/baseline.p\", \"wb\"))\n",
    "    \n",
    "    #load baseline model\n",
    "    #baseline = pickle.load(open(\"../output/baseline.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model with Improved Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 177.320505 seconds\n",
      "Prediction time: 0.023891 seconds\n",
      "\n",
      "Classification Error: 0.185000\n",
      "Accuracy: 0.815000\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89       473\n",
      "           1       0.67      0.24      0.36       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.75      0.61      0.63       600\n",
      "weighted avg       0.79      0.81      0.78       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[458  15]\n",
      " [ 96  31]]\n",
      "\n",
      "AUC: 0.799421\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_improved == True:\n",
    "    \n",
    "    #grid search for optimal parameters\n",
    "    #params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3,scoring='roc_auc').fit(feature_train,label_train)\n",
    "    #gscv.best_params_\n",
    "    # output: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
    "    \n",
    "    start = time.time()\n",
    "    baseline = GradientBoostingClassifier(learning_rate=0.1,max_depth=3,n_estimators=150)\n",
    "    baseline.fit(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = baseline.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = baseline.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.185983 seconds\n",
      "Prediction time: 0.001168 seconds\n",
      "\n",
      "Classification Error: 0.210000\n",
      "Accuracy: 0.790000\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       473\n",
      "           1       0.52      0.09      0.16       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.66      0.54      0.52       600\n",
      "weighted avg       0.74      0.79      0.73       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[462  11]\n",
      " [115  12]]\n",
      "\n",
      "AUC: 0.691598\n"
     ]
    }
   ],
   "source": [
    "if run_baseline_pca == True:\n",
    "    \n",
    "    #params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3], 'n_estimators':[50,100,150]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=3).fit(feature_train_PCA,label_train)\n",
    "    #gscv.best_params_\n",
    "    #output: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}\n",
    "\n",
    "    #Baseline model with PCA\n",
    "    #need to do grid search to get optimal parameters though\n",
    "    start = time.time()\n",
    "    gbm_pca=GradientBoostingClassifier(learning_rate=0.1,max_depth=2,n_estimators=150)\n",
    "    gbm_pca.fit(feature_train_PCA,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = gbm_pca.predict(feature_test_PCA)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = gbm_pca.predict_proba(feature_test_PCA)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.377309 seconds\n",
      "Prediction time: 5.328446 seconds\n",
      "\n",
      "Classification Error: 0.208333\n",
      "Accuracy: 0.791667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       473\n",
      "           1       0.62      0.04      0.07       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.71      0.52      0.48       600\n",
      "weighted avg       0.76      0.79      0.71       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[470   3]\n",
      " [122   5]]\n",
      "\n",
      "AUC: 0.674527\n"
     ]
    }
   ],
   "source": [
    "if run_knn == True:\n",
    "    \n",
    "    #params = {'n_neighbors':list(range(5,55,5))}\n",
    "    #gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train,label_train)\n",
    "    #gscv.best_params_\n",
    "    #output: {'n_neighbors': 25}\n",
    "    \n",
    "    start = time.time()\n",
    "    #need to cross validate to pick best value (after finalizing features)\n",
    "    knn = KNeighborsClassifier(n_neighbors = 25) \n",
    "    knn.fit(feature_train,label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = knn.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test)) #Classification Error\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = knn.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN With SMOTE and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.717526 seconds\n",
      "Prediction time: 7.323308 seconds\n",
      "\n",
      "Classification Error: 0.408333\n",
      "Accuracy: 0.591667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.57      0.69       473\n",
      "           1       0.29      0.66      0.41       127\n",
      "\n",
      "    accuracy                           0.59       600\n",
      "   macro avg       0.58      0.62      0.55       600\n",
      "weighted avg       0.74      0.59      0.63       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[271 202]\n",
      " [ 43  84]]\n",
      "\n",
      "AUC: 0.682293\n"
     ]
    }
   ],
   "source": [
    "if run_knn_smote == True:\n",
    "    \n",
    "    over = SMOTE(sampling_strategy='auto')\n",
    "    under = RandomUnderSampler(sampling_strategy='auto')\n",
    "    sm = Pipeline(steps = [('o', over), ('u', under)])\n",
    "    feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "    \n",
    "    #params = {'n_neighbors':list(range(5,55,5))}\n",
    "    #gscv = GridSearchCV(KNeighborsClassifier(),params,cv=5).fit(feature_train_sm,label_train_sm)\n",
    "    #gscv.best_params_\n",
    "    #output: {'n_neighbors': 5}\n",
    "    \n",
    "    start = time.time()\n",
    "    #need to cross validate to pick best value (after finalizing features)\n",
    "    knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "    knn.fit(feature_train_sm,label_train_sm)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_preds = knn.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test)) #Classification Error\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = knn.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 83.955818 seconds\n",
      "Prediction time: 0.069285 seconds\n",
      "\n",
      "Classification Error: 0.186667\n",
      "Accuracy: 0.813333\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       473\n",
      "           1       0.57      0.47      0.52       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.72      0.69      0.70       600\n",
      "weighted avg       0.80      0.81      0.81       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[428  45]\n",
      " [ 67  60]]\n",
      "\n",
      "AUC: 0.808726\n"
     ]
    }
   ],
   "source": [
    "if run_xgboost == True:\n",
    "    \n",
    "    # train_labels_xgb = [ x  - 1 for x in label_train ] \n",
    "    # test_labels_xgb = [ x  - 1 for x in label_test ] \n",
    "    start_time=time.time()\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "     n_estimators= 200,\n",
    "     max_depth=3,\n",
    "     min_child_weight=1,\n",
    "     objective= 'binary:logistic',\n",
    "     scale_pos_weight=4\n",
    "    )\n",
    "    \n",
    "    xgb.fit(feature_train, label_train ,eval_metric='auc')\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_preds = xgb.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = xgb.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'learning_rate':[0.1,0.25,0.5],'n_estimators':[100,200,300]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(XGBClassifier(min_child_weight=1,max_depth=3,objective= 'binary:logistic',scale_pos_weight=4),params,cv=cv,scoring='roc_auc',verbose=True).fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {'learning_rate': 0.25, 'n_estimators': 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 197.353105 seconds\n",
      "Prediction time: 0.076977 seconds\n",
      "\n",
      "Classification Error: 0.173333\n",
      "Accuracy: 0.826667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89       473\n",
      "           1       0.60      0.54      0.57       127\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.74      0.72      0.73       600\n",
      "weighted avg       0.82      0.83      0.82       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[427  46]\n",
      " [ 58  69]]\n",
      "\n",
      "AUC: 0.810441\n"
     ]
    }
   ],
   "source": [
    "# train_labels_xgb = [ x  - 1 for x in label_train ] \n",
    "# test_labels_xgb = [ x  - 1 for x in label_test ] \n",
    "\n",
    "if run_xgboost_smote == True:\n",
    "    #sm = SMOTE(sampling_strategy='auto',k_neighbors=20,random_state=42)\n",
    "    #feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "\n",
    "    over = SMOTE(sampling_strategy='auto')\n",
    "    under = RandomUnderSampler(sampling_strategy='auto')\n",
    "    sm = Pipeline(steps = [('o', over), ('u', under)])\n",
    "    feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "    \n",
    "    start_time=time.time()\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "     learning_rate =0.25,\n",
    "     n_estimators= 300,\n",
    "     max_depth=3,\n",
    "     min_child_weight=1,\n",
    "     objective= 'binary:logistic',\n",
    "     scale_pos_weight=4\n",
    "    )\n",
    "\n",
    "    xgb.fit(feature_train_sm, label_train_sm ,eval_metric='auc')\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_preds = xgb.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "\n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "\n",
    "    test_probs = xgb.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandonForest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.330855 seconds\n",
      "Prediction time: 0.030510 seconds\n",
      "\n",
      "Classification Error: 0.190000\n",
      "Accuracy: 0.810000\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       473\n",
      "           1       0.78      0.14      0.24       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.80      0.57      0.57       600\n",
      "weighted avg       0.81      0.81      0.75       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[468   5]\n",
      " [109  18]]\n",
      "\n",
      "AUC: 0.760483\n"
     ]
    }
   ],
   "source": [
    "if run_randonforest==True:\n",
    "    start_time = time.time()\n",
    "    rf = RandomForestClassifier(n_estimators = 100, criterion = 'gini', min_samples_leaf=1, max_features='sqrt')\n",
    "    rf_model = rf.fit(feature_train, label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = rf_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = rf_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.302199 seconds\n",
      "Prediction time: 0.021439 seconds\n",
      "\n",
      "Classification Error: 0.178333\n",
      "Accuracy: 0.821667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.89       473\n",
      "           1       0.67      0.31      0.43       127\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.75      0.64      0.66       600\n",
      "weighted avg       0.80      0.82      0.80       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[453  20]\n",
      " [ 87  40]]\n",
      "\n",
      "AUC: 0.786203\n"
     ]
    }
   ],
   "source": [
    "if run_LDA==True:    \n",
    "    start_time = time.time()\n",
    "    lda = LDA(solver='eigen', shrinkage=.1, n_components=1)\n",
    "    lda_model = lda.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds=lda_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lda_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 106.160704 seconds\n",
      "Prediction time: 0.023070 seconds\n",
      "\n",
      "Classification Error: 0.178333\n",
      "Accuracy: 0.821667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       473\n",
      "           1       0.60      0.49      0.54       127\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.73      0.70      0.71       600\n",
      "weighted avg       0.81      0.82      0.81       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[431  42]\n",
      " [ 65  62]]\n",
      "\n",
      "AUC: 0.822310\n"
     ]
    }
   ],
   "source": [
    "if run_logistic==True:    \n",
    "    start_time = time.time() \n",
    "    lr = LogisticRegression(C=0.01, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=1200000,\n",
    "                   multi_class='multinomial', penalty='l2',\n",
    "                   solver='lbfgs', tol=0.0001)\n",
    "    lr_model = lr.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds = lr_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lr_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'C':[0.1,0.25,0.5,1]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(LogisticRegression(dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, max_iter=1200000,\n",
    "#                   multi_class='multinomial', penalty='l2',\n",
    "#                   solver='lbfgs', tol=0.1),params,cv=cv,scoring='roc_auc',verbose=True).fit(feature_train,label_train)\n",
    "#gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 190.796331 seconds\n",
      "Prediction time: 0.030899 seconds\n",
      "\n",
      "Classification Error: 0.205000\n",
      "Accuracy: 0.795000\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87       473\n",
      "           1       0.52      0.54      0.53       127\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.69      0.70      0.70       600\n",
      "weighted avg       0.80      0.80      0.80       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[409  64]\n",
      " [ 59  68]]\n",
      "\n",
      "AUC: 0.812106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "if run_logistic==True:    \n",
    "    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    start_time = time.time() \n",
    "    lr = LogisticRegression(C=0.1, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=12000000,\n",
    "                   multi_class='ovr', penalty='l2',\n",
    "                   solver='lbfgs', tol=0.1,class_weight=weights)\n",
    "    lr_model = lr.fit(feature_train_sm,label_train_sm)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    start_time = time.time()\n",
    "    test_preds = lr_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = lr_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 51.635884 seconds\n",
      "Prediction time: 1.817035 seconds\n",
      "\n",
      "Classification Error: 0.188333\n",
      "Accuracy: 0.811667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       473\n",
      "           1       0.85      0.13      0.23       127\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.83      0.56      0.56       600\n",
      "weighted avg       0.82      0.81      0.75       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[470   3]\n",
      " [110  17]]\n",
      "\n",
      "AUC: 0.758053\n"
     ]
    }
   ],
   "source": [
    "if run_svm==True:    \n",
    "    start_time = time.time()\n",
    "    svm2 = SVC(C=0.00001, kernel='linear', random_state=2020,probability=True) \n",
    "    svm2_model = svm2.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm2_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = svm2_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.00001,0.0001,0.001,0.01,1,10],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 4, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 79.167671 seconds\n",
      "Prediction time: 1.844987 seconds\n",
      "\n",
      "Classification Error: 0.145000\n",
      "Accuracy: 0.855000\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91       473\n",
      "           1       0.84      0.39      0.53       127\n",
      "\n",
      "    accuracy                           0.85       600\n",
      "   macro avg       0.85      0.68      0.72       600\n",
      "weighted avg       0.85      0.85      0.83       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[464   9]\n",
      " [ 78  49]]\n",
      "\n",
      "AUC: 0.828137\n"
     ]
    }
   ],
   "source": [
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:    \n",
    "    start_time = time.time() \n",
    "    svm = SVC(C=10, kernel='poly', degree=4, random_state=2020,probability=True)\n",
    "    svm_model = svm.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = svm_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search with cv 3 to find the best performed parameters\n",
    "# param= {'C': [0.001,0.01,1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "\n",
    "# gscv = GridSearchCV(SVC(random_state = 2020), param, cv=3, return_train_score=True)\n",
    "# gscv.fit(feature_train_PCA,label_train)\n",
    "# gscv.best_params_\n",
    "# #output: {'C': 10, 'degree': 2, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.746107 seconds\n",
      "Prediction time: 0.015865 seconds\n",
      "\n",
      "Classification Error: 0.213333\n",
      "Accuracy: 0.786667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       473\n",
      "           1       0.49      0.24      0.32       127\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.66      0.59      0.60       600\n",
      "weighted avg       0.75      0.79      0.76       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[442  31]\n",
      " [ 97  30]]\n",
      "\n",
      "AUC: 0.745093\n"
     ]
    }
   ],
   "source": [
    "#improved svm with PCA\n",
    "if run_svm==True:    \n",
    "    start_time = time.time() \n",
    "    svm = SVC(C=10, kernel='rbf', degree=2, random_state=2020,probability=True)\n",
    "    svm_model = svm.fit(feature_train_PCA,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test_PCA)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = svm_model.predict_proba(feature_test_PCA)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = {0:80.0, 1:20.0}\n",
    "#params= {'C': [1,10,15,20],\n",
    "#        'kernel':['linear', 'rbf', 'poly'],\n",
    "#        'degree':[2,3,4]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(SVC(class_weight=weights,random_state = 2020,probability=True), params, cv=3, scoring='roc_auc',verbose=True)\n",
    "#gscv.fit(feature_train,label_train)\n",
    "#gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 382.177514 seconds\n",
      "Prediction time: 1.555441 seconds\n",
      "\n",
      "Classification Error: 0.163333\n",
      "Accuracy: 0.836667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       473\n",
      "           1       0.64      0.51      0.57       127\n",
      "\n",
      "    accuracy                           0.84       600\n",
      "   macro avg       0.76      0.72      0.73       600\n",
      "weighted avg       0.83      0.84      0.83       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[437  36]\n",
      " [ 62  65]]\n",
      "\n",
      "AUC: 0.837209\n"
     ]
    }
   ],
   "source": [
    "#improved svm using parameters from grid search\n",
    "if run_svm==True:    \n",
    "    weights = {0:80.0, 1:20.0}\n",
    "    start_time = time.time() \n",
    "    svm = SVC(C=10, kernel='poly', degree=4, random_state=2020,class_weight=weights,probability=True)\n",
    "    svm_model = svm.fit(feature_train,label_train)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_preds = svm_model.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "    \n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "    \n",
    "    test_probs = svm_model.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.131302 seconds\n",
      "Prediction time: 0.033022 seconds\n",
      "\\nClassification Error: 0.336667\n",
      "Accuracy: 0.663333\\n\n",
      "Classification Report:\\n\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.69      0.76       473\n",
      "           1       0.33      0.57      0.42       127\n",
      "\n",
      "    accuracy                           0.66       600\n",
      "   macro avg       0.59      0.63      0.59       600\n",
      "weighted avg       0.74      0.66      0.69       600\n",
      "\n",
      "Confusion Matrix:\\n\n",
      "[[326 147]\n",
      " [ 55  72]]\n",
      "\n",
      "AUC: 0.660369\n"
     ]
    }
   ],
   "source": [
    "if run_naivebayes == True:\n",
    "    start = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gaussian = gnb.fit(feature_train, label_train)\n",
    "    end = time.time()\n",
    "    print('Training time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_pred = gaussian.predict(feature_test)\n",
    "    end = time.time()\n",
    "    print('Prediction time: {:4f} seconds'.format(end-start))\n",
    "    \n",
    "    classification_error = np.mean(np.array(test_pred) != np.array(label_test)) #Classification Error\n",
    "    \n",
    "    print('\\\\nClassification Error: {:4f}'.format(classification_error))\n",
    "    print('Accuracy: {:4f}\\\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\\\n')\n",
    "    print(classification_report(label_test,test_pred))\n",
    "    print('Confusion Matrix:\\\\n')\n",
    "    print(confusion_matrix(label_test,test_pred))\n",
    "    \n",
    "    test_probs = gaussian.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_estimators':[25,50,75,100]}\n",
    "#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "#gscv = GridSearchCV(BaggingClassifier(),params,cv=cv,scoring='roc_auc').fit(feature_train_sm,label_train_sm)\n",
    "#gscv.best_params_\n",
    "#output: {{'n_estimators': 100}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 660.855384 seconds\n",
      "Prediction time: 0.435849 seconds\n",
      "\n",
      "Classification Error: 0.198333\n",
      "Accuracy: 0.801667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88       473\n",
      "           1       0.55      0.35      0.43       127\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.70      0.64      0.66       600\n",
      "weighted avg       0.78      0.80      0.78       600\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[436  37]\n",
      " [ 82  45]]\n",
      "\n",
      "AUC: 0.775174\n"
     ]
    }
   ],
   "source": [
    "if run_bagging_smote == True:\n",
    "    \n",
    "    over = SMOTE(sampling_strategy='auto')\n",
    "    under = RandomUnderSampler(sampling_strategy='auto')\n",
    "    sm = Pipeline(steps = [('o', over), ('u', under)])\n",
    "    feature_train_sm, label_train_sm = sm.fit_resample(feature_train,label_train)\n",
    "    \n",
    "    start_time=time.time()\n",
    "\n",
    "    bagging_smote = BaggingClassifier(n_estimators = 100)\n",
    "    bagging_smote.fit(feature_train_sm, label_train_sm)\n",
    "    print('Training time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_preds = bagging_smote.predict(feature_test)\n",
    "    print('Prediction time: {:4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    classification_error = np.mean(np.array(test_preds) != np.array(label_test))\n",
    "\n",
    "    print('\\nClassification Error: {:4f}'.format(classification_error)) \n",
    "    print('Accuracy: {:4f}\\n'.format(1-classification_error))\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(label_test,test_preds))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(confusion_matrix(label_test,test_preds))\n",
    "\n",
    "    test_probs = bagging_smote.predict_proba(feature_test)[:,1]\n",
    "    #Note: AUC is a better metric than accuracy because of imbalanced classes\n",
    "    print('\\nAUC: {:4f}'.format(roc_auc_score(label_test,test_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After undersampling/oversampling, we now have equal number of members in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion):   1929 \n",
      "Number of records with label 1 (complex emotion): 1929 \n"
     ]
    }
   ],
   "source": [
    "print('Number of records with label 0 (basic emotion):   {:4d} '.format(len(label_train_sm)-sum(label_train_sm)))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(sum(label_train_sm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
