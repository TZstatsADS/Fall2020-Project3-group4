{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "import time\n",
    "import math\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,pairwise_distances, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "import matplotlib.image as img\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following code doesn't run, then do 'pip install ipynb' in the command line. This code lets us import functions from notebooks in the lib folder. Lib is supposed to have all the model training/predicting functions and the doc folder is only supposed to have report/presentation files like main.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: set work directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide directories for training images. Training images and Training fiducial points will be in different subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change train_dir to your own path\n",
    "\n",
    "root = sys.path[0]\n",
    "train_dir =  os.path.join(root,  '../data/train_set/')  \n",
    "train_image_dir = train_dir+\"images/\"\n",
    "train_pt_dir = train_dir+\"points/\"\n",
    "train_label_path = train_dir+\"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: set up controls for evaluation experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chunk, we have a set of controls for the evaluation experiments. \n",
    "\n",
    "+ (T/F) cross-validation on the training set\n",
    "+ (T/F) reweighting the samples for training set \n",
    "+ (number) K, the number of CV folds\n",
    "+ (T/F) process features for training set\n",
    "+ (T/F) run evaluation on an independent test set\n",
    "+ (T/F) process features for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cv = True # run cross-validation on the training set\n",
    "sample_reweight = True # run sample reweighting in model training\n",
    "K = 5  # number of CV folds\n",
    "run_feature_train = True # process features for training set\n",
    "run_test = True # run evaluation on an independent test set\n",
    "run_feature_test = True # process features for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: import data and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with label 0 (basic emotion): 2402 \n",
      "Number of records with label 1 (complex emotion): 598 \n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "n = info.shape[0]\n",
    "\n",
    "#Data is imbalanced \n",
    "print('Number of records with label 0 (basic emotion): {:4d} '.format(info.loc[info['label']==0].shape[0]))\n",
    "print('Number of records with label 1 (complex emotion): {:2d} '.format(info.loc[info['label']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we could use sklearn train_test_split here instead of doing it\n",
    "#manually like in the starter code\n",
    "n_train = int(round(n*(4/5),0))\n",
    "train_idx = np.random.choice(list(info.index),size=n_train,replace=False)\n",
    "test_idx = list(set(list(info.index))-set(train_idx)) #set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to extract features from images, such as using Gabor filter, R memory will exhaust all images are read together. The solution is to repeat reading a smaller batch(e.g 100) and process them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(os.listdir(train_image_dir))\n",
    "\n",
    "# The following codes may be irrelevant to our analysis and when running them 'No module names PIL' error\n",
    "# popped up. So I temporarily commented them.\n",
    "# image_list = []\n",
    "# for i in range(1,101): # 1 to 100\n",
    "#     image = img.imread(train_image_dir+'{:04d}'.format(i)+'.jpg')\n",
    "#     image_list.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial points are stored in matlab format. In this step, we read them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#function to read fiducial points\n",
    "#input: index\n",
    "#output: matrix of fiducial points corresponding to the index\n",
    "\n",
    "def readMat_matrix(index):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinatesUnwarped']\n",
    "    except KeyError:\n",
    "        mat_data = scipy.io.loadmat(train_pt_dir+'{:04d}'.format(index)+'.mat')['faceCoordinates2']\n",
    "    return np.matrix.round(mat_data,0)\n",
    "\n",
    "#load fiducial points\n",
    "#pickle is the closest equivalent to .RData that I could find in Python\n",
    "fiducial_pt_list = list(map(readMat_matrix,list(range(1,n_files+1))))\n",
    "print(fiducial_pt_list[0].shape[1])\n",
    "pickle.dump(fiducial_pt_list, open( \"../output/fiducial_pt_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: construct features and responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. \n",
    "  \n",
    "  + `feature.R`\n",
    "  + Input: list of images or fiducial point\n",
    "  + Output: an RData file that contains extracted features and corresponding responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use feature.ipynb's feature function to generate features for the train and test points\n",
    "# Please use one of the feature extraction methods\n",
    "\n",
    "from ipynb.fs.full.feature import feature\n",
    "\n",
    "tm_feature_train = np.nan\n",
    "if run_feature_train == True:\n",
    "    start = time.time()\n",
    "    dat_train = feature(fiducial_pt_list, train_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_train = end-start\n",
    "    pickle.dump(dat_train, open( \"../output/feature_train.p\", \"wb\" ) )\n",
    "else:\n",
    "    pickle.load(open(\"../output/feature_train.p\", \"rb\"))\n",
    "    \n",
    "    \n",
    "tm_feature_test = np.nan\n",
    "if run_feature_test == True:\n",
    "    start = time.time()\n",
    "    dat_test = feature(fiducial_pt_list, test_idx, info)\n",
    "    end = time.time()\n",
    "    tm_feature_test = end-start\n",
    "    pickle.dump(dat_test, open( \"../output/feature_test.p\", \"wb\" ) )\n",
    "else:\n",
    "    pickle.load(open(\"../output/feature_test.p\", \"rb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use feature_dis_between_points.ipynb's feature function to generate features for the train and test points\n",
    "# This is a feature extraction method calculating distance between two 2-D points\n",
    "# However, it seems that this method is worse than that in starter code\n",
    "# The test accuracy rates of baseline model and XGBoost model are lower by using this feature extraction method\n",
    "\n",
    "# from ipynb.fs.full.feature_dis_between_points import feature_dis_between_points\n",
    "\n",
    "# tm_feature_train = np.nan\n",
    "# if run_feature_train == True:\n",
    "#     start = time.time()\n",
    "#     dat_train = feature_dis_between_points(fiducial_pt_list, train_idx, info)\n",
    "#     end = time.time()\n",
    "#     tm_feature_train = end-start\n",
    "#     pickle.dump(dat_train, open( \"../output/feature_train.p\", \"wb\" ) )\n",
    "# else:\n",
    "#     pickle.load(open(\"../output/feature_train.p\", \"rb\"))\n",
    "    \n",
    "    \n",
    "# tm_feature_test = np.nan\n",
    "# if run_feature_test == True:\n",
    "#     start = time.time()\n",
    "#     dat_test = feature_dis_between_points(fiducial_pt_list, test_idx, info)\n",
    "#     end = time.time()\n",
    "#     tm_feature_test = end-start\n",
    "#     pickle.dump(dat_test, open( \"../output/feature_test.p\", \"wb\" ) )\n",
    "# else:\n",
    "#     pickle.load(open(\"../output/feature_test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the traning/test features and labels\n",
    "\n",
    "feature_train = dat_train.loc[:, dat_train.columns != 'labels']\n",
    "label_train = dat_train['labels']\n",
    "\n",
    "feature_test = dat_test.loc[:, dat_test.columns != 'labels']\n",
    "label_test = dat_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA (not sure if i did it correctly)\n",
    "\n",
    "pca = PCA().fit(feature_train)\n",
    "feature_train = pca.transform(feature_train)\n",
    "feature_test = pca.transform(feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a classification model with training features and responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the train model and test model from library. \n",
    "\n",
    "`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. \n",
    "\n",
    "+ `train.R`\n",
    "  + Input: a data frame containing features and labels and a parameter list.\n",
    "  + Output:a trained model\n",
    "+ `test.R`\n",
    "  + Input: the fitted classification model using training data and processed features from testing images \n",
    "  + Input: an R object that contains a trained classifier.\n",
    "  + Output: training model specification\n",
    "\n",
    "+ In this Starter Code, we use logistic regression with LASSO penalty to do classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection with cross-validation\n",
    "* Do model selection by choosing among different values of training model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
    "model_labels = [\"Gradient Boosting with learning rate = \"+str(x) for x in lmbd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to do a grid search for optimal parameters\n",
    "#takes a really long time and we don't know the features yet\n",
    "#so i commented the grid search out for now\n",
    "\n",
    "if (run_cv):\n",
    "    params = {'learning_rate':lmbd, 'max_depth': [1,2,3,4], 'n_estimators':[100,200,300,400,500]}\n",
    "    #gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=K).fit(feature_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 85.377215 seconds\n"
     ]
    }
   ],
   "source": [
    "#Baseline model\n",
    "#need to do grid search to get optimal parameters though\n",
    "# import time\n",
    "start = time.time()\n",
    "gbm=GradientBoostingClassifier(learning_rate=0.1,max_depth=2,n_estimators=100)\n",
    "gbm.fit(feature_train,label_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Training time {:4f} seconds'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error:  0.4583333333333333\n",
      "Test Accuracy:  0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "test_preds=gbm.predict(feature_test)\n",
    "print('Classification Error: ',np.mean(np.array(test_preds)!=np.array(label_test))) #Classification Error\n",
    "print('Test Accuracy: ',np.mean(np.array(test_preds)==np.array(label_test))) # Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5416666666666666"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score is accuracy = 1-classification error i.e.\n",
    "#1-(np.mean(np.array(test_preds)!=np.array(label_test)))\n",
    "\n",
    "#problem with accuracy is that it'll be very high for our model\n",
    "#because of imbalanced classes\n",
    "\n",
    "#when we get the real test data in class, the accuracy will definitely\n",
    "#not be as high because the real test data is balanced\n",
    "\n",
    "gbm.score(feature_test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.51      0.64       473\n",
      "           1       0.26      0.65      0.37       127\n",
      "\n",
      "    accuracy                           0.54       600\n",
      "   macro avg       0.55      0.58      0.51       600\n",
      "weighted avg       0.72      0.54      0.58       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test,test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[243, 230],\n",
       "       [ 45,  82]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(label_test,test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5797056816100947"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ROC curve is FPR vs TPR so it's probably better than accuracy\n",
    "roc_auc_score(label_test,test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "training  model takes 12.006 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "xgb = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " num_class=2,\n",
    " n_estimators= 200,\n",
    " max_depth=2,\n",
    " eta=1,\n",
    " min_child_weight=1,\n",
    " objective= 'multi:softmax',  # for multi-labels classification \n",
    " scale_pos_weight=4,\n",
    " seed=123)\n",
    "\n",
    "xgb.fit(feature_train, label_train, eval_metric='auc')\n",
    "print(\"training  model takes %s seconds\" % round((time.time() - start_time),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model takes 0.026 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_xgb = xgb.predict(feature_test)\n",
    "print(\"testing model takes %s seconds\" % round((time.time() - start_time),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 76.83333333333333 percent\n"
     ]
    }
   ],
   "source": [
    "acc_xgb = accuracy_score(pred_xgb,label_test )\n",
    "print(\"Test accuracy is %s percent\" %(acc_xgb*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       473\n",
      "           1       0.42      0.24      0.30       127\n",
      "\n",
      "    accuracy                           0.77       600\n",
      "   macro avg       0.62      0.57      0.58       600\n",
      "weighted avg       0.73      0.77      0.74       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test,pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[431,  42],\n",
       "       [ 97,  30]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(label_test,pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5737127732183582"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ROC curve is FPR vs TPR so it's probably better than accuracy\n",
    "roc_auc_score(label_test,pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
